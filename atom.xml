<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lijiancheng0614</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lijiancheng0614.github.io/"/>
  <updated>2018-03-18T16:34:25.531Z</updated>
  <id>http://lijiancheng0614.github.io/</id>
  
  <author>
    <name>Jiancheng Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>修改TensorFlow-DeepLab</title>
    <link href="http://lijiancheng0614.github.io/2018/03/16/2018_03_16_TensorFlow-DeepLab/"/>
    <id>http://lijiancheng0614.github.io/2018/03/16/2018_03_16_TensorFlow-DeepLab/</id>
    <published>2018-03-15T16:00:00.000Z</published>
    <updated>2018-03-18T16:34:25.531Z</updated>
    
    <content type="html"><![CDATA[<p>代码仓库：<a href="https://github.com/lijiancheng0614/tensorflow_deeplab" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/tensorflow_deeplab</a></p><p>修改TensorFlow DeepLab，添加一些方便使用或新的功能。</p><a id="more"></a><h2 id="中文">中文</h2><p>使用方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/lijiancheng0614/tensorflow_deeplab deeplab</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`</div><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`/models/research/slim</div></pre></td></tr></table></figure><h3 id="在eval.py中添加gpu_allow_growth参数">在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><code>eval.py</code></li></ul><h3 id="在train.py中添加gpu_allow_growth参数">在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><code>train.py</code></li></ul><h3 id="在train.py中添加max_to_keep参数">在<code>train.py</code>中添加<code>max_to_keep</code>参数</h3><p>在<code>train.py</code>中添加<code>max_to_keep</code>参数，默认为<code>5</code>，即保留最后5个checkpoint。如为<code>0</code>则保留所有的checkpoint。</p><p>修改文件：</p><ul><li><code>train.py</code></li></ul><h2 id="english">English</h2><p>Usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/lijiancheng0614/tensorflow_deeplab deeplab</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`</div><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`/models/research/slim</div></pre></td></tr></table></figure><h3 id="add-gpu_allow_growth-parameter-in-eval.py">Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="add-gpu_allow_growth-parameter-in-train.py">Add <code>gpu_allow_growth</code> parameter in <code>train.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>train.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="add-max_to_keep-parameter-in-train.py">Add <code>max_to_keep</code> parameter in <code>train.py</code></h3><p>Add <code>max_to_keep</code> parameter in <code>train.py</code>, default value is <code>5</code> which means the 5 most recent checkpoint files are kept. If <code>0</code>, all checkpoint files are kept.</p><p>Modified files:</p><ul><li><code>train.py</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码仓库：&lt;a href=&quot;https://github.com/lijiancheng0614/tensorflow_deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lijiancheng0614/tensorflow_deeplab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;修改TensorFlow DeepLab，添加一些方便使用或新的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow DeepLab进行语义分割</title>
    <link href="http://lijiancheng0614.github.io/2018/03/13/2018_03_13_TensorFlow-DeepLab/"/>
    <id>http://lijiancheng0614.github.io/2018/03/13/2018_03_13_TensorFlow-DeepLab/</id>
    <published>2018-03-12T16:00:00.000Z</published>
    <updated>2018-03-18T16:31:58.168Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p><p>使用 TensorFlow DeepLab 进行语义分割</p><a id="more"></a><div class="figure"><img src="https://raw.githubusercontent.com/tensorflow/models/master/research/deeplab/g3doc/img/vis1.png"></div><h2 id="准备">准备</h2><ol start="0" style="list-style-type: decimal"><li><p>文件结构</p><p>这里以 PASCAL VOC 2012 为例，参考官方推荐的文件结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">deeplab/datasets/pascal_voc_seg</div><div class="line">├── exp</div><div class="line">│   └── train_on_train_set</div><div class="line">│       ├── eval</div><div class="line">│       │   └── events.out.tfevents....</div><div class="line">│       ├── export</div><div class="line">│       │   └── frozen_inference_graph.pb</div><div class="line">│       ├── train</div><div class="line">│       │   ├── checkpoint</div><div class="line">│       │   ├── events.out.tfevents....</div><div class="line">│       │   ├── graph.pbtxt</div><div class="line">│       │   ├── model.ckpt-0.data-00000-of-00001</div><div class="line">│       │   ├── model.ckpt-0.index</div><div class="line">│       │   ├── model.ckpt-0.meta</div><div class="line">│       │   └── ...</div><div class="line">│       └── vis</div><div class="line">│           ├── graph.pbtxt</div><div class="line">│           ├── raw_segmentation_results</div><div class="line">│           └── segmentation_results</div><div class="line">├── init_models</div><div class="line">│   └── deeplabv3_pascal_train_aug</div><div class="line">│       ├── frozen_inference_graph.pb</div><div class="line">│       ├── model.ckpt.data-00000-of-00001</div><div class="line">│       └── model.ckpt.index</div><div class="line">├── tfrecord</div><div class="line">│   ├── ....tfrecord</div><div class="line">│   └── ...</div><div class="line">└── VOCdevkit</div><div class="line">    └── VOC2012</div><div class="line">        ├── Annotations</div><div class="line">        ├── ImageSets</div><div class="line">        │   ├── Action</div><div class="line">        │   ├── Layout</div><div class="line">        │   ├── Main</div><div class="line">        │   └── Segmentation</div><div class="line">        ├── JPEGImages</div><div class="line">        ├── SegmentationClass</div><div class="line">        ├── SegmentationClassRaw</div><div class="line">        └── SegmentationObject</div></pre></td></tr></table></figure></li><li><p>安装 TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/install/</a> ，安装 TensorFlow v1.5.0 或更新的版本。</p><p>如果操作系统、GPU 型号、Python 版本号等配置跟官方一致，可直接使用官网提供的安装包安装。</p><blockquote><p>编译源码时注意 bazel 可能并不能总是获取 <code>$LD_LIBRARY_PATH</code>，如有报错，可以尝试添加参数 <code>action_env</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bazel build --config=opt --config=cuda tensorflow/tools/pip_package:build_pip_package --action_env=&quot;LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;&quot;</div></pre></td></tr></table></figure></p></blockquote></li><li><p>配置 TensorFlow Models</p><ul><li>下载 TensorFlow Models</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div></pre></td></tr></table></figure><ul><li>添加 <code>$PYTHONPATH</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/research/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/research/</span></div><div class="line">python deeplab/model_test.py</div></pre></td></tr></table></figure><p>若成功，显示<code>OK</code>。</p></li><li><p>准备数据</p><p>这里以 <code>PASCAL VOC 2012</code> 为例。</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md</a></p><p>运行以下代码即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/</span></div><div class="line">sh download_and_convert_voc2012.sh</div></pre></td></tr></table></figure><p>实际上，该脚本执行了以下操作：</p><ul><li>下载并解压</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/</span></div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</div><div class="line">tar -xf VOCtrainval_11-May-2012.tar</div></pre></td></tr></table></figure><ul><li>移除 ground-truth 中的 colormap</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/</span></div><div class="line">PASCAL_ROOT=<span class="string">"pascal_voc_seg/VOCdevkit/VOC2012"</span></div><div class="line">SEG_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/SegmentationClass"</span></div><div class="line">SEMANTIC_SEG_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/SegmentationClassRaw"</span></div><div class="line">python ./remove_gt_colormap.py \</div><div class="line">    --original_gt_folder=<span class="string">"<span class="variable">$&#123;SEG_FOLDER&#125;</span>"</span> \</div><div class="line">    --output_dir=<span class="string">"<span class="variable">$&#123;SEMANTIC_SEG_FOLDER&#125;</span>"</span></div></pre></td></tr></table></figure><ul><li>生成 TFRecord</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/</span></div><div class="line">OUTPUT_DIR=<span class="string">"pascal_voc_seg/tfrecord"</span></div><div class="line">mkdir -p <span class="string">"<span class="variable">$&#123;OUTPUT_DIR&#125;</span>"</span></div><div class="line">IMAGE_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/JPEGImages"</span></div><div class="line">LIST_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/ImageSets/Segmentation"</span></div><div class="line">python ./build_voc2012_data.py \</div><div class="line">    --image_folder=<span class="string">"<span class="variable">$&#123;IMAGE_FOLDER&#125;</span>"</span> \</div><div class="line">    --semantic_segmentation_folder=<span class="string">"<span class="variable">$&#123;SEMANTIC_SEG_FOLDER&#125;</span>"</span> \</div><div class="line">    --list_folder=<span class="string">"<span class="variable">$&#123;LIST_FOLDER&#125;</span>"</span> \</div><div class="line">    --image_format=<span class="string">"jpg"</span> \</div><div class="line">    --output_dir=<span class="string">"<span class="variable">$&#123;OUTPUT_DIR&#125;</span>"</span></div></pre></td></tr></table></figure></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型（ <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</a> ），</p><p>这里以 <code>deeplabv3_pascal_train_aug_2018_01_04</code> 以例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/</span></div><div class="line">mkdir init_models</div><div class="line"><span class="built_in">cd</span> init_models</div><div class="line">wget http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz</div><div class="line">tar zxf ssd_mobilenet_v1_coco_11_06_2017.tar.gz</div></pre></td></tr></table></figure></li></ol><h2 id="训练">训练</h2><p>如果使用现有模型进行预测则不需要训练。</p><ol style="list-style-type: decimal"><li><p>训练</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">mkdir -p logs/</div><div class="line">now=$(date +<span class="string">"%Y%m%d_%H%M%S"</span>)</div><div class="line">python ../../../../train.py \</div><div class="line">    --logtostderr \</div><div class="line">    --train_split=<span class="string">"train"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --train_crop_size=513 \</div><div class="line">    --train_crop_size=513 \</div><div class="line">    --train_batch_size=4 \</div><div class="line">    --training_number_of_steps=10 \</div><div class="line">    --fine_tune_batch_norm=<span class="literal">false</span> \</div><div class="line">    --tf_initial_checkpoint=<span class="string">"../../init_models/deeplabv3_pascal_train_aug/model.ckpt"</span> \</div><div class="line">    --train_logdir=<span class="string">"train/"</span> \</div><div class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> 2&gt;&amp;1 | tee logs/train_<span class="variable">$now</span>.txt &amp;</div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh train.sh</code> 即可训练。</p></li><li><p>验证</p><p>可一边训练一边验证，注意使用其它的GPU或合理分配显存。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/eval.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">python ../../../../eval.py \</div><div class="line">    --logtostderr \</div><div class="line">    --eval_split=<span class="string">"val"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --eval_crop_size=513 \</div><div class="line">    --eval_crop_size=513 \</div><div class="line">    --checkpoint_dir=<span class="string">"train/"</span> \</div><div class="line">    --eval_logdir=<span class="string">"eval/"</span> \</div><div class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> &amp;</div><div class="line">    <span class="comment"># --max_number_of_evaluations=1 &amp;</span></div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>CUDA_VISIBLE_DEVICES=&quot;1&quot; sh eval.sh</code> 即可验证（这里指定了第二个 GPU）。</p></li><li><p>可视化 log</p><p>可一边训练一边可视化训练的 log，访问 <code>http://localhost:6006/</code> 即可看到 loss 等的变化。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></div><div class="line">tensorboard --logdir train/</div></pre></td></tr></table></figure><p>可视化验证的 log，可看到 <code>miou_1.0</code> 的变化，这里指定了另一个端口。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></div><div class="line">tensorboard --logdir <span class="built_in">eval</span>/ --port 6007</div></pre></td></tr></table></figure><p>或同时可视化训练与验证的log：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></div><div class="line">tensorboard --logdir .</div></pre></td></tr></table></figure></li><li><p>可视化分割结果</p><p>可一边训练一边可视化分割结果。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/vis.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">python ../../../../vis.py \</div><div class="line">    --logtostderr \</div><div class="line">    --vis_split=<span class="string">"val"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --vis_crop_size=513 \</div><div class="line">    --vis_crop_size=513 \</div><div class="line">    --checkpoint_dir=<span class="string">"train/"</span> \</div><div class="line">    --vis_logdir=<span class="string">"vis/"</span> \</div><div class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> &amp;</div><div class="line">    <span class="comment"># --max_number_of_evaluations=1 &amp;</span></div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh vis.sh</code> 即可生成分割结果，<code>vis/segmentation_results/</code> 里有彩色化的分割结果，<code>vis/raw_segmentation_results/</code> 里有原始的分割结果。</p></li></ol><h2 id="测试">测试</h2><ol style="list-style-type: decimal"><li><p>导出模型</p><p>训练完成后得到一些 checkpoint 文件在 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train/</code> 中，如：</p><ul><li>graph.pbtxt</li><li>model.ckpt-1000.data-00000-of-00001</li><li>model.ckpt-1000.info</li><li>model.ckpt-1000.meta</li></ul><p>其中 meta 文件保存了 graph 和 metadata，ckpt 文件保存了网络的 weights。</p><p>而进行预测时只需模型和权重，不需要 metadata，故可使用官方提供的脚本生成推导图。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/export_model.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">python ../../../../export_model.py \</div><div class="line">    --logtostderr \</div><div class="line">    --checkpoint_path=<span class="string">"train/model.ckpt-<span class="variable">$1</span>"</span> \</div><div class="line">    --export_path=<span class="string">"export/frozen_inference_graph-<span class="variable">$1</span>.pb"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --num_classes=21 \</div><div class="line">    --crop_size=513 \</div><div class="line">    --crop_size=513 \</div><div class="line">    --inference_scales=1.0</div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh export_model.sh 1000</code> 即可导出模型 <code>export/frozen_inference_graph-1000.pb</code>。</p></li><li><p>测试图片</p><ul><li><p>运行 <code>deeplab_demo.ipynb</code> 并修改其中的各种路径即可。</p></li><li><p>或自写 inference 脚本，如 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/infer.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">'../../../../utils/'</span>)</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> get_dataset_colormap</div><div class="line"></div><div class="line">LABEL_NAMES = np.asarray([</div><div class="line">    <span class="string">'background'</span>, <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>, <span class="string">'bottle'</span>, <span class="string">'bus'</span>,</div><div class="line">    <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>, <span class="string">'cow'</span>, <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>, <span class="string">'motorbike'</span>,</div><div class="line">    <span class="string">'person'</span>, <span class="string">'pottedplant'</span>, <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tv'</span></div><div class="line">])</div><div class="line"></div><div class="line">FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), <span class="number">1</span>)</div><div class="line">FULL_COLOR_MAP = get_dataset_colormap.label_to_color_image(FULL_LABEL_MAP)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepLabModel</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Class to load deeplab model and run inference."""</span></div><div class="line"></div><div class="line">    INPUT_TENSOR_NAME = <span class="string">'ImageTensor:0'</span></div><div class="line">    OUTPUT_TENSOR_NAME = <span class="string">'SemanticPredictions:0'</span></div><div class="line">    INPUT_SIZE = <span class="number">513</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_path)</span>:</span></div><div class="line">        <span class="string">"""Creates and loads pretrained deeplab model."""</span></div><div class="line">        self.graph = tf.Graph()</div><div class="line">        <span class="keyword">with</span> open(model_path) <span class="keyword">as</span> fd:</div><div class="line">            graph_def = tf.GraphDef.FromString(fd.read())</div><div class="line">        <span class="keyword">with</span> self.graph.as_default():</div><div class="line">            tf.import_graph_def(graph_def, name=<span class="string">''</span>)</div><div class="line">        self.sess = tf.Session(graph=self.graph)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, image)</span>:</span></div><div class="line">        <span class="string">"""Runs inference on a single image.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            image: A PIL.Image object, raw input image.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            resized_image: RGB image resized from original input image.</span></div><div class="line"><span class="string">            seg_map: Segmentation map of `resized_image`.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        width, height = image.size</div><div class="line">        resize_ratio = <span class="number">1.0</span> * self.INPUT_SIZE / max(width, height)</div><div class="line">        target_size = (int(resize_ratio * width), int(resize_ratio * height))</div><div class="line">        resized_image = image.convert(<span class="string">'RGB'</span>).resize(target_size,</div><div class="line">                                                    Image.ANTIALIAS)</div><div class="line">        batch_seg_map = self.sess.run(</div><div class="line">            self.OUTPUT_TENSOR_NAME,</div><div class="line">            feed_dict=&#123;</div><div class="line">                self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]</div><div class="line">            &#125;)</div><div class="line">        seg_map = batch_seg_map[<span class="number">0</span>]</div><div class="line">        <span class="keyword">return</span> resized_image, seg_map</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_segmentation</span><span class="params">(image, seg_map)</span>:</span></div><div class="line">    plt.figure()</div><div class="line"></div><div class="line">    plt.subplot(<span class="number">221</span>)</div><div class="line">    plt.imshow(image)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.title(<span class="string">'input image'</span>)</div><div class="line"></div><div class="line">    plt.subplot(<span class="number">222</span>)</div><div class="line">    seg_image = get_dataset_colormap.label_to_color_image(</div><div class="line">        seg_map, get_dataset_colormap.get_pascal_name()).astype(np.uint8)</div><div class="line">    plt.imshow(seg_image)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.title(<span class="string">'segmentation map'</span>)</div><div class="line"></div><div class="line">    plt.subplot(<span class="number">223</span>)</div><div class="line">    plt.imshow(image)</div><div class="line">    plt.imshow(seg_image, alpha=<span class="number">0.7</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.title(<span class="string">'segmentation overlay'</span>)</div><div class="line"></div><div class="line">    unique_labels = np.unique(seg_map)</div><div class="line">    ax = plt.subplot(<span class="number">224</span>)</div><div class="line">    plt.imshow(</div><div class="line">        FULL_COLOR_MAP[unique_labels].astype(np.uint8),</div><div class="line">        interpolation=<span class="string">'nearest'</span>)</div><div class="line">    ax.yaxis.tick_right()</div><div class="line">    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])</div><div class="line">    plt.xticks([], [])</div><div class="line">    ax.tick_params(width=<span class="number">0</span>)</div><div class="line"></div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</div><div class="line">        print(<span class="string">'Usage: python &#123;&#125; image_path model_path'</span>.format(sys.argv[<span class="number">0</span>]))</div><div class="line">        exit()</div><div class="line"></div><div class="line">    image_path = sys.argv[<span class="number">1</span>]</div><div class="line">    model_path = sys.argv[<span class="number">2</span>]</div><div class="line">    model = DeepLabModel(model_path)</div><div class="line">    orignal_im = Image.open(image_path)</div><div class="line">    resized_im, seg_map = model.run(orignal_im)</div><div class="line">    vis_segmentation(resized_im, seg_map)</div></pre></td></tr></table></figure><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</span></div><div class="line">python infer.py \</div><div class="line">    ../../../../g3doc/img/image1.jpg \</div><div class="line">    <span class="built_in">export</span>/frozen_inference_graph.pb</div></pre></td></tr></table></figure><p>运行结果：</p><div class="figure"><img src="plot_image1.png"></div></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/deeplab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用 TensorFlow DeepLab 进行语义分割&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab-v3+</title>
    <link href="http://lijiancheng0614.github.io/2018/02/27/2018_02_27_DeepLab-v3+/"/>
    <id>http://lijiancheng0614.github.io/2018/02/27/2018_02_27_DeepLab-v3+/</id>
    <published>2018-02-26T16:00:00.000Z</published>
    <updated>2018-03-18T16:39:07.972Z</updated>
    
    <content type="html"><![CDATA[<p>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</p><p>Paper: <a href="https://arxiv.org/abs/1802.02611" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1802.02611</a></p><p>Blog: <a href="https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html" class="uri" target="_blank" rel="external">https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html</a></p><p>Code: <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p><a id="more"></a><p>在 DeepLab-v3 上添加 decoder 细化分割结果（尤其是物体边界），且使用 depthwise separable convolution 加速。</p><p>DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries.</p><p>We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network.</p><h2 id="introduction">Introduction</h2><p>考虑语义分割的两种类型：</p><ol style="list-style-type: decimal"><li><p>空间金字塔池化模块：通过池化不同分辨率的特征，捕获丰富的上下文信息</p></li><li><p>编码器解码器结构：能够获得锐利的物体边界</p></li></ol><p>we consider two types of neural networks for semantic segmentation:</p><ol style="list-style-type: decimal"><li><p>spatial pyramid pooling module: captures rich contextual information by pooling features at different resolution</p></li><li><p>encoder-decoder structure: is able to obtain sharp object boundaries.</p></li></ol><p>空间金字塔池化模块计算密集，因为提取输出特征只是输入大小的 8 倍甚至 4 倍。</p><p>it is computationally prohibitive to extract output feature maps that are 8, or even 4 times smaller than the input resolution.</p><p>编码器解码器结构在编码器路径中计算更快（因为没有扩散特征）。</p><p>encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path.</p><p>结合二者优点，提出加入多尺度上下文信息来丰富编码器解码器网络中的编码器模块。</p><p>Attempting to combine the advantages from both methods, we propose to enrich the encoder module in the encoder-decoder networks by incorporating the multi-scale contextual information.</p><p>采用 Xception 模型在速度和精度上都有所提升</p><p>show improvement in terms of both speed and accuracy by adapting the Xception model</p><p>contributions:</p><ol style="list-style-type: decimal"><li><p>propose a novel encoder-decoder structure which employs DeepLabv3 as a powerful encoder module.</p></li><li><p>can arbitrarily control the resolution of extracted encoder features by atrous convolution to trade-off precision and runtime, which is not possible with existing encoder-decoder models.</p></li><li><p>adapt the Xception model for the segmentation task and apply depthwise separable convolution to both<br>ASPP module and decoder module, resulting in a faster and stronger encoder-decoder network.</p></li><li><p>attains a new state-of-art performance on PASCAL VOC 2012 dataset.</p></li><li><p>make our Tensorflow-based implementation of the proposed model publicly available.</p></li></ol><h2 id="related-work">Related Work</h2><p>Spatial pyramid pooling</p><p>Encoder-decoder</p><p>Depthwise separable convolution</p><h2 id="methods">Methods</h2><h3 id="encoder-decoder-with-atrous-convolution">Encoder-Decoder with Atrous Convolution</h3><ul><li><p>Atrous convolution</p></li><li><p>Depthwise separable convolution</p></li><li><p>DeepLabv3 as encoder</p></li><li><p>Proposed decoder</p><p>In the work of DeepLabv3, the features are bilinearly upsampled by a factor of 16, which could be considered a naive decoder module.</p><p>However, this naive decoder module may not successfully recover object segmentation details.</p><p>We thus propose a simple yet effective decoder module, as illustrated in Fig. 2.</p><ol style="list-style-type: decimal"><li><p>The encoder features are first bilinearly upsampled by a factor of 4</p></li><li><p>then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution</p><p>apply another 1 × 1 convolution on the low-level features to reduce the number of channels</p></li><li><p>apply a few 3 × 3 convolutions to refine the features</p></li><li><p>another bilinear upsampling by a factor of 4</p></li></ol><p>解码器结构：</p><ol style="list-style-type: decimal"><li><p>编码器特征双线性插值上采样 4 倍</p></li><li><p>然后与具有相同空间分辨率的相应低级特征合并</p><p>作用一个 1 x 1 卷积在低级特征上以减少通道数</p></li><li><p>经过一些 3 x 3 的卷积以精炼特征</p></li><li><p>再双线性插值上采样 4 倍</p></li></ol></li></ul><div class="figure"><img src="http://liangchiehchen.com/fig/deeplabv3plus.png"></div><p>Figure 2. Our proposed DeepLabv3+ extends DeepLabv3 by employing a encoder-decoder structure. The encoder module encodes multiscale contextual information by applying atrous convolution at multiple scales, while the simple yet effective decoder module refines the segmentation results along object boundaries.</p><h3 id="modified-aligned-xception">Modified Aligned Xception</h3><p>The Xception model has shown promising image classification results on ImageNet with fast computation.</p><p>More recently, the MSRA team modifies the Xception model (called Aligned Xception) and further pushes the performance in the task of object detection.</p><div class="figure"><img src="figure3.png"></div><p>Figure 3. The Xception model is modified as follows: (1) more layers (same as MSRA’s modification except the changes in Entry flow), (2) all the max pooling operations are replaced by depthwise separable convolutions with striding, and (3) extra batch normalization and ReLU are added after each 3 × 3 depthwise convolution, similar to MobileNet.</p><p>a few more changes:</p><p>一些改进：</p><ol style="list-style-type: decimal"><li><p>不修改入口流的网络结构，为了快速计算和存储效率</p></li><li><p>替代最大池化操作为深度可分离卷积，这使我们能够应用多孔分离卷积在任意分辨率提取特征（另一种选择是延长 arous 算法到最大池化操作）</p></li><li><p>在 3 x 3 的深度可分离卷积后添加额外的 BN 和 ReLU 激活</p></li><li><p>do not modify the entry flow network structure for fast computation and memory efficiency</p></li><li><p>all max pooling operations are replaced by depthwise separable convolution with striding, which enables us to apply atrous separable convolution to extract feature maps at an arbitrary resolution (another option is to extend the atrous algorithm to max pooling operations)</p></li><li><p>extra batch normalization and ReLU activation are added after each 3 × 3 depthwise convolution</p></li></ol><h2 id="experimental-evaluation">Experimental Evaluation</h2><h3 id="decoder-design-choices">Decoder Design Choices</h3><p>考虑三个地方进行不同的设计：</p><ol style="list-style-type: decimal"><li><p>用来减少编码器模块的底层特征图的通道的 1 × 1 卷积</p></li><li><p>得到清晰的分割结果的 3 × 3 卷积</p></li><li><p>使用哪些编码器低级特征</p></li></ol><p>we consider three places for different design choices:</p><ol style="list-style-type: decimal"><li><p>the 1 × 1 convolution used to reduce the channels of the low-level feature map from the encoder module</p><p>实验表明，48 个 channel 的 1 x 1 卷积效果最好</p></li><li><p>the 3 × 3 convolution used to obtain sharper segmentation results</p><p>实验表明，2 个 256 channel 的 3 x 3 卷积效果最好</p></li><li><p>what encoder low-level features should be used</p><p>实验表明，只使用 Conv2 的特征效果最好</p></li></ol><h3 id="resnet-101-as-network-backbone">ResNet-101 as Network Backbone</h3><ul><li><p>Baseline</p></li><li><p>Adding decoder</p></li><li><p>Coarser feature maps</p></li></ul><div class="figure"><img src="table3.png"></div><p>Table 3. Inference strategy on the PASCAL VOC 2012 val set when using ResNet-101 as feature extractor. train OS: The output stride used during training. eval OS: The output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multi-scale inputs during evaluation. Flip: Adding left-right flipped inputs.</p><h3 id="xception-as-network-backbone">Xception as Network Backbone</h3><ul><li><p>ImageNet pretraining</p></li><li><p>Baseline</p></li><li><p>Adding decoder</p></li><li><p>Using depthwise separable convolution</p></li><li><p>Pretraining on COCO</p></li><li><p>Pretraining on JFT</p></li><li><p>Test set results</p></li><li><p>Qualitative results</p></li><li><p>Failure mode</p></li></ul><div class="figure"><img src="table5.png"></div><p>Table 5. Inference strategy on the PASCAL VOC 2012 val set when using modified Xception as feature extractor. train OS: The output stride used during training. eval OS: The output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multi-scale inputs during evaluation. Flip: Adding left-right flipped inputs. SC: Adopting depthwise separable convolution for both ASPP and decoder modules. COCO: Models pretrained on MS-COCO dataset. JFT: Models pretrained on JFT dataset.</p><h3 id="improvement-along-object-boundaries">Improvement along Object Boundaries</h3><p>employing the proposed decoder for both ResNet-101 and Xception network backbones improves the performance compared to the naive bilinear upsampling.</p><h2 id="conclusion">Conclusion</h2><p>Our proposed model “DeepLabv3+” employs the encoderdecoder structure where DeepLabv3 is used to encode the rich contextual information and a simple yet effective decoder module is adopted to recover the object boundaries. One could also apply the atrous convolution to extract the encoder features at an arbitrary resolution, depending on the available computation resources. We also explore the Xception model and atrous separable convolution to make the proposed model faster and stronger. Finally, our experimental results show that the proposed model sets a new state-of-the-art performance on the PASCAL VOC 2012 semantic image segmentation benchmark.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1802.02611&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Blog: &lt;a href=&quot;https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/deeplab&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>代码评审 Code Review</title>
    <link href="http://lijiancheng0614.github.io/2018/01/12/2018_01_12_code_review/"/>
    <id>http://lijiancheng0614.github.io/2018/01/12/2018_01_12_code_review/</id>
    <published>2018-01-11T16:00:00.000Z</published>
    <updated>2018-03-07T16:30:12.350Z</updated>
    
    <content type="html"><![CDATA[<p>介绍几个轻量级代码检查工具，包括代码静态检查，整理代码等。</p><a id="more"></a><p>无论是自己一个人写代码，还是与其他人合作写代码，都希望能有一份高质量的代码，以便别人或未来的自己可读、可维护和可扩展。</p><p>于是往往我们需要代码评审（Code Review）。正式的代码评审已经有不少书籍介绍且与开发环境有关，这里只介绍几个轻量级代码检查工具，方便日常开发过程中提高自己的代码质量。</p><h2 id="python">Python</h2><h3 id="pylint">Pylint</h3><p><a href="https://www.pylint.org/" class="uri" target="_blank" rel="external">https://www.pylint.org/</a></p><p>Pylint 是一个 Python 源代码分析工具，可以分析代码错误，查找不符合代码风格标准（Pylint 默认使用的代码风格是 <a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="external">PEP 8</a>）和有潜在问题的代码。</p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install pylint</div></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pylint 1.py</div></pre></td></tr></table></figure><p>安装直接用 pip 就可以了，使用则是输入代码文件路径就可以了，它会输出存在不同级别问题的代码的所在位置，还能评分。更多的用法可以参考官方网站。</p><h3 id="yapf">yapf</h3><p><a href="https://github.com/google/yapf" class="uri" target="_blank" rel="external">https://github.com/google/yapf</a></p><p>yapf 是一个 Google 开源的 Python 代码格式化工具。可以格式化代码，统一缩进、换行、符号等格式。</p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install yapf</div></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yapf 1.py &gt; out.py</div></pre></td></tr></table></figure><p>安装直接用 pip 就可以了，使用则是输入代码文件路径就可以了，它会输出美化后的代码，因此可以重定向至一个文件。更多的用法可以参考官方网站。</p><h2 id="c">C++</h2><h3 id="cpplint">cpplint</h3><p><a href="https://github.com/cpplint/cpplint" class="uri" target="_blank" rel="external">https://github.com/cpplint/cpplint</a></p><p>cpplint 是一个遵循 Google C++ 风格指南的 C++ 静态代码检索工具。</p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install cpplint</div></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cpplint 1.cpp</div></pre></td></tr></table></figure><p>安装直接用 pip 就可以了，使用则是输入代码文件路径就可以了，它会输出存在不同级别问题的代码的所在位置。更多的用法可以参考官方网站。</p><h3 id="astyle">AStyle</h3><p><a href="http://astyle.sourceforge.net/" class="uri" target="_blank" rel="external">http://astyle.sourceforge.net/</a></p><p>AStyle 即 Artistic Style，是一个支持 C, C++, C++/CLI，Objective‑C, C# 和 Java 的格式化工具。可以格式化代码，统一缩进、换行、符号等格式。</p><p>注意 AStyle 风格与 Google C++ 风格不一样。</p><p>安装：</p><p>到 <a href="https://sourceforge.net/projects/astyle/" class="uri" target="_blank" rel="external">https://sourceforge.net/projects/astyle/</a> 点“Download”按钮即可根据当前平台下载相应安装包，根据说明安装即可。</p><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">astyle 1.cpp</div></pre></td></tr></table></figure><p>使用则是输入代码文件路径就可以了，它会输出美化后的代码，并把原代码备份至 <code>1.cpp.orig</code>。更多的用法可以参考官方网站。</p><h2 id="sublime-插件">Sublime 插件</h2><p>以上工具均在 Sublime 中能找到插件，只需 Install Package 安装即可。之后可在编码时实现一键格式化或一键检查。</p><p><a href="http://www.sublimelinter.com/en/stable/" target="_blank" rel="external">SublimeLinter</a> 里包含各种语言的 linter。</p><p><a href="https://github.com/jason-kane/PyYapf" target="_blank" rel="external">PyYapf</a> 是 Python yapf 格式化工具。</p><p><a href="https://github.com/timonwong/SublimeAStyleFormatter" target="_blank" rel="external">SublimeAStyleFomatter</a> 是 C++ AStyle 格式化工具。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍几个轻量级代码检查工具，包括代码静态检查，整理代码等。&lt;/p&gt;
    
    </summary>
    
      <category term="Develop" scheme="http://lijiancheng0614.github.io/categories/Develop/"/>
    
    
      <category term="Python" scheme="http://lijiancheng0614.github.io/tags/Python/"/>
    
      <category term="C++" scheme="http://lijiancheng0614.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>句子分类</title>
    <link href="http://lijiancheng0614.github.io/2018/01/08/2018_01_08_sentence_classification/"/>
    <id>http://lijiancheng0614.github.io/2018/01/08/2018_01_08_sentence_classification/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-03-05T16:12:10.894Z</updated>
    
    <content type="html"><![CDATA[<p>学习句子分类，使用深度学习的方法对句子数据集进行分类。</p><a id="more"></a><h2 id="问题">问题</h2><p>句子分类（Sentence Classification）是指给定一个句子，标注预先设定的若干类别中的一个类别。</p><p>句子分类包括情感分析（Sentiment Analysis）、问题分类（Question<br>Classification）等任务。情感分析又称倾向性分析、意见抽取（Opinion extraction）、意见挖掘（Opinion mining）、情感挖掘（Sentiment mining）、主观分析（Subjectivity analysis），它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从评论文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向。</p><h2 id="应用">应用</h2><p>了解对电影、商品、Twitter 等的褒贬评价，以此来改善产品和服务、发现竞争对手的优劣势、预测股票走势等。</p><h2 id="数据集">数据集</h2><table><thead><tr class="header"><th>Data</th><th>c</th><th>l</th><th>N</th><th>|V|</th><th>|V_pre|</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>MR</td><td>2</td><td>20</td><td>10662</td><td>18765</td><td>16448</td><td>CV</td></tr><tr class="even"><td>SST-1</td><td>5</td><td>18</td><td>11855</td><td>17836</td><td>16262</td><td>2210</td></tr><tr class="odd"><td>SST-2</td><td>2</td><td>19</td><td>9613</td><td>16185</td><td>14838</td><td>1821</td></tr><tr class="even"><td>Subj</td><td>2</td><td>23</td><td>10000</td><td>21323</td><td>17913</td><td>CV</td></tr><tr class="odd"><td>TREC</td><td>6</td><td>10</td><td>5952</td><td>9592</td><td>9125</td><td>500</td></tr><tr class="even"><td>CR</td><td>2</td><td>19</td><td>3775</td><td>5340</td><td>5046</td><td>CV</td></tr><tr class="odd"><td>MPQA</td><td>2</td><td>3</td><td>10606</td><td>6246</td><td>6083</td><td>CV</td></tr></tbody></table><ul><li><p>MR: Movie reviews 电影评论，每条评论包含一个句子。<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p></li><li><p>SST-1: Stanford Sentiment Treebank，MR 的扩展但划分了 train/dev/test 集合并提供 5 个细粒度标签（非常积极的，积极的，中性的，负面的，非常消极的）。</p></li><li><p>SST-2: 与 SST-1 一样但移除中性评论并用二进制标签。<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p></li><li><p>Subj: Subjectivity 主观性数据集，任务是将句子分类为主观或客观的。<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p></li><li><p>TREC: TREC question dataset TREC 问题数据集，任务是将一个问题分成 6 类（关于人、位置、数字信息等）。<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p></li><li><p>CR: Customer reviews 各种产品的客户评论，任务是预测正面/负面评论。<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p></li><li><p>MPQA: MPQA 数据集意见极性检测任务。<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p></li></ul><h2 id="方法">方法</h2><p>通常会把任务拆分成几个子任务：</p><ol style="list-style-type: decimal"><li><p>分词</p><p>把句子根据意思分成多个词，有时可能还需要去掉停用词、了解词性、转换成词向量等操作。</p></li><li><p>提取特征</p><p>有时我们不会直接使用分词后的多个词来直接分类，这时需要提取特征来方便分类。</p><p>常用特征：TF-IDF、LDA、LSI</p></li><li><p>构建分类器</p><p>输入特征或词向量等，通过一些模型，对该句子进行分类。</p></li></ol><h3 id="naive-bayes">Naive Bayes</h3><p>NBSVM: Naive Bayes SVM</p><p>MNB: Multinomial Naive Bayes <a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p><p>combine-skip</p><p>combine-skip + NB <a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>NBSVM</td><td>79.4</td><td>-</td><td>-</td><td>93.2</td><td>-</td><td>81.8</td><td>86.3</td></tr><tr class="even"><td>MNB</td><td>79.0</td><td>-</td><td>-</td><td>93.6</td><td>-</td><td>80.0</td><td>86.3</td></tr><tr class="odd"><td>combine-skip</td><td>76.5</td><td>-</td><td>-</td><td>93.6</td><td>92.2</td><td>80.1</td><td>87.1</td></tr><tr class="even"><td>combine-skip+NB</td><td>80.4</td><td>-</td><td>-</td><td>93.6</td><td>-</td><td>81.3</td><td>87.5</td></tr></tbody></table><h3 id="rnn">RNN</h3><p>RCNN: Recurrent Convolutional Neural Networks <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p><p>S-LSTM: Long Short-Term Memory Over Recursive Structures <a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p><p>LSTM: Long Short-Term Memory</p><p>BLSTM: Bidirectional Long Short-Term Memory</p><p>Tree-LSTM: Tree-structured Long Short-Term Memory <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p><p>LSTMN: Long Short-Term Memory-Network <a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p><p>Multi-Task: Recurrent Neural Network for Text Classification with Multi-Task Learning <a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p><p>BLSTM-Att: Bidirectional Long Short-Term Memory, attention-based model</p><p>BLSTM-2DPooling: Bidirectional Long Short-Term Memory Networks with Two-Dimensional Max Pooling</p><p>BLSTM-2DCNN: Bidirectional Long Short-Term Memory Networks with 2D convolution <a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>RCNN</td><td>-</td><td>47.21</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>S-LSTM</td><td>-</td><td>-</td><td>81.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="odd"><td>LSTM</td><td>-</td><td>46.4</td><td>84.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>BLSTM</td><td>-</td><td>49.1</td><td>87.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="odd"><td>Tree-LSTM</td><td>-</td><td>51.0</td><td>88.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>LSTMN</td><td>-</td><td>49.3</td><td>87.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="odd"><td>Multi-Task</td><td>-</td><td>49.6</td><td>87.9</td><td>94.1</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>BLSTM</td><td>80.0</td><td>49.1</td><td>87.6</td><td>92.1</td><td>93.0</td><td>-</td><td>-</td></tr><tr class="odd"><td>BLSTM-Att</td><td>81.0</td><td>49.8</td><td>88.2</td><td>93.5</td><td>93.8</td><td>-</td><td>-</td></tr><tr class="even"><td>BLSTM-2DPooling</td><td>81.5</td><td>50.5</td><td>88.3</td><td>93.7</td><td>94.8</td><td>-</td><td>-</td></tr><tr class="odd"><td>BLSTM-2DCNN</td><td>82.3</td><td>52.4</td><td>89.5</td><td>94.0</td><td>96.1</td><td>-</td><td>-</td></tr></tbody></table><h3 id="cnn">CNN</h3><p>DCNN: Dynamic Convolutional Neural Network <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p><p>CNN-non-static: Convolutional Neural Networks, the pretrained vectors are fine-tuned for each task</p><p>CNN-multichannel: Convolutional Neural Networks with two sets of word vectors <a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p><p>TBCNN: Tree-based Convolutional Neural Network <a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p><p>Molding-CNN: Molding Convolutional Neural Networks <a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a></p><p>CNN-Ana: Non-static GloVe+word2vec CNN <a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a></p><p>MVCNN: Multichannel Variable-Size Convolution <a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a></p><p>DSCNN: Dependency Sensitive Convolutional Neural Networks <a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>DCNN</td><td>-</td><td>48.5</td><td>86.8</td><td>-</td><td>93.0</td><td>-</td><td>-</td></tr><tr class="even"><td>CNN-non-static</td><td>81.5</td><td>48.0</td><td>87.2</td><td>93.4</td><td>93.6</td><td>84.3</td><td>89.5</td></tr><tr class="odd"><td>CNN-multichannel</td><td>81.1</td><td>47.4</td><td>88.1</td><td>93.2</td><td>92.2</td><td>85.0</td><td>89.4</td></tr><tr class="even"><td>TBCNN</td><td>-</td><td>51.4</td><td>87.9</td><td>-</td><td>96.0</td><td>-</td><td>-</td></tr><tr class="odd"><td>Molding-CNN</td><td>-</td><td>51.2</td><td>88.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>CNN-Ana</td><td>81.02</td><td>45.98</td><td>85.45</td><td>93.66</td><td>91.37</td><td>84.65</td><td>89.55</td></tr><tr class="odd"><td>MVCNN</td><td>-</td><td>49.6</td><td>89.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>DSCNN</td><td>81.5</td><td>49.7</td><td>89.1</td><td>93.2</td><td>95.4</td><td>-</td><td>-</td></tr></tbody></table><h3 id="others">Others</h3><p>RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia <a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></p><p>AdaSent: self-adaptive hierarchical sentence model <a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a></p><p>RNTN: Recursive Neural Tensor Network <a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a></p><p>DRNN: Deep Recursive Neural Networks <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>RAE</td><td>77.7</td><td>43.2</td><td>82.4</td><td>-</td><td>-</td><td>-</td><td>86.4</td></tr><tr class="even"><td>AdaSent</td><td>83.1</td><td>-</td><td>-</td><td>95.5</td><td>92.4</td><td>86.3</td><td>93.3</td></tr><tr class="odd"><td>RNTN</td><td>-</td><td>45.7</td><td>85.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>DRNN</td><td>-</td><td>49.8</td><td>86.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><h2 id="参考">参考</h2><div class="footnotes"><hr><ol><li id="fn1"><p>(ACL 2005) Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" class="uri" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><a href="#fnref1">↩</a></p></li><li id="fn2"><p>(EMNLP 2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank <a href="https://nlp.stanford.edu/sentiment/" class="uri" target="_blank" rel="external">https://nlp.stanford.edu/sentiment/</a><a href="#fnref2">↩</a></p></li><li id="fn3"><p>(ACL 2004) A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data" class="uri" target="_blank" rel="external">http://www.cs.cornell.edu/people/pabo/movie-review-data</a><a href="#fnref3">↩</a></p></li><li id="fn4"><p>(ACL 2002) Learning Question Classifiers <a href="http://cogcomp.org/Data/QA/QC/" class="uri" target="_blank" rel="external">http://cogcomp.org/Data/QA/QC/</a><a href="#fnref4">↩</a></p></li><li id="fn5"><p>(SIGKDD 2004) Mining and Summarizing Customer Reviews <a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html" class="uri" target="_blank" rel="external">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a><a href="#fnref5">↩</a></p></li><li id="fn6"><p>(Language Resources and Evaluation 2005) Annotating Expressions Of Opinions And Emotions In Language <a href="http://mpqa.cs.pitt.edu/" class="uri" target="_blank" rel="external">http://mpqa.cs.pitt.edu/</a><a href="#fnref6">↩</a></p></li><li id="fn7"><p>(ACL 2012) Baselines and Bigrams: Simple, Good Sentiment and Topic Classification<a href="#fnref7">↩</a></p></li><li id="fn8"><p>(NIPS 2015) Skip-Thought Vectors<a href="#fnref8">↩</a></p></li><li id="fn9"><p>(AAAI 2015) Recurrent Convolutional Neural Networks for Text Classification<a href="#fnref9">↩</a></p></li><li id="fn10"><p>(ICML 2015) Long Short-Term Memory Over Recursive Structures<a href="#fnref10">↩</a></p></li><li id="fn11"><p>(ACL 2015) Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks<a href="#fnref11">↩</a></p></li><li id="fn12"><p>(EMNLP2016) Long Short-Term Memory-Networks for Machine Reading<a href="#fnref12">↩</a></p></li><li id="fn13"><p>(IJCAI 2016) Recurrent Neural Network for Text Classification with Multi-Task Learning<a href="#fnref13">↩</a></p></li><li id="fn14"><p>(COLING 2016) Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling<a href="#fnref14">↩</a></p></li><li id="fn15"><p>(ACL 2014) A Convolutional Neural Network for Modelling Sentences<a href="#fnref15">↩</a></p></li><li id="fn16"><p>(EMNLP 2014) Convolutional Neural Networks for Sentence Classification<a href="#fnref16">↩</a></p></li><li id="fn17"><p>(EMNLP 2015) Discriminative Neural Sentence Modeling by Tree-Based Convolution<a href="#fnref17">↩</a></p></li><li id="fn18"><p>(EMNLP 2015) Molding CNNs for text: non-linear, non-consecutive convolutions<a href="#fnref18">↩</a></p></li><li id="fn19"><p>(IJCNLP 2017) A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification<a href="#fnref19">↩</a></p></li><li id="fn20"><p>(CoNLL 2015) Multichannel Variable-Size Convolution for Sentence Classification<a href="#fnref20">↩</a></p></li><li id="fn21"><p>(NAACL 2016) Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents<a href="#fnref21">↩</a></p></li><li id="fn22"><p>(EMNLP 2011) Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions<a href="#fnref22">↩</a></p></li><li id="fn23"><p>(IJCAI 2015) Self-adaptive hierarchical sentence model<a href="#fnref23">↩</a></p></li><li id="fn24"><p>(EMNLP 2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank<a href="#fnref24">↩</a></p></li><li id="fn25"><p>(NIPS 2014) Deep Recursive Neural Networks for Compositionality in Language<a href="#fnref25">↩</a></p></li></ol></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习句子分类，使用深度学习的方法对句子数据集进行分类。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>神经网络权值量化</title>
    <link href="http://lijiancheng0614.github.io/2017/12/10/2017_12_10_network_weights_quantization/"/>
    <id>http://lijiancheng0614.github.io/2017/12/10/2017_12_10_network_weights_quantization/</id>
    <published>2017-12-09T16:00:00.000Z</published>
    <updated>2018-03-01T16:22:11.971Z</updated>
    
    <content type="html"><![CDATA[<p>对神经网络的权值进行量化，使模型大小变小，运行速度变快，且准确率与原来相近。</p><a id="more"></a><p>参考 <a href="https://www.tensorflow.org/performance/quantization" class="uri" target="_blank" rel="external">https://www.tensorflow.org/performance/quantization</a></p><h2 id="什么是量化">什么是量化</h2><p>把网络权值从高精度转化成低精度（32位浮点数 float32 转化成 8位定点数 int8 或二值化为 1 bit），但模型准确率等指标与原来相近，模型大小变小，运行速度加快。</p><h2 id="为什么量化">为什么量化</h2><p>量化可以看作是噪声的一种来源，所以量化后的模型效果与原来相近。</p><ul><li><p>优点</p><ol style="list-style-type: decimal"><li><p>模型变小，运行速度变快。</p></li><li><p>int8 只需 float32 内存带宽的25％，可以更好使用缓存并避免 RAM 访问出现瓶颈。</p></li><li><p>每个时钟周期执行更多的 SIMD 操作。</p></li><li><p>如有加速8位计算的 DSP 芯片则更快。</p></li></ol></li><li><p>缺点</p><p>效果稍差。</p></li></ul><h2 id="如何量化">如何量化</h2><p>先训练模型，再进行量化，测试时使用量化后的模型。</p><ul><li><p>训练</p><p>一般使用 float32 来训练模型效果较好（特别是反向传播和梯度需要浮点来表示）</p></li><li><p>量化</p><ol style="list-style-type: decimal"><li><p>加入量化和反量化操作（如一种量化操作为根据该层权值的最大值和最小值映射到 8位区间）</p><p>如下图 1 变成图 2</p><div class="figure"><img src="https://www.tensorflow.org/images/quantization0.png"></div><div class="figure"><img src="https://www.tensorflow.org/images/quantization1.png"></div></li><li><p>把相应的运算转化为量化的运算（实现 8位版本的卷积、矩阵乘法等）</p></li><li><p>删除相邻的 反量化-量化 操作</p><p>如下图</p><div class="figure"><img src="https://www.tensorflow.org/images/quantization2.png"></div></li></ol></li><li><p>测试</p><p>使用量化后的模型来预测</p></li></ul><h2 id="参考">参考</h2><ol style="list-style-type: decimal"><li><p>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1606.06160</a> <a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net" class="uri" target="_blank" rel="external">https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net</a></p></li><li><p>(ECCV 2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks <a href="https://arxiv.org/abs/1603.05279" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1603.05279</a> <a href="https://github.com/allenai/XNOR-Net" class="uri" target="_blank" rel="external">https://github.com/allenai/XNOR-Net</a></p></li><li><p>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1 <a href="https://arxiv.org/abs/1602.02830" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.02830</a> <a href="https://github.com/MatthieuCourbariaux/BinaryNet" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryNet</a></p></li><li><p>BinaryConnect: Training Deep Neural Networks with binary weights during propagations <a href="https://arxiv.org/abs/1511.00363" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1511.00363</a> <a href="https://github.com/MatthieuCourbariaux/BinaryConnect" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryConnect</a></p></li><li><p>(CVPR 2016) Quantized Convolutional Neural Networks for Mobile Devices <a href="https://github.com/jiaxiang-wu/quantized-cnn" class="uri" target="_blank" rel="external">https://github.com/jiaxiang-wu/quantized-cnn</a></p></li><li><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对神经网络的权值进行量化，使模型大小变小，运行速度变快，且准确率与原来相近。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>神经网络剪枝</title>
    <link href="http://lijiancheng0614.github.io/2017/11/12/2017_11_12_network_pruning/"/>
    <id>http://lijiancheng0614.github.io/2017/11/12/2017_11_12_network_pruning/</id>
    <published>2017-11-11T16:00:00.000Z</published>
    <updated>2018-03-13T15:31:14.301Z</updated>
    
    <content type="html"><![CDATA[<p>对神经网络（主要是CNN）进行剪枝，使模型运行速度变快，大小变小，且准确率与原来相近。</p><a id="more"></a><h2 id="如何剪枝">如何剪枝</h2><h3 id="移除滤波器">移除滤波器</h3><p>参考论文 <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>，对所有滤波器（filters）计算L1范数，移除值较小的滤波器。</p><ul><li><p>优点</p><p>模型变小，运行速度变快。</p></li><li><p>缺点</p><p>依然保留部分冗余的连接。</p></li></ul><ol style="list-style-type: decimal"><li><p>普通卷积</p><div class="figure"><img src="pruning_filters.png"></div><p>输入的特征图：<span class="math inline">\(x_i \in \mathbb{R}^{n_i \times h_i \times w_i}\)</span></p><p>输出的特征图：<span class="math inline">\(x_{i + 1} \in \mathbb{R}^{n_{i + 1} \times h_{i + 1} \times w_{i + 1}}\)</span></p><p>不考虑 bias，参数维度：<span class="math inline">\(n_i \times n_{i + 1} \times k_h \times k_w\)</span>，即有 <span class="math inline">\(n_{i + 1}\)</span> 个 3D 滤波器 <span class="math inline">\(\mathbb{F}_{i, j} \in \mathbb{R}^{n_i \times k_h \times k_w}\)</span></p><p>计算每个滤波器的 L1 值，取最小的若干个移除：<span class="math inline">\(n_{i + 1}\)</span> -&gt; <span class="math inline">\(n&#39;_{i + 1}\)</span></p><p>这会影响后续层（卷积 / 全连接 / Batch Normalization 等）的输入：</p><p>如后续卷积层的参数维度为 <span class="math inline">\(n&#39;_{i + 1} \times n_{i + 2} \times k_h \times k_w\)</span></p></li><li><p>Depthwise 卷积</p><p>Depthwise 卷积参数维度为 <span class="math inline">\(1 \times n_i \times k_h \times k_w\)</span></p><p>后续的 Pointwise 卷积参数维度为 <span class="math inline">\(n_i \times n_{i + 1} \times 1 \times 1\)</span></p><p>应与后续的 Pointwise 卷积一起计算 L1：即使用 <span class="math inline">\(dw[0, :, :, :] \cdot pw[:, i, :, :]\)</span></p></li></ol><h3 id="移除连接">移除连接</h3><p>参考论文 <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>，移除权值小于一定阈值的连接。</p><ul><li><p>优点</p><p>模型变小，运行速度变快。</p><p>能尽可能去掉冗余的连接。</p></li><li><p>缺点</p><p>需要设计更稀疏的格式保存模型，否则模型不变小也不加速。</p></li></ul><h3 id="合并-batch-normalization">合并 Batch Normalization</h3><p>Batch Normalization 的参数可以合并到上一个卷积/全连接的参数中</p><p>如设卷积的参数为 <span class="math inline">\(W\)</span>, <span class="math inline">\(b\)</span>，则卷积可表示为 <span class="math inline">\(y = Wx + b\)</span></p><p>Batch Normalization 的参数为 scale, bias, mean, variance</p><p>Batch Normalization 可表示为 <span class="math inline">\(y = \frac{scale}{\sqrt{variance + \varepsilon}} \cdot x + \left( bias - \frac{scale \cdot mean}{\sqrt{variance + \varepsilon}} \right)\)</span></p><p>Batch Normalization 的参数合并后卷积的参数为</p><p><span class="math inline">\(W&#39; = W \cdot \frac{scale}{\sqrt{variance + \varepsilon}}\)</span></p><p><span class="math inline">\(b&#39; = (b - mean) \cdot \frac{scale}{\sqrt{variance + \varepsilon}} + bias\)</span></p><h2 id="剪枝策略">剪枝策略</h2><ol style="list-style-type: decimal"><li><p>逐层剪枝比一次性剪枝效果好</p></li><li><p>每层剪枝比例应根据敏感度分析去删减</p></li><li><p>移除滤波器时，计算L1移除值较小的比随机移除、其它计算方法效果好</p></li><li><p>剪枝后进行 finetune 比 train from scratch 效果好</p></li><li><p>剪枝后固定较为敏感的层的权值再训练的效果比较好</p></li></ol><h2 id="参考">参考</h2><div class="footnotes"><hr><ol><li id="fn1"><p>(ICLR 2017) Pruning Filters for Efficient Convnets <a href="https://arxiv.org/abs/1608.08710" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1608.08710</a><a href="#fnref1">↩</a></p></li><li id="fn2"><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a><a href="#fnref2">↩</a></p></li><li id="fn3"><p>(NIPS 2015) Learning both Weights and Connections for Efficient Neural Networks <a href="https://arxiv.org/abs/1506.02626" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1506.02626</a><a href="#fnref3">↩</a></p></li></ol></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对神经网络（主要是CNN）进行剪枝，使模型运行速度变快，大小变小，且准确率与原来相近。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>神经网络模型压缩与加速</title>
    <link href="http://lijiancheng0614.github.io/2017/10/01/2017_10_01_network_compression_and_acceleration/"/>
    <id>http://lijiancheng0614.github.io/2017/10/01/2017_10_01_network_compression_and_acceleration/</id>
    <published>2017-09-30T16:00:00.000Z</published>
    <updated>2018-03-01T16:39:33.488Z</updated>
    
    <content type="html"><![CDATA[<p>介绍神经网络（主要是CNN）模型压缩与加速的常见方法</p><a id="more"></a><p>目标：模型运行速度尽可能快，大小尽可能小，准确率尽可能保持不变</p><h2 id="模型压缩">模型压缩</h2><h3 id="改变网络结构">改变网络结构</h3><h4 id="使用特定结构">使用特定结构</h4><p>如 ShuffleNet, MobileNet, Xception, SqueezeNet</p><ul><li><p>MobileNet</p><p>把普通卷积操作分成两部分</p><ul><li><p>Depthwise Convolution</p><p>计算量 <span class="math inline">\(D_K \cdot D_K \cdot M \cdot D_F \cdot D_F\)</span></p></li><li><p>Pointwise Convolution</p><p>计算量 <span class="math inline">\(M \cdot N \cdot D_F \cdot D_F\)</span></p></li></ul><p>上面两步合称Depthwise Separable Convolution</p><p>与原卷积计算量之比 <span class="math inline">\(\frac{D_K \cdot D_K \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F}{D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F} = \frac{1}{N} + \frac{1}{D_K^2}\)</span></p><div class="figure"><img src="MobileNet.png"></div></li></ul><p>参考：</p><ol style="list-style-type: decimal"><li><p>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices <a href="https://arxiv.org/abs/1707.01083" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1707.01083</a></p></li><li><p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications <a href="https://arxiv.org/abs/1704.04861" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a> <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md</a></p></li><li><p>Xception: Deep Learning with Depthwise Separable Convolutions <a href="https://arxiv.org/abs/1610.02357" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1610.02357</a></p></li><li><p>(ICLR 2017) Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters and &lt;0.5MB Model Size <a href="https://arxiv.org/abs/1602.07360" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.07360</a> <a href="https://github.com/DeepScale/SqueezeNet" class="uri" target="_blank" rel="external">https://github.com/DeepScale/SqueezeNet</a></p></li></ol><h4 id="剪枝">剪枝</h4><p>裁剪连接、滤波器</p><p>权值稀疏化</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(ICLR 2017) Pruning Filters for Efficient Convnets <a href="https://arxiv.org/abs/1608.08710" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1608.08710</a></p></li><li><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a></p></li><li><p>(NIPS 2015) Learning both Weights and Connections for Efficient Neural Networks <a href="https://arxiv.org/abs/1506.02626" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1506.02626</a></p></li></ol><h4 id="蒸馏">蒸馏</h4><p>用一个性能好的大网络来教小网络学习，使小网络具备跟大网络一样的性能，但参数规模小</p><p>训练小模型 (distilled model) 的目标函数由两部分组成：</p><ol style="list-style-type: decimal"><li>与大模型的softmax输出的交叉熵，称为软目标</li><li>与groundtruth的交叉熵</li></ol><p>训练的损失为上述两项损失的加权和</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(ICLR 2017) Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer <a href="https://arxiv.org/abs/1612.03928" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1612.03928</a></p></li><li><p>(NIPSW 2014) Distilling the Knowledge in a Neural Network <a href="https://arxiv.org/abs/1503.02531" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1503.02531</a></p></li></ol><h3 id="只改变权值">只改变权值</h3><h4 id="量化">量化</h4><p>把网络权值从高精度转化成低精度（32位浮点数 float32 转化成 8位定点数 int8 或二值化为 1 bit），但模型准确率等指标与原来相近，模型大小变小，运行速度加快。</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1606.06160</a> <a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net" class="uri" target="_blank" rel="external">https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net</a></p></li><li><p>(ECCV 2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks <a href="https://arxiv.org/abs/1603.05279" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1603.05279</a> <a href="https://github.com/allenai/XNOR-Net" class="uri" target="_blank" rel="external">https://github.com/allenai/XNOR-Net</a></p></li><li><p>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1 <a href="https://arxiv.org/abs/1602.02830" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.02830</a> <a href="https://github.com/MatthieuCourbariaux/BinaryNet" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryNet</a></p></li><li><p>BinaryConnect: Training Deep Neural Networks with binary weights during propagations <a href="https://arxiv.org/abs/1511.00363" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1511.00363</a> <a href="https://github.com/MatthieuCourbariaux/BinaryConnect" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryConnect</a></p></li><li><p>(CVPR 2016) Quantized Convolutional Neural Networks for Mobile Devices <a href="https://github.com/jiaxiang-wu/quantized-cnn" class="uri" target="_blank" rel="external">https://github.com/jiaxiang-wu/quantized-cnn</a></p></li><li><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a></p></li></ol><h4 id="矩阵分解">矩阵分解</h4><p>低秩分解（SVD分解、Tucker分解、Block Term分解）</p><p>原理：权值向量主要分布在一些低秩子空间，用少数基来重构权值矩阵</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(ICCV 2017) Coordinating Filters for Faster Deep Neural Networks <a href="https://arxiv.org/abs/1703.09746" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1703.09746</a> <a href="https://github.com/wenwei202/caffe" class="uri" target="_blank" rel="external">https://github.com/wenwei202/caffe</a></p></li><li><p>(TPAMI 2015) Accelerating Very Deep Convolutional Networks for Classification and Detection <a href="https://arxiv.org/abs/1505.06798" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1505.06798</a></p></li><li><p>(NIPS 2014) Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation <a href="https://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation" class="uri" target="_blank" rel="external">https://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation</a></p></li></ol><h2 id="平台加速">平台加速</h2><h3 id="软件加速">软件加速</h3><h4 id="卷积计算加速">卷积计算加速</h4><ul><li><p>im2col + GEMM：将问题转化为矩阵乘法后使用矩阵运算库</p><p>参考：</p><ol style="list-style-type: decimal"><li>(ICML 2017) MEC: Memory-efficient Convolution for Deep Neural Network <a href="https://arxiv.org/abs/1706.06873" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1706.06873</a></li></ol></li><li><p>FFT变换：时域卷积等于频域相乘，将问题转化为简单的乘法问题</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(BMVC 2015) Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add <a href="https://arxiv.org/abs/1601.06815" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1601.06815</a></p></li><li><p>(ICLR 2015) Fast Convolutional Nets With fbfft: A GPU Performance Evaluation <a href="https://arxiv.org/abs/1412.7580" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1412.7580</a></p></li><li><p>Fast Training of Convolutional Networks through FFTs <a href="https://arxiv.org/abs/1312.5851" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1312.5851</a></p></li></ol></li><li><p>Winograd</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(CODES 2016) Zero and data reuse-aware fast convolution for deep neural networks on gpu</p></li><li><p>(CVPR 2016) Fast Algorithms for Convolutional Neural Networks <a href="https://arxiv.org/abs/1509.09308" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1509.09308</a></p></li></ol></li></ul><h4 id="不同框架速度不同">不同框架速度不同</h4><ol style="list-style-type: decimal"><li>TensorFlow Mobile, TensorFlow Lite <a href="https://www.tensorflow.org/mobile/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/mobile/</a></li><li>Caffe2 <a href="https://caffe2.ai/" class="uri" target="_blank" rel="external">https://caffe2.ai/</a></li><li>腾讯ncnn（不依赖 BLAS/NNPACK 等计算框架，NEON优化，多核并行） <a href="https://github.com/Tencent/ncnn/" class="uri" target="_blank" rel="external">https://github.com/Tencent/ncnn/</a></li><li>百度mdl（无任何第三方依赖，汇编优化，NEON优化） <a href="https://github.com/baidu/mobile-deep-learning" class="uri" target="_blank" rel="external">https://github.com/baidu/mobile-deep-learning</a></li></ol><h3 id="硬件加速">硬件加速</h3><ol style="list-style-type: decimal"><li><p>多GPU并行</p></li><li><p>多核并行</p></li><li><p>使用硬件提供最优的指令</p><p>如支持arm64则编译64位的库而非32位的库</p><p>如使用支持相应 SIMD (Single Instruction, Multiple Data) 的库</p></li></ol><p>参考：</p><ol style="list-style-type: decimal"><li>(ISCA 2016) EIE: Efficient Inference Engine on Compressed Deep Neural Network <a href="https://arxiv.org/abs/1602.01528" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.01528</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍神经网络（主要是CNN）模型压缩与加速的常见方法&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>修改TensorFlow Object Detection API</title>
    <link href="http://lijiancheng0614.github.io/2017/09/07/2017_09_07_TensorFlow-Object-Detection-API/"/>
    <id>http://lijiancheng0614.github.io/2017/09/07/2017_09_07_TensorFlow-Object-Detection-API/</id>
    <published>2017-09-06T16:00:00.000Z</published>
    <updated>2017-09-09T12:47:57.809Z</updated>
    
    <content type="html"><![CDATA[<p>代码仓库：<a href="https://github.com/lijiancheng0614/tensorflow_object_detection" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/tensorflow_object_detection</a></p><p>修改TensorFlow Object Detection API，添加一些方便使用或新的功能。</p><a id="more"></a><h2 id="中文">中文</h2><p>使用时记得添加PYTHONPATH：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><p>以及编译protobuf：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure><h3 id="在train_config中添加from_detection_checkpoint参数">在<code>train_config</code>中添加<code>from_detection_checkpoint</code>参数</h3><p>增加加载模型所有参数和不加载模型的模式。</p><p>原来：</p><p><code>from_detection_checkpoint</code>为<code>bool</code>型：</p><ul><li><p>True: 加载detection模型参数中FeatureExtractor部分。</p></li><li><p>False: 加载classification模型参数。</p></li></ul><p>现在：</p><p><code>from_detection_checkpoint</code>为<code>uint32</code>型：</p><ul><li><p>3: 不加载模型参数（适用于train from scratch的情况。）</p></li><li><p>2: 加载模型所有参数（适用于训练中断后重新加载checkpoint的情况。）</p></li><li><p>1: 加载detection模型参数中FeatureExtractor部分。</p></li><li><p>0: 加载classification模型参数。</p></li></ul><p>修改文件：</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>core/model.py</code></p></li><li><p><code>protos/train.proto</code></p></li><li><p><code>meta_architectures/ssd_meta_arch.py</code></p></li><li><p><code>meta_architectures/faster_rcnn_meta_arch.py</code></p></li></ul><h3 id="更新训练时的summary">更新训练时的summary</h3><p>把histograms和first_clone_scope等summaries去掉。这样训练的event文件变小，方便tensorboard加载。</p><p>修改文件：</p><ul><li><code>trainer.py</code></li></ul><h3 id="在eval.py中添加gpu_allow_growth参数">在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="在train.py中添加gpu_allow_growth参数">在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="在train.config中添加max_to_keep参数">在<code>train.config</code>中添加<code>max_to_keep</code>参数</h3><p>在<code>train_config</code>中添加<code>max_to_keep</code>参数，默认为<code>5</code>，即保留最后5个checkpoint。如为<code>0</code>则保留所有的checkpoint。</p><p>修改文件：</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>train.proto</code></p></li></ul><h3 id="网络添加focalsigmoidclassificationloss">网络添加<code>FocalSigmoidClassificationLoss</code></h3><p><code>model</code>中的<code>loss</code>中的<code>classification_loss</code>可使用<code>focal_sigmoid</code>。</p><p>关于Focal loss，参考 <a href="https://arxiv.org/pdf/1708.02002.pdf" class="uri" target="_blank" rel="external">https://arxiv.org/pdf/1708.02002.pdf</a></p><p>修改文件：</p><ul><li><p><code>core/losses.py</code></p></li><li><p><code>builders/losses_builder.py</code></p></li><li><p><code>protos/losses.proto</code></p></li></ul><h2 id="english">English</h2><p>Remember update PYTHONPATH:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><p>And compile protobuf:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure><h3 id="add-from_detection_checkpoint-parameter-in-train_config">Add <code>from_detection_checkpoint</code> parameter in <code>train_config</code></h3><p>Old:</p><p><code>bool from_detection_checkpoint</code></p><ul><li><p>True: the checkpoint was an object detection model that have the same parameters with the exception of the num_classes parameter.</p></li><li><p>False: the checkpoint was a object classification model.</p></li></ul><p>New:</p><p><code>uint32 from_detection_checkpoint</code></p><ul><li><p>3: don’t load any variables.</p></li><li><p>2: load all variables.</p></li><li><p>1: load feature extractor variables from an object detection model, same as <code>True</code>.</p></li><li><p>0: load feature extractor variables from a object classification model, same as <code>False</code>.</p></li></ul><p>Modified files:</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>core/model.py</code></p></li><li><p><code>protos/train.proto</code></p></li><li><p><code>meta_architectures/ssd_meta_arch.py</code></p></li><li><p><code>meta_architectures/faster_rcnn_meta_arch.py</code></p></li></ul><h3 id="remove-some-summaries-when-training">Remove some summaries when training</h3><p>Remove summaries about histograms and first_clone_scope when training.</p><p>Modified files:</p><ul><li><code>trainer.py</code></li></ul><h3 id="add-gpu_allow_growth-parameter-in-eval.py">Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="add-gpu_allow_growth-parameter-in-train.py">Add <code>gpu_allow_growth</code> parameter in <code>train.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>train.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="add-max_to_keep-parameter-in-train_config">Add <code>max_to_keep</code> parameter in <code>train_config</code></h3><p>Add <code>max_to_keep</code> parameter in <code>train_config</code>, default value is <code>5</code> which means the 5 most recent checkpoint files are kept. If <code>0</code>, all checkpoint files are kept.</p><p>Modified files:</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>protos/train.proto</code></p></li></ul><h3 id="add-focalsigmoidclassificationloss-in-model">Add <code>FocalSigmoidClassificationLoss</code> in <code>model</code></h3><p>In config, <code>model</code> -&gt; <code>loss</code> -&gt; <code>classification_loss</code> can be <code>focal_sigmoid</code>, parameters: anchorwise_output, gamma.</p><p>Reference: <a href="https://arxiv.org/pdf/1708.02002.pdf" class="uri" target="_blank" rel="external">https://arxiv.org/pdf/1708.02002.pdf</a></p><p>Modified files:</p><ul><li><p><code>core/losses.py</code></p></li><li><p><code>builders/losses_builder.py</code></p></li><li><p><code>protos/losses.proto</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码仓库：&lt;a href=&quot;https://github.com/lijiancheng0614/tensorflow_object_detection&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lijiancheng0614/tensorflow_object_detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;修改TensorFlow Object Detection API，添加一些方便使用或新的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow在Android上进行物体检测</title>
    <link href="http://lijiancheng0614.github.io/2017/09/01/2017_09_01_TensorFlow-Android-Detection/"/>
    <id>http://lijiancheng0614.github.io/2017/09/01/2017_09_01_TensorFlow-Android-Detection/</id>
    <published>2017-08-31T16:00:00.000Z</published>
    <updated>2017-09-09T12:43:10.660Z</updated>
    
    <content type="html"><![CDATA[<p>使用TensorFlow Android Inference Interface在Android上进行图像物体检测</p><p>不支持Camera2 API的手机也可以物体检测：<a href="https://github.com/lijiancheng0614/android-TFDetect" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/android-TFDetect</a></p><a id="more"></a><h2 id="从tensorflow源代码编译tensorflow-android-camera-demo">从TensorFlow源代码编译TensorFlow Android Camera Demo</h2><p>参考 <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/" class="uri" target="_blank" rel="external">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/</a></p><h3 id="使用bazel编译">使用Bazel编译</h3><ol style="list-style-type: decimal"><li><p>下载TensorFlow源代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> --recurse-submodules https://github.com/tensorflow/tensorflow.git</div></pre></td></tr></table></figure></li><li><p>准备环境</p><ul><li><p>安装Bazel</p></li><li><p>安装Android NDK，版本应与Bazel配合，见官网对应的版本</p></li><li><p>安装Android SDK</p></li><li><p>编辑<code>tensorflow/WORKSPACE</code>中NDK和SDK的路径</p></li></ul></li><li><p>编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow</span></div><div class="line">bazel build -c opt //tensorflow/examples/android:tensorflow_demo</div></pre></td></tr></table></figure></li><li><p>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk</div></pre></td></tr></table></figure></li></ol><h3 id="使用android-studio和bazel编译">使用Android Studio和Bazel编译</h3><p>修改<code>tensorflow/examples/android/build.gradle</code>中Bazel的路径，直接用Android Studio导入<code>tensorflow/examples/android/</code>目录作为新的Android Studio项目。</p><h2 id="不支持camera2-api的手机">不支持Camera2 API的手机</h2><p>由于部分手机不支持Camera2 API，故需要把调用Camera2 API的代码去掉。</p><p>具体来说，把tracking部分的代码删掉，只做object detection，并更新相应画bounding box的代码，这样速度也有所加快。</p><p>详细代码见：<a href="https://github.com/lijiancheng0614/android-TFDetect" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/android-TFDetect</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用TensorFlow Android Inference Interface在Android上进行图像物体检测&lt;/p&gt;
&lt;p&gt;不支持Camera2 API的手机也可以物体检测：&lt;a href=&quot;https://github.com/lijiancheng0614/android-TFDetect&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lijiancheng0614/android-TFDetect&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Android" scheme="http://lijiancheng0614.github.io/tags/Android/"/>
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>用VPS搭建IPv6的VPN</title>
    <link href="http://lijiancheng0614.github.io/2017/08/27/2017_08_27_vps_ipv6/"/>
    <id>http://lijiancheng0614.github.io/2017/08/27/2017_08_27_vps_ipv6/</id>
    <published>2017-08-26T16:00:00.000Z</published>
    <updated>2017-09-08T16:51:42.648Z</updated>
    
    <content type="html"><![CDATA[<p>使用tunnelbroker为VPS创建IPv6隧道。</p><p>使用shadowsocks建立IPv6的VPN。</p><a id="more"></a><p>一般VPS没有IPv6地址，如想有IPv6地址，则可以为VPS创建IPv6隧道，只需要有公网IPv4地址即可。因此其它主机也可以创建隧道。</p><h2 id="隧道申请">隧道申请</h2><p>选择一个隧道服务的提供商，如 <a href="https://www.tunnelbroker.net/" class="uri" target="_blank" rel="external">https://www.tunnelbroker.net/</a></p><p>注册账号并登陆，点击<code>Create Regular Tunnel</code>，在<code>IPv4 Endpoint (Your side)</code>中填入VPS的IPv4地址，在<code>Available Tunnel Servers</code>中选一个较近的Server（这样延迟较小），最后点击<code>Create Tunnel</code>。</p><p>创建完成后，在<code>Main Page</code>中会显示所创建的隧道，点击进入详情。</p><p><code>IPv6 Tunnel</code>即可看到相关信息，点击旁边<code>Example Configurations</code>即可看到不同操作系统的配置。</p><h2 id="vps配置">VPS配置</h2><p>根据VPS的系统执行<code>Example Configurations</code>中的命令，一般就完成了。这里选取几种常见的系统的命令。</p><ul><li><p>Debian/Ubuntu:</p><p>把以下文本添加到<code>/etc/network/interfaces</code>中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">auto he-ipv6</div><div class="line">iface he-ipv6 inet6 v4tunnel</div><div class="line">    address 2001:470:XXX:XXX::2</div><div class="line">    netmask 64</div><div class="line">    endpoint 216.218.221.6</div><div class="line">    local XXX.XXX.XXX.XXX</div><div class="line">    ttl 255</div><div class="line">    gateway 2001:470:XXX:XXX::1</div></pre></td></tr></table></figure></li><li><p>Linux-route2:</p><p>在command中运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">modprobe ipv6</div><div class="line">ip tunnel add he-ipv6 mode sit remote 216.218.221.6 <span class="built_in">local</span> XXX.XXX.XXX.XXX ttl 255</div><div class="line">ip link <span class="built_in">set</span> he-ipv6 up</div><div class="line">ip addr add 2001:470:XXX:XXX::2/64 dev he-ipv6</div><div class="line">ip route add ::/0 dev he-ipv6</div><div class="line">ip -f inet6 addr</div></pre></td></tr></table></figure></li><li><p>Mac OS X:</p><p>在command中运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ifconfig gif0 create</div><div class="line">ifconfig gif0 tunnel XXX.XXX.XXX.XXX 216.218.221.6</div><div class="line">ifconfig gif0 inet6 2001:470:XXX:XXX::2 2001:470:XXX:XXX::1 prefixlen 128</div><div class="line">route -n add -inet6 default 2001:470:XXX:XXX::1</div></pre></td></tr></table></figure></li><li><p>Windows 10:</p><p>在命令行中运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">netsh interface teredo set state disabled</div><div class="line">netsh interface ipv6 add v6v4tunnel interface=IP6Tunnel localaddress=XXX.XXX.XXX.XXX remoteaddress=216.218.221.6</div><div class="line">netsh interface ipv6 add address interface=IP6Tunnel address=2001:470:XXX:XXX::2</div><div class="line">netsh interface ipv6 add route prefix=::/0 interface=IP6Tunnel nexthop=2001:470:XXX:XXX::1</div></pre></td></tr></table></figure><p>可能还需要添加IPv6 DNS服务器，在“网络和共享中心”中，选择其中一个网络，修改属性中的“Internet 协议版本 6（TCP/IPv6）”属性，添加DNS服务器，如<code>2001:470:20::2</code>。</p></li></ul><h2 id="查看ipv6地址">查看IPv6地址</h2><p>根据操作系统，运行<code>ifconfig</code>或<code>ipconfig</code>即可看到IPv6的网络接口，尝试ping或<code>ping6 ipv6.google.com</code>如果成功即创建IPv6隧道成功。</p><p>这样，可以在VPS中访问IPv6的资源。</p><h2 id="搭建ipv6的vpn">搭建IPv6的VPN</h2><p>配置VPN有多种方法，这里以shadowsocks为例。</p><ul><li><p>安装shadowsocks服务端</p><p>根据操作系统不同可能有不同的安装方法，这里以python版为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install shadowsocks</div></pre></td></tr></table></figure></li><li><p>编辑配置文件，如<code>ss-conf-ipv6.json</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;server&quot;:&quot;::&quot;,</div><div class="line">    &quot;server_port&quot;:8388,</div><div class="line">    &quot;local_address&quot;:&quot;127.0.0.1&quot;,</div><div class="line">    &quot;local_port&quot;:1080,</div><div class="line">    &quot;password&quot;:&quot;$PASSWORD&quot;,</div><div class="line">    &quot;timeout&quot;:600,</div><div class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><code>$PASSWORD</code>填入自定义的密码，<code>server</code>不是IPv4时的<code>0.0.0.0</code>而是<code>::</code>，其它参数自行修改。</p></li><li><p>启动shadowsocks</p><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssserver -c ss-conf-ipv6.json -d start</div></pre></td></tr></table></figure></li><li><p>关闭shadowsocks</p><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssserver -d stop</div></pre></td></tr></table></figure></li><li><p>在客户端中运行shadowsocks</p><p>在主机、平板、手机等上安装相应的shadowsocks客户端并填入相关配置即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Server Addr: 2001:470:xxx:xxx::2</div><div class="line">Server Port: 8388</div><div class="line">Password: $PASSWORD</div><div class="line">Encryption: aes-256-cfb</div><div class="line">Proxy Port: 1080</div></pre></td></tr></table></figure><p>这样，客户端可以通过IPv6连接VPN，再通过VPS访问IPv4的资源。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用tunnelbroker为VPS创建IPv6隧道。&lt;/p&gt;
&lt;p&gt;使用shadowsocks建立IPv6的VPN。&lt;/p&gt;
    
    </summary>
    
      <category term="Default" scheme="http://lijiancheng0614.github.io/categories/Default/"/>
    
    
      <category term="VPS" scheme="http://lijiancheng0614.github.io/tags/VPS/"/>
    
      <category term="IPv6" scheme="http://lijiancheng0614.github.io/tags/IPv6/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow Object Detection API进行物体检测</title>
    <link href="http://lijiancheng0614.github.io/2017/08/22/2017_08_22_TensorFlow-Object-Detection-API/"/>
    <id>http://lijiancheng0614.github.io/2017/08/22/2017_08_22_TensorFlow-Object-Detection-API/</id>
    <published>2017-08-21T16:00:00.000Z</published>
    <updated>2017-10-06T12:14:37.165Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/object_detection" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/object_detection</a></p><p>使用TensorFlow Object Detection API进行物体检测</p><a id="more"></a><div class="figure"><img src="https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/g3doc/img/kites_detections_output.jpg"></div><h2 id="准备">准备</h2><ol start="0" style="list-style-type: decimal"><li><p>文件结构</p><p>为了方便查看文件，使用以下文件结构。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">object_detection</div><div class="line">├── checkpoints</div><div class="line">│   └── ssd_mobilenet_v1_coco_11_06_2017</div><div class="line">│       ├── frozen_inference_graph.pb</div><div class="line">│       ├── graph.pbtxt</div><div class="line">│       ├── model.ckpt.data-00000-of-00001</div><div class="line">│       ├── model.ckpt.index</div><div class="line">│       └── model.ckpt.meta</div><div class="line">└── VOC2012</div><div class="line">    ├── data</div><div class="line">    │   ├── VOCdevkit</div><div class="line">    │   │   └── ...</div><div class="line">    │   ├── pascal_label_map.pbtxt</div><div class="line">    │   ├── pascal_train.record</div><div class="line">    │   └── pascal_val.record</div><div class="line">    ├── ssd_mobilenet</div><div class="line">    │   ├── eval_logs</div><div class="line">    │   │   ├── ...</div><div class="line">    │   │   └── events.out.tfevents....</div><div class="line">    │   ├── logs</div><div class="line">    │   │   ├── ...</div><div class="line">    │   │   └── train_....txt</div><div class="line">    │   ├── output</div><div class="line">    │   │   ├── saved_model</div><div class="line">    │   │   │   ├── variables</div><div class="line">    │   │   │   └── saved_model.pb</div><div class="line">    │   │   ├── checkpoint</div><div class="line">    │   │   ├── frozen_inference_graph.pb</div><div class="line">    │   │   ├── model.ckpt.data-00000-of-00001</div><div class="line">    │   │   ├── model.ckpt.index</div><div class="line">    │   │   └── model.ckpt.meta</div><div class="line">    │   ├── train_logs</div><div class="line">    │   │   ├── ...</div><div class="line">    │   │   ├── model.ckpt-0.data-00000-of-00001</div><div class="line">    │   │   ├── model.ckpt-0.index</div><div class="line">    │   │   └── model.ckpt-0.meta</div><div class="line">    │   ├── eval.sh</div><div class="line">    │   ├── ssd_mobilenet_v1.config</div><div class="line">    │   └── train.sh</div><div class="line">    ├── create_pascal_tf_record.py</div><div class="line">    ├── eval.py</div><div class="line">    ├── infer.py</div><div class="line">    └── test.py</div></pre></td></tr></table></figure></li><li><p>安装TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/install/</a></p><p>如在Ubuntu下安装TensorFlow with GPU support, python 2.7版本的TensorFlow 1.2.0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div><div class="line">pip install tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div></pre></td></tr></table></figure></li><li><p>配置TensorFlow Models</p><ul><li>下载TensorFlow Models</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div></pre></td></tr></table></figure><ul><li>编译protobuf</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure><p>生成若干py文件在<code>object_detection/protos/</code>。</p><ul><li>添加PYTHONPATH</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">python object_detection/builders/model_builder_test.py</div></pre></td></tr></table></figure><p>若成功，显示<code>OK</code>。</p></li><li><p>准备数据</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/preparing_inputs.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/preparing_inputs.md</a></p><p>这里以<code>PASCAL VOC 2012</code>为例。</p><ul><li>下载并解压</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection</span></div><div class="line">mkdir -p VOC2012/data</div><div class="line"><span class="built_in">cd</span> VOC2012/data</div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</div><div class="line">tar -xvf VOCtrainval_11-May-2012.tar</div></pre></td></tr></table></figure><ul><li>生成TFRecord</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/</span></div><div class="line">cp ../../create_pascal_tf_record.py .</div><div class="line">cp ../../data/pascal_label_map.pbtxt data/</div><div class="line">python create_pascal_tf_record.py \</div><div class="line">    --label_map_path=data/pascal_label_map.pbtxt \</div><div class="line">    --data_dir=data/VOCdevkit --year=VOC2012 --<span class="built_in">set</span>=train \</div><div class="line">    --output_path=data/pascal_train.record</div><div class="line">python create_pascal_tf_record.py \</div><div class="line">    --label_map_path=data/pascal_label_map.pbtxt \</div><div class="line">    --data_dir=data/VOCdevkit --year=VOC2012 --<span class="built_in">set</span>=val \</div><div class="line">    --output_path=data/pascal_val.record</div></pre></td></tr></table></figure><p>得到<code>data/pascal_train.record</code>和<code>data/pascal_val.record</code>。</p><p>如果需要用自己的数据，则参考<code>create_pascal_tf_record.py</code>编写处理数据生成TFRecord的脚本。可参考 <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md</a></p></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型（ <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md</a> ），这里以<code>ssd_mobilenet_v1_coco</code>以例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/</span></div><div class="line">mkdir checkpoints</div><div class="line"><span class="built_in">cd</span> checkpoints</div><div class="line">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz</div><div class="line">tar zxf ssd_mobilenet_v1_coco_11_06_2017.tar.gz</div></pre></td></tr></table></figure></li></ol><h2 id="训练">训练</h2><p>如果使用现有模型进行预测则不需要训练。</p><ol style="list-style-type: decimal"><li><p>配置</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/configuring_jobs.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/configuring_jobs.md</a></p><p>这里使用SSD with MobileNet，把<code>object_detection/samples/configs/ssd_mobilenet_v1_pets.config</code>复制到<code>object_detection/VOC2012/ssd_mobilenet/ssd_mobilenet_v1.config</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012</span></div><div class="line">mkdir ssd_mobilenet</div><div class="line">cp ../samples/configs/ssd_mobilenet_v1_pets.config ssd_mobilenet/ssd_mobilenet_v1.config</div></pre></td></tr></table></figure><p>并进行相应的修改：</p><p>修改第9行为<code>num_classes: 20</code>。</p><p>修改第158行为<code>fine_tune_checkpoint: &quot;../../checkpoints/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt&quot;</code></p><p>修改第177行为<code>input_path: &quot;../data/pascal_train.record&quot;</code></p><p>修改第179行和193行为<code>label_map_path: &quot;../data/pascal_label_map.pbtxt&quot;</code></p><p>修改第191行为<code>input_path: &quot;../data/pascal_val.record&quot;</code></p></li><li><p>训练</p><p>新建<code>object_detection/VOC2012/ssd_mobilenet/train.sh</code>，内容以下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p logs/</div><div class="line">now=$(date +<span class="string">"%Y%m%d_%H%M%S"</span>)</div><div class="line">python ../../train.py \</div><div class="line">    --logtostderr \</div><div class="line">    --pipeline_config_path=ssd_mobilenet_v1.config \</div><div class="line">    --train_dir=train_logs 2&gt;&amp;1 | tee logs/train_<span class="variable">$now</span>.txt &amp;</div></pre></td></tr></table></figure><p>进入<code>object_detection/VOC2012/ssd_mobilenet/</code>，运行<code>./train.sh</code>即可训练。</p></li><li><p>验证</p><p>可一边训练一边验证，注意使用其它的GPU或合理分配显存。</p><p>新建<code>object_detection/VOC2012/ssd_mobilenet/eval.sh</code>，内容以下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p eval_logs</div><div class="line">python ../../eval.py \</div><div class="line">    --logtostderr \</div><div class="line">    --pipeline_config_path=ssd_mobilenet_v1.config \</div><div class="line">    --checkpoint_dir=train_logs \</div><div class="line">    --eval_dir=eval_logs &amp;</div></pre></td></tr></table></figure><p>进入<code>object_detection/VOC2012/ssd_mobilenet/</code>，运行<code>CUDA_VISIBLE_DEVICES=&quot;1&quot; ./eval.sh</code>即可验证（这里指定了第二个GPU）。</p></li><li><p>可视化log</p><p>可一边训练一边可视化训练的log，访问<code>http://localhost:6006/</code>即可看到Loss等的变化。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">tensorboard --logdir train_logs/</div></pre></td></tr></table></figure><p>可视化验证的log，可看到<code>Precision/mAP@0.5IOU</code>的变化以及具体image的预测结果，这里指定了另一个端口。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">tensorboard --logdir eval_logs/ --port 6007</div></pre></td></tr></table></figure><p>或同时可视化训练与验证的log：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">tensorboard --logdir .</div></pre></td></tr></table></figure></li></ol><h2 id="测试">测试</h2><ol style="list-style-type: decimal"><li><p>导出模型</p><p>训练完成后得到一些checkpoint文件在<code>tensorflow/models/object_detection/VOC2012/ssd_mobilenet/train_logs/</code>中，如：</p><ul><li>graph.pbtxt</li><li>model.ckpt-200000.data-00000-of-00001</li><li>model.ckpt-200000.info</li><li>model.ckpt-200000.meta</li></ul><p>其中meta保存了graph和metadata，ckpt保存了网络的weights。</p><p>而进行预测时只需模型和权重，不需要metadata，故可使用官方提供的脚本生成推导图。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">mkdir -p output</div><div class="line">CUDA_VISIBLE_DEVICES=<span class="string">"1"</span> python ../../export_inference_graph.py \</div><div class="line">    --input_type image_tensor \</div><div class="line">    --pipeline_config_path ssd_mobilenet_v1.config \</div><div class="line">    --trained_checkpoint_prefix train_logs/model.ckpt-200000 \</div><div class="line">    --output_directory output/</div></pre></td></tr></table></figure></li><li><p>测试图片</p><ul><li><p>运行<code>object_detection_tutorial.ipynb</code>并修改其中的各种路径即可。</p></li><li><p>或自写inference脚本，如<code>tensorflow/models/object_detection/VOC2012/infer.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">'..'</span>)</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> label_map_util</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> visualization_utils <span class="keyword">as</span> vis_util</div><div class="line"></div><div class="line"><span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</div><div class="line">    print(<span class="string">'Usage: python &#123;&#125; test_image_path checkpoint_path'</span>.format(sys.argv[<span class="number">0</span>]))</div><div class="line">    exit()</div><div class="line"></div><div class="line">PATH_TEST_IMAGE = sys.argv[<span class="number">1</span>]</div><div class="line">PATH_TO_CKPT = sys.argv[<span class="number">2</span>]</div><div class="line">PATH_TO_LABELS = <span class="string">'data/pascal_label_map.pbtxt'</span></div><div class="line">NUM_CLASSES = <span class="number">21</span></div><div class="line">IMAGE_SIZE = (<span class="number">18</span>, <span class="number">12</span>)</div><div class="line"></div><div class="line">label_map = label_map_util.load_labelmap(PATH_TO_LABELS)</div><div class="line">categories = label_map_util.convert_label_map_to_categories(</div><div class="line">    label_map, max_num_classes=NUM_CLASSES, use_display_name=<span class="keyword">True</span>)</div><div class="line">category_index = label_map_util.create_category_index(categories)</div><div class="line"></div><div class="line">detection_graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    od_graph_def = tf.GraphDef()</div><div class="line">    <span class="keyword">with</span> tf.gfile.GFile(PATH_TO_CKPT, <span class="string">'rb'</span>) <span class="keyword">as</span> fid:</div><div class="line">        serialized_graph = fid.read()</div><div class="line">        od_graph_def.ParseFromString(serialized_graph)</div><div class="line">        tf.import_graph_def(od_graph_def, name=<span class="string">''</span>)</div><div class="line"></div><div class="line">config = tf.ConfigProto()</div><div class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    <span class="keyword">with</span> tf.Session(graph=detection_graph, config=config) <span class="keyword">as</span> sess:</div><div class="line">        start_time = time.time()</div><div class="line">        print(time.ctime())</div><div class="line">        image = Image.open(PATH_TEST_IMAGE)</div><div class="line">        image_np = np.array(image).astype(np.uint8)</div><div class="line">        image_np_expanded = np.expand_dims(image_np, axis=<span class="number">0</span>)</div><div class="line">        image_tensor = detection_graph.get_tensor_by_name(<span class="string">'image_tensor:0'</span>)</div><div class="line">        boxes = detection_graph.get_tensor_by_name(<span class="string">'detection_boxes:0'</span>)</div><div class="line">        scores = detection_graph.get_tensor_by_name(<span class="string">'detection_scores:0'</span>)</div><div class="line">        classes = detection_graph.get_tensor_by_name(<span class="string">'detection_classes:0'</span>)</div><div class="line">        num_detections = detection_graph.get_tensor_by_name(<span class="string">'num_detections:0'</span>)</div><div class="line">        (boxes, scores, classes, num_detections) = sess.run(</div><div class="line">            [boxes, scores, classes, num_detections],</div><div class="line">            feed_dict=&#123;image_tensor: image_np_expanded&#125;)</div><div class="line">        print(<span class="string">'&#123;&#125; elapsed time: &#123;:.3f&#125;s'</span>.format(time.ctime(), time.time() - start_time))</div><div class="line">        vis_util.visualize_boxes_and_labels_on_image_array(</div><div class="line">            image_np, np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores),</div><div class="line">            category_index, use_normalized_coordinates=<span class="keyword">True</span>, line_thickness=<span class="number">8</span>)</div><div class="line">        plt.figure(figsize=IMAGE_SIZE)</div><div class="line">        plt.imshow(image_np)</div></pre></td></tr></table></figure><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/</span></div><div class="line">python infer.py \</div><div class="line">    ../test_images/image1.jpg \</div><div class="line">    ssd_mobilenet/output/frozen_inference_graph.pb</div></pre></td></tr></table></figure></li></ul></li><li><p>批量测试</p><ul><li><p>运行<code>./eval.sh</code>即可测试<code>train_logs/</code>中<code>train_logs/checkpoint</code>里包含的checkpoint并计算<code>Precision/mAP@0.5IOU</code>。</p></li><li><p>或自写脚本，对每张图片进行预测并把分类得分大于某个阈值的预测结果保存到json文件中，如<code>tensorflow/models/object_detection/VOC2012/test.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">'..'</span>)</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> label_map_util</div><div class="line"></div><div class="line"><span class="keyword">if</span> len(sys.argv) &lt; <span class="number">5</span>:</div><div class="line">    print(<span class="string">'Usage: python &#123;&#125; output_json_path checkpoint_path test_ids_path image_dir'</span>.format(sys.argv[<span class="number">0</span>]))</div><div class="line">    exit()</div><div class="line"></div><div class="line">PATH_OUTPUT = sys.argv[<span class="number">1</span>]</div><div class="line">PATH_TO_CKPT = sys.argv[<span class="number">2</span>]</div><div class="line">PATH_TEST_IDS = sys.argv[<span class="number">3</span>]</div><div class="line">DIR_IMAGE = sys.argv[<span class="number">4</span>]</div><div class="line">PATH_TO_LABELS = <span class="string">'data/pascal_label_map.pbtxt'</span></div><div class="line">NUM_CLASSES = <span class="number">21</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_results</span><span class="params">(boxes, classes, scores, category_index, im_width, im_height,</span></span></div><div class="line"><span class="function"><span class="params">    min_score_thresh=<span class="number">.5</span>)</span>:</span></div><div class="line">    bboxes = list()</div><div class="line">    <span class="keyword">for</span> i, box <span class="keyword">in</span> enumerate(boxes):</div><div class="line">        <span class="keyword">if</span> scores[i] &gt; min_score_thresh:</div><div class="line">            ymin, xmin, ymax, xmax = box</div><div class="line">            bbox = &#123;</div><div class="line">                <span class="string">'bbox'</span>: &#123;</div><div class="line">                    <span class="string">'xmax'</span>: xmax * im_width,</div><div class="line">                    <span class="string">'xmin'</span>: xmin * im_width,</div><div class="line">                    <span class="string">'ymax'</span>: ymax * im_height,</div><div class="line">                    <span class="string">'ymin'</span>: ymin * im_height</div><div class="line">                &#125;,</div><div class="line">                <span class="string">'category'</span>: category_index[classes[i]][<span class="string">'name'</span>],</div><div class="line">                <span class="string">'score'</span>: float(scores[i])</div><div class="line">            &#125;</div><div class="line">            bboxes.append(bbox)</div><div class="line">    <span class="keyword">return</span> bboxes</div><div class="line"></div><div class="line">label_map = label_map_util.load_labelmap(PATH_TO_LABELS)</div><div class="line">categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=<span class="keyword">True</span>)</div><div class="line">category_index = label_map_util.create_category_index(categories)</div><div class="line"></div><div class="line">detection_graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    od_graph_def = tf.GraphDef()</div><div class="line">    <span class="keyword">with</span> tf.gfile.GFile(PATH_TO_CKPT, <span class="string">'rb'</span>) <span class="keyword">as</span> fid:</div><div class="line">        serialized_graph = fid.read()</div><div class="line">        od_graph_def.ParseFromString(serialized_graph)</div><div class="line">        tf.import_graph_def(od_graph_def, name=<span class="string">''</span>)</div><div class="line"></div><div class="line">config = tf.ConfigProto()</div><div class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line"></div><div class="line">test_ids = [line.split()[<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> open(PATH_TEST_IDS)]</div><div class="line">total_time = <span class="number">0</span></div><div class="line">test_annos = dict()</div><div class="line">flag = <span class="keyword">False</span></div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    <span class="keyword">with</span> tf.Session(graph=detection_graph, config=config) <span class="keyword">as</span> sess:</div><div class="line">        <span class="keyword">for</span> image_id <span class="keyword">in</span> test_ids:</div><div class="line">            image_path = os.path.join(DIR_IMAGE, image_id + <span class="string">'.jpg'</span>)</div><div class="line">            image = Image.open(image_path)</div><div class="line">            image_np = np.array(image).astype(np.uint8)</div><div class="line">            im_width, im_height, _ = image_np.shape</div><div class="line">            image_np_expanded = np.expand_dims(image_np, axis=<span class="number">0</span>)</div><div class="line">            image_tensor = detection_graph.get_tensor_by_name(<span class="string">'image_tensor:0'</span>)</div><div class="line">            boxes = detection_graph.get_tensor_by_name(<span class="string">'detection_boxes:0'</span>)</div><div class="line">            scores = detection_graph.get_tensor_by_name(<span class="string">'detection_scores:0'</span>)</div><div class="line">            classes = detection_graph.get_tensor_by_name(<span class="string">'detection_classes:0'</span>)</div><div class="line">            num_detections = detection_graph.get_tensor_by_name(<span class="string">'num_detections:0'</span>)</div><div class="line">            start_time = time.time()</div><div class="line">            (boxes, scores, classes, num_detections) = sess.run(</div><div class="line">                [boxes, scores, classes, num_detections],</div><div class="line">                feed_dict=&#123;image_tensor: image_np_expanded&#125;)</div><div class="line">            end_time = time.time()</div><div class="line">            print(<span class="string">'&#123;&#125; &#123;&#125; &#123;:.3f&#125;s'</span>.format(time.ctime(), image_id, end_time - start_time))</div><div class="line">            <span class="keyword">if</span> flag:</div><div class="line">                total_time += end_time - start_time</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                flag = <span class="keyword">True</span></div><div class="line">            test_annos[image_id] = &#123;<span class="string">'objects'</span>: get_results(</div><div class="line">                np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores), category_index,</div><div class="line">                im_width, im_height)&#125;</div><div class="line"></div><div class="line">print(<span class="string">'total time: &#123;&#125;, total images: &#123;&#125;, average time: &#123;&#125;'</span>.format(</div><div class="line">    total_time, len(test_ids), total_time / len(test_ids)))</div><div class="line">test_annos = &#123;<span class="string">'imgs'</span>: test_annos&#125;</div><div class="line">fd = open(PATH_OUTPUT, <span class="string">'w'</span>)</div><div class="line">json.dump(test_annos, fd)</div><div class="line">fd.close()</div></pre></td></tr></table></figure><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/</span></div><div class="line">python test.py \</div><div class="line">    ssd_mobilenet/output/result_annos.json \</div><div class="line">    ssd_mobilenet/output/frozen_inference_graph.pb \</div><div class="line">    data/VOCdevkit/VOC2012/ImageSets/Main/train_val.txt \</div><div class="line">    data/VOCdevkit/VOC2012/JPEGImages/</div></pre></td></tr></table></figure></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/object_detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用TensorFlow Object Detection API进行物体检测&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow-Slim进行图像分类</title>
    <link href="http://lijiancheng0614.github.io/2017/06/29/2017_06_29_TensorFlow-Slim-image-classification/"/>
    <id>http://lijiancheng0614.github.io/2017/06/29/2017_06_29_TensorFlow-Slim-image-classification/</id>
    <published>2017-06-28T16:00:00.000Z</published>
    <updated>2017-11-22T11:28:28.354Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/slim" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/slim</a></p><p>使用TensorFlow-Slim进行图像分类</p><a id="more"></a><h2 id="准备">准备</h2><ol style="list-style-type: decimal"><li><p>安装TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/install/</a></p><p>如在Ubuntu下安装TensorFlow with GPU support, python 2.7版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div><div class="line">pip install tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div></pre></td></tr></table></figure></li><li><p>下载TF-slim图像模型库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span></div><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models/</div></pre></td></tr></table></figure></li><li><p>准备数据</p><p>有不少公开数据集，这里以官网提供的<code>Flowers</code>为例。</p><p>官网提供了下载和转换数据的代码，为了理解代码并能使用自己的数据，这里参考官方提供的代码进行修改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/data</div><div class="line">wget http://download.tensorflow.org/example_images/flower_photos.tgz</div><div class="line">tar zxf flower_photos.tgz</div></pre></td></tr></table></figure><p>数据集文件夹结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">flower_photos</div><div class="line">├── daisy</div><div class="line">│   ├── 100080576_f52e8ee070_n.jpg</div><div class="line">│   └── ...</div><div class="line">├── dandelion</div><div class="line">├── LICENSE.txt</div><div class="line">├── roses</div><div class="line">├── sunflowers</div><div class="line">└── tulips</div></pre></td></tr></table></figure><p>由于实际情况中我们自己的数据集并不一定把图片按类别放在不同的文件夹里，故我们生成<code>list.txt</code>来表示图片路径与标签的关系。</p><p>Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">class_names_to_ids = &#123;<span class="string">'daisy'</span>: <span class="number">0</span>, <span class="string">'dandelion'</span>: <span class="number">1</span>, <span class="string">'roses'</span>: <span class="number">2</span>, <span class="string">'sunflowers'</span>: <span class="number">3</span>, <span class="string">'tulips'</span>: <span class="number">4</span>&#125;</div><div class="line">data_dir = <span class="string">'flower_photos/'</span></div><div class="line">output_path = <span class="string">'list.txt'</span></div><div class="line"></div><div class="line">fd = open(output_path, <span class="string">'w'</span>)</div><div class="line"><span class="keyword">for</span> class_name <span class="keyword">in</span> class_names_to_ids.keys():</div><div class="line">    images_list = os.listdir(data_dir + class_name)</div><div class="line">    <span class="keyword">for</span> image_name <span class="keyword">in</span> images_list:</div><div class="line">        fd.write(<span class="string">'&#123;&#125;/&#123;&#125; &#123;&#125;\n'</span>.format(class_name, image_name, class_names_to_ids[class_name]))</div><div class="line"></div><div class="line">fd.close()</div></pre></td></tr></table></figure><p>为了方便后期查看label标签，也可以定义<code>labels.txt</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">daisy</div><div class="line">dandelion</div><div class="line">roses</div><div class="line">sunflowers</div><div class="line">tulips</div></pre></td></tr></table></figure><p>随机生成训练集与验证集：</p><p>Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line">_NUM_VALIDATION = <span class="number">350</span></div><div class="line">_RANDOM_SEED = <span class="number">0</span></div><div class="line">list_path = <span class="string">'list.txt'</span></div><div class="line">train_list_path = <span class="string">'list_train.txt'</span></div><div class="line">val_list_path = <span class="string">'list_val.txt'</span></div><div class="line"></div><div class="line">fd = open(list_path)</div><div class="line">lines = fd.readlines()</div><div class="line">fd.close()</div><div class="line">random.seed(_RANDOM_SEED)</div><div class="line">random.shuffle(lines)</div><div class="line"></div><div class="line">fd = open(train_list_path, <span class="string">'w'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines[_NUM_VALIDATION:]:</div><div class="line">    fd.write(line)</div><div class="line"></div><div class="line">fd.close()</div><div class="line">fd = open(val_list_path, <span class="string">'w'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines[:_NUM_VALIDATION]:</div><div class="line">    fd.write(line)</div><div class="line"></div><div class="line">fd.close()</div></pre></td></tr></table></figure><p>生成TFRecord数据：</p><p>Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.insert(<span class="number">0</span>, <span class="string">'../models/slim/'</span>)</div><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_utils</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_dataset</span><span class="params">(list_path, data_dir, output_dir, _NUM_SHARDS=<span class="number">5</span>)</span>:</span></div><div class="line">    fd = open(list_path)</div><div class="line">    lines = [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> fd]</div><div class="line">    fd.close()</div><div class="line">    num_per_shard = int(math.ceil(len(lines) / float(_NUM_SHARDS)))</div><div class="line">    <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">        decode_jpeg_data = tf.placeholder(dtype=tf.string)</div><div class="line">        decode_jpeg = tf.image.decode_jpeg(decode_jpeg_data, channels=<span class="number">3</span>)</div><div class="line">        <span class="keyword">with</span> tf.Session(<span class="string">''</span>) <span class="keyword">as</span> sess:</div><div class="line">            <span class="keyword">for</span> shard_id <span class="keyword">in</span> range(_NUM_SHARDS):</div><div class="line">                output_path = os.path.join(output_dir,</div><div class="line">                    <span class="string">'data_&#123;:05&#125;-of-&#123;:05&#125;.tfrecord'</span>.format(shard_id, _NUM_SHARDS))</div><div class="line">                tfrecord_writer = tf.python_io.TFRecordWriter(output_path)</div><div class="line">                start_ndx = shard_id * num_per_shard</div><div class="line">                end_ndx = min((shard_id + <span class="number">1</span>) * num_per_shard, len(lines))</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(start_ndx, end_ndx):</div><div class="line">                    sys.stdout.write(<span class="string">'\r&gt;&gt; Converting image &#123;&#125;/&#123;&#125; shard &#123;&#125;'</span>.format(</div><div class="line">                        i + <span class="number">1</span>, len(lines), shard_id))</div><div class="line">                    sys.stdout.flush()</div><div class="line">                    image_data = tf.gfile.FastGFile(os.path.join(data_dir, lines[i][<span class="number">0</span>]), <span class="string">'rb'</span>).read()</div><div class="line">                    image = sess.run(decode_jpeg, feed_dict=&#123;decode_jpeg_data: image_data&#125;)</div><div class="line">                    height, width = image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>]</div><div class="line">                    example = dataset_utils.image_to_tfexample(</div><div class="line">                        image_data, <span class="string">b'jpg'</span>, height, width, int(lines[i][<span class="number">1</span>]))</div><div class="line">                    tfrecord_writer.write(example.SerializeToString())</div><div class="line">                tfrecord_writer.close()</div><div class="line">    sys.stdout.write(<span class="string">'\n'</span>)</div><div class="line">    sys.stdout.flush()</div><div class="line"></div><div class="line">os.system(<span class="string">'mkdir -p train'</span>)</div><div class="line">convert_dataset(<span class="string">'list_train.txt'</span>, <span class="string">'flower_photos'</span>, <span class="string">'train/'</span>)</div><div class="line">os.system(<span class="string">'mkdir -p val'</span>)</div><div class="line">convert_dataset(<span class="string">'list_val.txt'</span>, <span class="string">'flower_photos'</span>, <span class="string">'val/'</span>)</div></pre></td></tr></table></figure><p>得到的文件夹结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">data</div><div class="line">├── flower_photos</div><div class="line">├── labels.txt</div><div class="line">├── list_train.txt</div><div class="line">├── list.txt</div><div class="line">├── list_val.txt</div><div class="line">├── train</div><div class="line">│   ├── data_00000-of-00005.tfrecord</div><div class="line">│   ├── ...</div><div class="line">│   └── data_00004-of-00005.tfrecord</div><div class="line">└── val</div><div class="line">    ├── data_00000-of-00005.tfrecord</div><div class="line">    ├── ...</div><div class="line">    └── data_00004-of-00005.tfrecord</div></pre></td></tr></table></figure></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型，这里以<code>Inception-ResNet-v2</code>以例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/checkpoints</div><div class="line">wget http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz</div><div class="line">tar zxf inception_resnet_v2_2016_08_30.tar.gz</div></pre></td></tr></table></figure></li></ol><h2 id="训练">训练</h2><ol style="list-style-type: decimal"><li><p>读入数据</p><p>官方提供了读入<code>Flowers</code>数据集的代码<code>models/slim/datasets/flowers.py</code>，同样这里也是参考并修改成能读入上面定义的通用数据集。</p><p>把下面代码写入<code>models/slim/datasets/dataset_classification.py</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">slim = tf.contrib.slim</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(dataset_dir, num_samples, num_classes, labels_to_names_path=None, file_pattern=<span class="string">'*.tfrecord'</span>)</span>:</span></div><div class="line">    file_pattern = os.path.join(dataset_dir, file_pattern)</div><div class="line">    keys_to_features = &#123;</div><div class="line">        <span class="string">'image/encoded'</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">''</span>),</div><div class="line">        <span class="string">'image/format'</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">'jpg'</span>),</div><div class="line">        <span class="string">'image/class/label'</span>: tf.FixedLenFeature(</div><div class="line">            [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),</div><div class="line">    &#125;</div><div class="line">    items_to_handlers = &#123;</div><div class="line">        <span class="string">'image'</span>: slim.tfexample_decoder.Image(),</div><div class="line">        <span class="string">'label'</span>: slim.tfexample_decoder.Tensor(<span class="string">'image/class/label'</span>),</div><div class="line">    &#125;</div><div class="line">    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)</div><div class="line">    items_to_descriptions = &#123;</div><div class="line">        <span class="string">'image'</span>: <span class="string">'A color image of varying size.'</span>,</div><div class="line">        <span class="string">'label'</span>: <span class="string">'A single integer between 0 and '</span> + str(num_classes - <span class="number">1</span>),</div><div class="line">    &#125;</div><div class="line">    labels_to_names = <span class="keyword">None</span></div><div class="line">    <span class="keyword">if</span> labels_to_names_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        fd = open(labels_to_names_path)</div><div class="line">        labels_to_names = &#123;i : line.strip() <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fd)&#125;</div><div class="line">        fd.close()</div><div class="line">    <span class="keyword">return</span> slim.dataset.Dataset(</div><div class="line">            data_sources=file_pattern,</div><div class="line">            reader=tf.TFRecordReader,</div><div class="line">            decoder=decoder,</div><div class="line">            num_samples=num_samples,</div><div class="line">            items_to_descriptions=items_to_descriptions,</div><div class="line">            num_classes=num_classes,</div><div class="line">            labels_to_names=labels_to_names)</div></pre></td></tr></table></figure></li><li><p>构建模型</p><p>官方提供了许多模型在<code>models/slim/nets/</code>。</p><p>如需要自定义模型，则参考官方提供的模型并放在对应的文件夹即可。</p></li><li><p>开始训练</p><p>官方提供了训练脚本，如果使用官方的数据读入和处理，可使用以下方式开始训练。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/models/slim</div><div class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0"</span> python train_image_classifier.py \</div><div class="line">    --train_dir=train_logs \</div><div class="line">    --dataset_name=flowers \</div><div class="line">    --dataset_split_name=train \</div><div class="line">    --dataset_dir=../../data/flowers \</div><div class="line">    --model_name=inception_resnet_v2 \</div><div class="line">    --checkpoint_path=../../checkpoints/inception_resnet_v2_2016_08_30.ckpt \</div><div class="line">    --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \</div><div class="line">    --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \</div><div class="line">    --max_number_of_steps=1000 \</div><div class="line">    --batch_size=32 \</div><div class="line">    --learning_rate=0.01 \</div><div class="line">    --learning_rate_decay_type=fixed \</div><div class="line">    --save_interval_secs=60 \</div><div class="line">    --save_summaries_secs=60 \</div><div class="line">    --log_every_n_steps=10 \</div><div class="line">    --optimizer=rmsprop \</div><div class="line">    --weight_decay=0.00004</div></pre></td></tr></table></figure><p>不fine-tune把<code>--checkpoint_path</code>, <code>--checkpoint_exclude_scopes</code>和<code>--trainable_scopes</code>删掉。</p><p>fine-tune所有层把<code>--checkpoint_exclude_scopes</code>和<code>--trainable_scopes</code>删掉。</p><p>如果只使用CPU则加上<code>--clone_on_cpu=True</code>。</p><p>其它参数可删掉用默认值或自行修改。</p><p>使用自己的数据则需要修改<code>models/slim/train_image_classifier.py</code>：</p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_factory</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_classification</div></pre></td></tr></table></figure></p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_factory.get_dataset(</div><div class="line">    FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_classification.get_dataset(</div><div class="line">    FLAGS.dataset_dir, FLAGS.num_samples, FLAGS.num_classes, FLAGS.labels_to_names_path)</div></pre></td></tr></table></figure></p><p>在<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'dataset_dir'</span>, <span class="keyword">None</span>, <span class="string">'The directory where the dataset files are stored.'</span>)</div></pre></td></tr></table></figure></p><p>后加入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_samples'</span>, <span class="number">3320</span>, <span class="string">'Number of samples.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_classes'</span>, <span class="number">5</span>, <span class="string">'Number of classes.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'labels_to_names_path'</span>, <span class="keyword">None</span>, <span class="string">'Label names file path.'</span>)</div></pre></td></tr></table></figure></p><p>训练时执行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/models/slim</div><div class="line">python train_image_classifier.py \</div><div class="line">    --train_dir=train_logs \</div><div class="line">    --dataset_dir=../../data/train \</div><div class="line">    --num_samples=3320 \</div><div class="line">    --num_classes=5 \</div><div class="line">    --labels_to_names_path=../../data/labels.txt \</div><div class="line">    --model_name=inception_resnet_v2 \</div><div class="line">    --checkpoint_path=../../checkpoints/inception_resnet_v2_2016_08_30.ckpt \</div><div class="line">    --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \</div><div class="line">    --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits</div></pre></td></tr></table></figure></li><li><p>可视化log</p><p>可一边训练一边可视化训练的log，可看到Loss趋势。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir train_logs/</div></pre></td></tr></table></figure></li></ol><h2 id="验证">验证</h2><p>官方提供了验证脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">python eval_image_classifier.py \</div><div class="line">    --checkpoint_path=train_logs \</div><div class="line">    --eval_dir=eval_logs \</div><div class="line">    --dataset_name=flowers \</div><div class="line">    --dataset_split_name=validation \</div><div class="line">    --dataset_dir=../../data/flowers \</div><div class="line">    --model_name=inception_resnet_v2</div></pre></td></tr></table></figure><p>同样，如果是使用自己的数据集，则需要修改<code>models/slim/eval_image_classifier.py</code>：</p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_factory</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_classification</div></pre></td></tr></table></figure></p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_factory.get_dataset(</div><div class="line">    FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_classification.get_dataset(</div><div class="line">    FLAGS.dataset_dir, FLAGS.num_samples, FLAGS.num_classes, FLAGS.labels_to_names_path)</div></pre></td></tr></table></figure></p><p>在<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'dataset_dir'</span>, <span class="keyword">None</span>, <span class="string">'The directory where the dataset files are stored.'</span>)</div></pre></td></tr></table></figure></p><p>后加入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_samples'</span>, <span class="number">350</span>, <span class="string">'Number of samples.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_classes'</span>, <span class="number">5</span>, <span class="string">'Number of classes.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'labels_to_names_path'</span>, <span class="keyword">None</span>, <span class="string">'Label names file path.'</span>)</div></pre></td></tr></table></figure></p><p>验证时执行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">python eval_image_classifier.py \</div><div class="line">    --checkpoint_path=train_logs \</div><div class="line">    --eval_dir=eval_logs \</div><div class="line">    --dataset_dir=../../data/val \</div><div class="line">    --num_samples=350 \</div><div class="line">    --num_classes=5 \</div><div class="line">    --model_name=inception_resnet_v2</div></pre></td></tr></table></figure><p>可以一边训练一边验证，，注意使用其它的GPU或合理分配显存。</p><p>同样也可以可视化log，如果已经在可视化训练的log则建议使用其它端口，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir eval_logs/ --port 6007</div></pre></td></tr></table></figure><h2 id="测试">测试</h2><p>参考<code>models/slim/eval_image_classifier.py</code>，可编写批量读取图片用模型进行推导的脚本<code>models/slim/test_image_classifier.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">from</span> nets <span class="keyword">import</span> nets_factory</div><div class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> preprocessing_factory</div><div class="line"></div><div class="line">slim = tf.contrib.slim</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'master'</span>, <span class="string">''</span>, <span class="string">'The address of the TensorFlow master to use.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'checkpoint_path'</span>, <span class="string">'/tmp/tfmodel/'</span>,</div><div class="line">    <span class="string">'The directory where the model was written to or an absolute path to a '</span></div><div class="line">    <span class="string">'checkpoint file.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'test_list'</span>, <span class="string">''</span>, <span class="string">'Test image list.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'test_dir'</span>, <span class="string">'.'</span>, <span class="string">'Test image directory.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'batch_size'</span>, <span class="number">16</span>, <span class="string">'Batch size.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_classes'</span>, <span class="number">5</span>, <span class="string">'Number of classes.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'labels_offset'</span>, <span class="number">0</span>,</div><div class="line">    <span class="string">'An offset for the labels in the dataset. This flag is primarily used to '</span></div><div class="line">    <span class="string">'evaluate the VGG and ResNet architectures which do not use a background '</span></div><div class="line">    <span class="string">'class for the ImageNet dataset.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'model_name'</span>, <span class="string">'inception_v3'</span>, <span class="string">'The name of the architecture to evaluate.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'preprocessing_name'</span>, <span class="keyword">None</span>, <span class="string">'The name of the preprocessing to use. If left '</span></div><div class="line">    <span class="string">'as `None`, then the model_name flag is used.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'test_image_size'</span>, <span class="keyword">None</span>, <span class="string">'Eval image size'</span>)</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.test_list:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'You must supply the test list with --test_list'</span>)</div><div class="line"></div><div class="line">    tf.logging.set_verbosity(tf.logging.INFO)</div><div class="line">    <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">        tf_global_step = slim.get_or_create_global_step()</div><div class="line"></div><div class="line">        <span class="comment">####################</span></div><div class="line">        <span class="comment"># Select the model #</span></div><div class="line">        <span class="comment">####################</span></div><div class="line">        network_fn = nets_factory.get_network_fn(</div><div class="line">            FLAGS.model_name,</div><div class="line">            num_classes=(FLAGS.num_classes - FLAGS.labels_offset),</div><div class="line">            is_training=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">        <span class="comment">#####################################</span></div><div class="line">        <span class="comment"># Select the preprocessing function #</span></div><div class="line">        <span class="comment">#####################################</span></div><div class="line">        preprocessing_name = FLAGS.preprocessing_name <span class="keyword">or</span> FLAGS.model_name</div><div class="line">        image_preprocessing_fn = preprocessing_factory.get_preprocessing(</div><div class="line">            preprocessing_name,</div><div class="line">            is_training=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">        test_image_size = FLAGS.test_image_size <span class="keyword">or</span> network_fn.default_image_size</div><div class="line"></div><div class="line">        <span class="keyword">if</span> tf.gfile.IsDirectory(FLAGS.checkpoint_path):</div><div class="line">            checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            checkpoint_path = FLAGS.checkpoint_path</div><div class="line"></div><div class="line">        batch_size = FLAGS.batch_size</div><div class="line">        tensor_input = tf.placeholder(tf.float32, [<span class="keyword">None</span>, test_image_size, test_image_size, <span class="number">3</span>])</div><div class="line">        logits, _ = network_fn(tensor_input)</div><div class="line">        logits = tf.nn.top_k(logits, <span class="number">5</span>)</div><div class="line">        config = tf.ConfigProto()</div><div class="line">        config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line"></div><div class="line">        test_ids = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> open(FLAGS.test_list)]</div><div class="line">        tot = len(test_ids)</div><div class="line">        results = list()</div><div class="line">        <span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</div><div class="line">            sess.run(tf.global_variables_initializer())</div><div class="line">            saver = tf.train.Saver()</div><div class="line">            saver.restore(sess, checkpoint_path)</div><div class="line">            time_start = time.time()</div><div class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">0</span>, tot, batch_size):</div><div class="line">                images = list()</div><div class="line">                idx_end = min(tot, idx + batch_size)</div><div class="line">                print(idx)</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(idx, idx_end):</div><div class="line">                    image_id = test_ids[i]</div><div class="line">                    test_path = os.path.join(FLAGS.test_dir, image_id)</div><div class="line">                    image = open(test_path, <span class="string">'rb'</span>).read()</div><div class="line">                    image = tf.image.decode_jpeg(image, channels=<span class="number">3</span>)</div><div class="line">                    processed_image = image_preprocessing_fn(image, test_image_size, test_image_size)</div><div class="line">                    processed_image = sess.run(processed_image)</div><div class="line">                    images.append(processed_image)</div><div class="line">                images = np.array(images)</div><div class="line">                predictions = sess.run(logits, feed_dict = &#123;tensor_input : images&#125;).indices</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(idx, idx_end):</div><div class="line">                    print(<span class="string">'&#123;&#125; &#123;&#125;'</span>.format(image_id, predictions[i - idx].tolist())</div><div class="line">            time_total = time.time() - time_start</div><div class="line">            print(<span class="string">'total time: &#123;&#125;, total images: &#123;&#125;, average time: &#123;&#125;'</span>.format(</div><div class="line">                time_total, len(test_ids), time_total / len(test_ids)))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure><p>测试时执行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0"</span> python test_image_classifier.py \</div><div class="line">    --checkpoint_path=train_logs/ \</div><div class="line">    --test_list=../../data/list_val.txt \</div><div class="line">    --test_dir=../../data/flower_photos/ \</div><div class="line">    --batch_size=16 \</div><div class="line">    --num_classes=5 \</div><div class="line">    --model_name=inception_resnet_v2</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/slim&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用TensorFlow-Slim进行图像分类&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>DeepLabv3</title>
    <link href="http://lijiancheng0614.github.io/2017/06/28/2017_06_28_DeepLabv3/"/>
    <id>http://lijiancheng0614.github.io/2017/06/28/2017_06_28_DeepLabv3/</id>
    <published>2017-06-27T16:00:00.000Z</published>
    <updated>2017-09-08T16:47:16.710Z</updated>
    
    <content type="html"><![CDATA[<p>Rethinking Atrous Convolution for Semantic Image Segmentation<br>Paper: <a href="https://arxiv.org/abs/1706.05587" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1706.05587</a></p><a id="more"></a><p>在这篇论文，作者在deeplab的基础上重新研究了atrous convolution，并提出了cascaded module和parallel module两种结构。</p><h2 id="introduction">Introduction</h2><p>对于语义分割，考虑使用DCNN的两个挑战：</p><ol style="list-style-type: decimal"><li><p>由于池化和卷积的降采样操作，导致细节空间信息大大减少。</p><p>使用atrous convolution可以减缓这个问题。</p></li><li><p>物体有不同尺度。</p><div class="figure"><img src="fig2.png" alt="Figure 2"><p class="caption">Figure 2</p></div><p>Figure 2. Alternative architectures to capture multi-scale context.</p><p>有四类方法来解决此问题：</p><ol style="list-style-type: lower-alpha"><li><p>图像金字塔</p><p>使用图像金字塔对不同尺度输入提取特征，不同尺度的物体在不同的特征图上突出。</p><p>RCNN [61], Contextual Deep CRFs [48], Deeplab [11], Attention to Scale [10]</p></li><li><p>编码-解码</p><p>取编码器的多尺度特征，并从解码器恢复空间分辨率。</p><p>Segnet [3], U-net[63], Refinenet [47], GCN [60]</p></li><li><p>上下文模块</p><p>在原始网络的顶部级联额外的模块，用于捕捉远程信息。如DenseCRF。</p><p>DeepLab [9], Cascade [85], piecewise [48], DPN [52], Holistic [81]</p></li><li><p>空间金字塔池化</p><p>空间金字塔池化使用多个速率和多个有效的感受野进行池化。</p><p>Attention to Scale [10], PSPNet [84]</p></li></ol></li></ol><p>For the task of semantic segmentation, we consider two challenges in applying Deep Convolutional Neural Networks (DCNNs).</p><ol style="list-style-type: decimal"><li><p>this invariance to local image transformation may impede dense prediction tasks, where detailed spatial information is desired.</p><p>With atrous convolution, also known as dilated convolution, is able to control the resolution at which feature responses are computed within DCNNs without requiring learning extra parameters.</p></li><li><p>the existence of objects at multiple scales.</p><p>Several methods have been proposed to handle the problem and we mainly consider four categories:</p><ol style="list-style-type: lower-alpha"><li><p>Image Pyramid</p><p>an image pyramid to extract features for each scale input where objects at different scales become prominent at different feature maps.</p></li><li><p>Encoder-Decoder</p><p>exploits multi-scale features from the encoder part and recovers the spatial resolution from the decoder part.</p></li><li><p>Context module</p><p>extra modules are cascaded on top of the original network for capturing long range information.</p></li><li><p>Spatial Pyramid Pooling</p><p>spatial pyramid pooling probes an incoming feature map with filters or pooling operations at multiple rates and multiple effective field-of-views.</p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Rethinking Atrous Convolution for Semantic Image Segmentation&lt;br&gt;
Paper: &lt;a href=&quot;https://arxiv.org/abs/1706.05587&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1706.05587&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>ICNet</title>
    <link href="http://lijiancheng0614.github.io/2017/06/08/2017_06_08_ICNet/"/>
    <id>http://lijiancheng0614.github.io/2017/06/08/2017_06_08_ICNet/</id>
    <published>2017-06-07T16:00:00.000Z</published>
    <updated>2017-06-26T09:29:31.642Z</updated>
    
    <content type="html"><![CDATA[<p>ICNet for Real-Time Semantic Segmentation on High-Resolution Images<br>Paper: <a href="https://arxiv.org/abs/1704.08545" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1704.08545</a><br>Code: <a href="https://github.com/hszhao/ICNet" class="uri" target="_blank" rel="external">https://github.com/hszhao/ICNet</a></p><a id="more"></a><p>基于PSPNet提出Image Cascade Network (ICNet)，进行较高准确率的实时图片语义分割。</p><p>影响速度最重要的因素是图片分辨率，尝试了3种方法：对输入下采样，对特征下采样，模型压缩。但速度快了效果差很多。</p><p>输入低中高三种分辨率的图片，并提出Casecade Feature Fusion来融合三种feature map。训练时采用级联标签监督策略（缩小ground-truth来算loss并线性组合不同尺寸的loss）。</p><p>最后步进式地压缩模型：如果要保留1/2的kernel，则先保留3/4来fine-tune，然后再3/4，进行多次。</p><p>We propose an compressed-PSPNet-based image cascade network (ICNet) that incorporates multi-resolution branches.</p><h2 id="introduction">Introduction</h2><div class="figure"><img src="fig1.png" alt="Figure 1"><p class="caption">Figure 1</p></div><p>Figure 1. Inference speed and mIoU performance on Cityscapes [5] test set. Methods involved are ResNet38 [30], PSPNet [33], DUC [29], RefineNet [14], LRR [6], FRRN [22], DeepLabv2 [3], Dilation10 [32], DPN [18], FCN-8s [19], DeepLab [2], CRF-RNN [34], SQ [28], ENet [21], SegNet [1], and our ICNet.</p><p>Our experiments show that high-accuracy methods of PSPNet [33] and ResNet38 [30] take more than 1 second to predict a 1024 × 2048 high-resolution image on one Nvidia TitanX GPU card during testing. These methods fall into the area illustrated in Figure 1 with high accuracy and low speed.</p><p>Recent fast semantic segmentation methods of SegNet [1], ENet [21], and SQ [28], contrarily, take quite different positions in the plot of Figure 1. The speed is much accelerated; but accuracy notably drops, where the final mIoUs are lower than 60%. These methods locate near the right bottom corner in Figure 1.</p><p>The idea is to let low-resolution image go through the full semantic perception network first for a coarse prediction map. Then the proposed cascade fusion unit introduces middle- and high-resolution image feature and improves the coarse semantic map gradually.</p><p>contributions:</p><ul><li><p>We develop an image cascade network (ICNet), which utilizes semantic information in low resolution along<br>with details from high-resolution images efficiently.</p></li><li><p>The proposed ICNet achieves 5x+ speedup of inference, and reduces memory consumption by 5+ times.</p></li><li><p>Our proposed fast semantic segmentation system can run at resolution 1024×2048 in speed of 30.3 fps while accomplishing high-quality results.</p></li></ul><h2 id="related-work">Related Work</h2><p><strong>High Quality Semantic Segmentation</strong></p><p><strong>Fast Semantic Segmentation</strong></p><p>SegNet [1] abandons layers to reduce layer parameters and ENet [21] is a lightweight network.</p><p><strong>Video Segmentation Architectures</strong></p><p>Clockwork [27] reused feature maps given stable video input. Deep feature flow [35] was based on a small-scale optical flow network to propagate features from key frames to others.</p><h2 id="speed-analysis">Speed Analysis</h2><h3 id="time-budget">Time Budget</h3><p>We first study influence of image resolution in semantic segmentation using PSPNet.</p><p>In additional to image resolution, width of a network or the number of kernels also effect the inference time.</p><h3 id="intuitive-speedup">Intuitive Speedup</h3><p><strong>Downsampling Input</strong></p><p>A simple approach is to use the small-resolution image as input.</p><p>although the inference time is reduced by a large margin, the prediction map is very coarse, missing many small but important details compared to the higher resolution prediction.</p><p><strong>Downsampling Feature</strong></p><p>scale down the feature map by a large ratio in the inference process.</p><table><thead><tr class="header"><th>Downsample Size</th><th>8</th><th>16</th><th>32</th></tr></thead><tbody><tr class="odd"><td>mIoU (%)</td><td>71.7</td><td>70.2</td><td>67.1</td></tr><tr class="even"><td>Time (ms)</td><td>446</td><td>177</td><td>131</td></tr></tbody></table><p>Table 1. Total time spent on PSPNet50 when choosing downsampling factors 8, 16 and 32.</p><p>A smaller feature map can yield faster inference at the cost of sacrificing prediction accuracy. The lost information is similarly details contained in low-level layers.</p><p><strong>Model Compression</strong></p><p>trim kernels in each layer</p><p>For each filter, we first calculate the L1 sum of its kernel weights. Then we sort these L1 sums in a descending order for keeping only the most significant ones.</p><table><thead><tr class="header"><th>Kernel Keeping Rates</th><th>1</th><th>0.5</th><th>0.25</th></tr></thead><tbody><tr class="odd"><td>mIoU (%)</td><td>71.7</td><td>67.9</td><td>59.4</td></tr><tr class="even"><td>Time (ms)</td><td>446</td><td>170</td><td>72</td></tr></tbody></table><p>Table 2. Kernel keeping rates in model compression along with related mIoUs and inference time.</p><p>the inference time is still too long. Meanwhile the corresponding mIoU is intolerably low</p><h2 id="our-image-cascade-network">Our Image Cascade Network</h2><h3 id="main-structures-and-branches">Main structures and Branches</h3><div class="figure"><img src="https://hszhao.github.io/projects/icnet/figures/icnet.png"></div><p><strong>Low Resolution</strong></p><p>For the lowest resolution input, it goes through the top branch, which is an FCN-based PSPNet architecture. Since the input size is only 1/4 of the original one, convolution layers correspondingly downsize the feature maps by a ratio of 1/8 and yield 1/32 of the original spatial size.</p><p><strong>Median Resolution</strong></p><p>For the 1/2 size middle-resolution image, the output feature maps are of size 1/16 of the original ones.</p><p>To fusion the 1/16 size feature map with the 1/32 size feature map in the top branch, we propose a cascade feature fusion (CFF) unit that will be discussed later in this paper.</p><p>the convolutional parameters can be shared between the 1/4 and 1/2 resolution inputs, thus saving computational and reducing parameter number.</p><p><strong>High Resolution</strong></p><p>A 1/8 size feature map is resulted.</p><p>we use the CFF unit to incorporate the output of previous CFF unit and current feature map in full resolution in branch three.</p><p><strong>Cascade Label Guidance</strong></p><p>It uses 1/16, 1/8, and 1/4 of ground truth labels to guide the learning stage of low, median and high resolution input.</p><h3 id="branch-analysis">Branch Analysis</h3><p>branch one: even with more than 50 layers, the inference operation and memory consumption are not large as 18ms and 0.6GB.</p><p>There are 17 convolutional layers in branch two and only 3 in branch three.</p><p>only 6ms more is spent to construct the fusion feature map using two branches.</p><p>the inference time in branch three is just 9ms.</p><h3 id="difference-from-other-cascade-structures">Difference from Other Cascade Structures</h3><p>these methods focus on fusing features from different layers from a single-scale input or multi-scale ones.</p><p>They all face the same problem of expensive computation given high-resolution input.</p><p>our ICNet uses the low-resolution input to go through the main semantic segmentation branch and adopts the high-resolution information to help refinement.</p><h2 id="cascade-feature-fusion-and-final-model">Cascade Feature Fusion and Final Model</h2><div class="figure"><img src="fig6.png" alt="Figure 6"><p class="caption">Figure 6</p></div><p>Figure 6. Cascade feature fusion unit. Given input two feature maps <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> where the spatial resolution of the latter is twice of the former one, the fused feature map <span class="math inline">\(F&#39;_2\)</span> is of the same spatial size as <span class="math inline">\(F_2\)</span>.</p><p>The input to this unit contains three components: the two feature maps F1 and F2 of resolution <span class="math inline">\(H1 \times W1 \times C1\)</span> and <span class="math inline">\(H2 \times W2 \times C2\)</span> and a ground truth label in resolution <span class="math inline">\(H2 \times W2 \times 1\)</span>.</p><p>Upsampling is applied to make F1 the same size as F2. Then a dilated convolution layer with kernel size <span class="math inline">\(3 \times 3\)</span> and dilation 1 is applied to refine upsampled features.</p><p>for feature F2, a projection convolutional layer with kernel size <span class="math inline">\(1 \times 1\)</span> is utilized to project it with the same size as the output of feature F1.</p><p>Then two batch normalization layers are used to normalize these two features.</p><p>To enhance learning of F1, we use an auxiliary label guidance to the upsampled F1. The auxiliary loss weight is set to 0.4 as in [33].</p><h3 id="the-loss-function">The Loss Function</h3><p>To train ICNet, we append softmax cross entropy loss in each branch denoted as <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span> and <span class="math inline">\(L_3\)</span> with corresponding weights <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, and <span class="math inline">\(\lambda_3\)</span>.</p><p><span class="math inline">\(L = \lambda_1 L_1 + \lambda_2 L_2 + \lambda_3 L_3\)</span></p><p>All the losses we adopted are the cross-entropy loss on the corresponding downsampled score maps.</p><h3 id="final-model-compression">Final Model Compression</h3><p>We compress our model in a progressive way. Taking compression rate 1/2 as an example, instead of removing a half of kernels directly, we first choose to keep 3/4 of the kernels and initialize this compressed model for following fine tuning. After it is done, we remove more kernels and repeat this process until the goal of compression is achieved.</p><p>For each filter, we calculate the L1 sum of its kernel weights.</p><p>Then we sort these sums in a descending order for ranking.</p><p>Finally, we remove those least important kernels which have smaller weights.</p><h2 id="experimental-evaluation">Experimental Evaluation</h2><p>We conduct experiments based on learning platform Caffe [12].</p><p>Our testing uses only one card.</p><p>Our network structure is modified from PSPNet.</p><p>We changed the concatenation operations in the pyramid pooling module to summation, thus reducing feature length from 4096 to 2048.</p><p>We changed the kernel size in the convolution layer after pyramid pooling from original <span class="math inline">\(3 \times 3\)</span> to <span class="math inline">\(1 \times 1\)</span>. It does not much affect final accuracy but saves computation a lot.</p><p>To train the hyper-parameters, the mini-batch size is set to 16.</p><p><strong>Dataset and Evaluation Metrics</strong></p><p>Cityscapes</p><p>high-resolution images up to <span class="math inline">\(1024 \times 2048\)</span></p><p>training/validation/testing: 2975/500/1525</p><p>contains 30 common class labels, 19 of them are used in training and testing</p><p>For evaluation, both mean of class-wise intersection over union (mIoU) and network forward time are used.</p><h3 id="model-compression">Model Compression</h3><table><thead><tr class="header"><th>Items</th><th>Baseline</th><th>ICNet</th></tr></thead><tbody><tr class="odd"><td>mIoU (%)</td><td>67.9</td><td>67.7</td></tr><tr class="even"><td>Time (ms)</td><td>170</td><td>33</td></tr><tr class="odd"><td>Frame (fps)</td><td>5.9</td><td>30.3</td></tr><tr class="even"><td>Speedup</td><td>1x</td><td>5.2x</td></tr><tr class="odd"><td>Memory (G)</td><td>9.2</td><td>1.6</td></tr><tr class="even"><td>Memory Save</td><td>1x</td><td>5.8x</td></tr></tbody></table><p>Table 3. Performance of baseline and ICNet on validation set of Citysapes. The baseline method is the structure-optimized PSPNet50 with compress operation by half.</p><p>They indicate that only model compression has almost no chance to achieve realtime<br>performance under the condition of keeping decent segmentation quality.</p><p>In what follows, we take the model-compressed PSPNet50, which is reasonably accelerated, as our baseline system for comparison.</p><h3 id="ablation-study-for-image-cascade-framework">Ablation Study for Image Cascade Framework</h3><table><thead><tr class="header"><th>Items</th><th>Baseline</th><th>sub4</th><th>sub24</th><th>sub124</th></tr></thead><tbody><tr class="odd"><td>mIoU (%)</td><td>67.9</td><td>59.6</td><td>66.5</td><td>67.7</td></tr><tr class="even"><td>Time (ms)</td><td>170</td><td>18</td><td>25</td><td>33</td></tr><tr class="odd"><td>Frame (fps)</td><td>5.9</td><td>55.6</td><td>40</td><td>30.3</td></tr><tr class="even"><td>Speedup</td><td>1x</td><td>9.4x</td><td>6.8x</td><td>5.2x</td></tr><tr class="odd"><td>Memory (GB)</td><td>9.2</td><td>0.6</td><td>1.1</td><td>1.6</td></tr><tr class="even"><td>Memory Save</td><td>1x</td><td>15.3x</td><td>8.4x</td><td>5.8x</td></tr></tbody></table><p>Table 4. Our ICNet performance on the validation set of Cityscapes with different settings.</p><p>The setting ‘sub4’ only uses the top branch with the low-resolution input. ‘sub24’ and ‘sub124’ respectively contain top two and all three branches.</p><p><strong>Visual Comparison</strong></p><p><strong>Quantitative Analysis</strong></p><h3 id="final-results-and-comparison">Final Results and Comparison</h3><h2 id="conclusion">Conclusion</h2><p>We have proposed a realtime semantic segmentation system ICNet. It incorporates effective strategies to simplify network structures without significantly reducing performance.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICNet for Real-Time Semantic Segmentation on High-Resolution Images&lt;br&gt;
Paper: &lt;a href=&quot;https://arxiv.org/abs/1704.08545&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1704.08545&lt;/a&gt;&lt;br&gt;
Code: &lt;a href=&quot;https://github.com/hszhao/ICNet&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/hszhao/ICNet&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>ENet</title>
    <link href="http://lijiancheng0614.github.io/2017/06/08/2017_06_08_ENet/"/>
    <id>http://lijiancheng0614.github.io/2017/06/08/2017_06_08_ENet/</id>
    <published>2017-06-07T16:00:00.000Z</published>
    <updated>2017-06-26T09:43:33.297Z</updated>
    
    <content type="html"><![CDATA[<p>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation<br>Paper: <a href="https://arxiv.org/abs/1606.02147" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1606.02147</a><br>Code: <a href="https://github.com/e-lab/ENet-training" class="uri" target="_blank" rel="external">https://github.com/e-lab/ENet-training</a></p><a id="more"></a><p>提出一种做语义分割的速度快且准确率不低的网络ENet。</p><p>ENet采用类似ResNet的bottleneck模块：模块包含3个卷积层（1x1投影降维，主卷积层，1x1扩张），卷积之间添加BN层和PReLU。</p><p>下采样会丢失边缘信息，上采样计算量大，但下采样会有更大的感受野，更多上下文信息。综合考虑，使用dilated convolutions。</p><p>输入大图耗时间，故前两个block就下采样保留少量feature map去除图片中冗余部分。</p><p>采用大encoder，小decoder。</p><p>采用ReLU反而降低精度，可能因为层数较少。</p><p>维度下降容易丢失信息，故与stride=2的卷积并行进行pooling最后合并得到feature map。这样比原来的block快10倍。</p><p>卷积权重有冗余，把<span class="math inline">\(n \times n\)</span>的卷积分解成<span class="math inline">\(n \times 1\)</span>和<span class="math inline">\(1 \times n\)</span>的小卷积。</p><p>使用Spatial Dropout提高准确率。</p><p>we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation.</p><h2 id="introduction">Introduction</h2><p>These references propose networks with huge numbers of parameters, and long inference times.</p><p>we propose a new neural network architecture optimized for fast inference and high accuracy.</p><h2 id="related-work">Related work</h2><p>However, these networks are slow during inference due to their large architectures and numerous parameters.</p><p>Unlike in fully convolutional networks (FCN) [12], fully connected layers of VGG16 were discarded in the latest incarnation of SegNet, in order to reduce the number of floating point operations and memory footprint, making it the smallest of these networks.</p><h2 id="network-architecture">Network architecture</h2><p>We adopt a view of ResNets [24] that describes them as having a single main branch and extensions with convolutional filters that separate from it, and then merge back with an element-wise addition, as shown in Figure 2b.</p><p>Each block consists of three convolutional layers: a <span class="math inline">\(1 \times 1\)</span> projection that reduces the dimensionality, a main convolutional layer (conv in Figure 2b), and a <span class="math inline">\(1 \times 1\)</span> expansion. We place Batch Normalization [25] and PReLU [26] between all convolutions.</p><p>Just as in the original paper, we refer to these as bottleneck modules.</p><p>If the bottleneck is downsampling, a max pooling layer is added to the main branch.</p><p>Also, the first <span class="math inline">\(1 \times 1\)</span> projection is replaced with a <span class="math inline">\(2 \times 2\)</span> convolution with stride 2 in both dimensions.</p><p>We zero pad the activations, to match the number of feature maps.</p><p>conv is either a regular, dilated or full convolution (also known as deconvolution or fractionally strided convolution) with <span class="math inline">\(3 \times 3\)</span> filters.</p><p>Sometimes we replace it with asymmetric convolution i.e. a sequence of <span class="math inline">\(5 \times 1\)</span> and <span class="math inline">\(1 \times 5\)</span> convolutions.</p><p>For the regularizer, we use Spatial Dropout [27], with <span class="math inline">\(p = 0.01\)</span> before bottleneck2.0, and <span class="math inline">\(p = 0.1\)</span> afterwards.</p><div class="figure"><img src="fig2.png" alt="Figure 2"><p class="caption">Figure 2</p></div><p>Figure 2: (a) ENet initial block. MaxPooling is performed with non-overlapping <span class="math inline">\(2 \times 2\)</span> windows, and the convolution has 13 filters, which sums up to 16 feature maps after concatenation. This is heavily inspired by [28]. (b) ENet bottleneck module. conv is either a regular, dilated, or full convolution (also known as deconvolution) with <span class="math inline">\(3 \times 3\)</span> filters, or a <span class="math inline">\(5 \times 5\)</span> convolution decomposed into two asymmetric ones.</p><p>The initial stage contains a single block, that is presented in Figure 2a.</p><p>Stage 1 consists of 5 bottleneck blocks, while stage 2 and 3 have the same structure, with the exception that stage 3 does not downsample the input at the beginning (we omit the 0th bottleneck).</p><p>These three first stages are the encoder.</p><p>Stage 4 and 5 belong to the decoder.</p><p>In the decoder max pooling is replaced with max unpooling, and padding is replaced with spatial convolution without bias.</p><p>for performance reasons, we decided to place only a bare full convolution as the last module of the network.</p><h2 id="design-choices">Design choices</h2><p><strong>Feature map resolution</strong></p><p>Downsampling images during semantic segmentation has two main drawbacks.</p><ol style="list-style-type: decimal"><li><p>reducing feature map resolution implies loss of spatial information like exact edge shape.</p></li><li><p>full pixel segmentation requires that the output has the same resolution as the input.</p></li></ol><p>The first issue has been addressed in FCN [12] by adding the feature maps produced by encoder, and in SegNet [10] by saving indices of elements chosen in max-pooling layers, and using them to produce sparse upsampled maps in the decoder. We followed the SegNet approach, because it allows to reduce memory requirements.</p><p>downsampling has one big advantage. Filters operating on downsampled images have a bigger receptive field, that allows them to gather more context.</p><p>In the end, we have found that it is better to use dilated convolutions for this purpose [30].</p><p><strong>Early downsampling</strong></p><p>processing large input frames is very expensive.</p><p>ENet first two blocks heavily reduce the input size, and use only a small set of feature maps.</p><p>visual information is highly spatially redundant.</p><p>our intuition is that the initial network layers should not directly contribute to classification.</p><p><strong>Decoder size</strong></p><p>our architecture consists of a large encoder, and a small decoder.</p><p><strong>Nonlinear operations</strong></p><p>we have found that removing most ReLUs in the initial layers of the network improved the results.</p><p>We replaced all ReLUs in the network with PReLUs [26], which use an additional parameter per feature map, with the goal of learning the negative slope of non-linearities.</p><p>Initial layers weights exhibit a large variance and are slightly biased towards positive values, while in the later portions of the encoder they settle to a recurring pattern. All layers in the main branch behave nearly exactly like regular ReLUs, while the weights inside bottleneck modules are negative i.e. the function inverts and scales down negative values.</p><p>the decoder weights become much more positive and learn functions closer to identity.</p><p><strong>Information-preserving dimensionality changes</strong></p><p>but aggressive dimensionality reduction can also hinder the information flow.</p><p>introduces a representational bottleneck (or forces one to use a greater number of filters, which lowers computational efficiency).</p><p>as proposed in [28], we chose to perform pooling operation in parallel with a convolution of stride 2, and concatenate resulting feature maps.</p><p>This technique allowed us to speed up inference time of the initial block 10 times.</p><p>Additionally, we have found one problem in the original ResNet architecture. When downsampling, the first <span class="math inline">\(1 \times 1\)</span> projection of the convolutional branch is performed with a stride of 2 in both dimensions, which effectively discards 75% of the input.</p><p>Increasing the filter size to <span class="math inline">\(2 \times 2\)</span> allows to take the full input into consideration, and thus improves the information flow and accuracy.</p><p><strong>Factorizing filters</strong></p><p>convolutional weights have a fair amount of redundancy</p><p>each <span class="math inline">\(n \times n\)</span> convolution can be decomposed into two smaller ones following each other: one with a <span class="math inline">\(n \times 1\)</span> filter and the other with a <span class="math inline">\(1 \times n\)</span> filter [32].</p><p>refer to these as asymmetric convolutions.</p><p>increase the variety of functions learned by blocks and increase the receptive field.</p><p>a sequence of operations used in the bottleneck module (projection, convolution, projection) can be seen as decomposing one large convolutional layer into a series of smaller and simpler operations, that are its low-rank approximation.</p><p><strong>Dilated convolutions</strong></p><p>They replaced the main convolutional layers inside several bottleneck modules in the stages that operate on the smallest resolutions. These gave a significant accuracy boost, by raising IoU on Cityscapes by around 4 percentage points, with no additional cost.</p><p><strong>Regularization</strong></p><p>We placed Spatial Dropout at the end of convolutional branches, right before the addition, and it turned out to work much better than stochastic depth.</p><h2 id="results">Results</h2><h3 id="performance-analysis">Performance Analysis</h3><p>For inference we merge batch normalization and dropout layers into the convolutional filters, to speed up all networks.</p><p><strong>Inference time</strong></p><p>Table 2: Performance comparison.</p><p>NVIDIA TX1</p><table><thead><tr class="header"><th>Model</th><th>480×320</th><th>640×360</th><th>1280×720</th></tr></thead><tbody><tr class="odd"><td>SegNet</td><td>757 ms, 1.3 fps</td><td>1251 ms, 0.8 fps</td><td>- ms, - fps</td></tr><tr class="even"><td>ENet</td><td>47 ms, 21.1 fps</td><td>69 ms, 14.6 fps</td><td>262 ms, 3.8 fps</td></tr></tbody></table><p>NVIDIA Titan X</p><table><thead><tr class="header"><th>Model</th><th>640×360</th><th>1280×720</th><th>1920×1080</th></tr></thead><tbody><tr class="odd"><td>SegNet</td><td>69 ms, 14.6 fps</td><td>289 ms, 3.5 fps</td><td>637 ms, 1.6 fps</td></tr><tr class="even"><td>ENet</td><td>7 ms, 135.4 fps</td><td>21 ms, 46.8 fps</td><td>46 ms, 21.6 fps</td></tr></tbody></table><p>Table 2 compares inference time for a single input frame of varying resolution. We also report the number of frames per second that can be processed. Dashes indicate that we could not obtain a measurement, due to lack of memory.</p><p><strong>Hardware requirements</strong></p><p>Table 3: Hardware requirements. FLOPs are estimated for an input of 3 × 640 × 360.</p><table><thead><tr class="header"><th>Model</th><th>GFLOPs</th><th>Parameters</th><th>Model size (fp16)</th></tr></thead><tbody><tr class="odd"><td>SegNet</td><td>286.03</td><td>29.46M</td><td>56.2 MB</td></tr><tr class="even"><td>ENet</td><td>3.83</td><td>0.37M</td><td>0.7 MB</td></tr></tbody></table><p>Please note that we report storage required to save model parameters in half precision floating point format.</p><p><strong>Software limitations</strong></p><p>Although applying this method allowed us to greatly reduce the number of floating point operations and parameters, it also increased the number of individual kernels calls, making each of them smaller.</p><p>This means that using a higher number of kernels, increases the number of memory transactions, because feature maps have to be constantly saved and reloaded.</p><p>We have found that some of these operations can become so cheap, that the cost of GPU kernel launch starts to outweigh the cost of the actual computation.</p><p>This means that using a higher number of kernels, increases the number of memory transactions, because feature maps have to be constantly saved and reloaded.</p><p>In ENet, PReLUs consume more than a quarter of inference time. Since they are only simple point-wise operations and are very easy to parallelize, we hypothesize it is caused by the aforementioned data movement.</p><h3 id="benchmarks">Benchmarks</h3><p>We have used the Adam optimization algorithm [35] to train the network.</p><p>It allowed ENet to converge very quickly and on every dataset we have used training took only 3-6 hours, using four Titan X GPUs.</p><p>It was performed in two stages: first we trained only the encoder to categorize downsampled regions of the input image, then we appended the decoder and trained the network to perform upsampling and pixel-wise classification.</p><p><strong>Cityscapes</strong></p><p>consists of 5000 fine-annotated images</p><p>train/val/test: 2975/500/1525</p><p>We trained on 19 classes that have been selected in the official evaluation scripts</p><p><strong>CamVid</strong></p><p>train/test: 367/233</p><p><strong>SUN RGB-D</strong></p><p>train/test: 5285/5050</p><p>37 indoor object classes.</p><h2 id="conclusion">Conclusion</h2><p>We have proposed a novel neural network architecture designed from the ground up specifically for semantic segmentation. Our main aim is to make efficient use of scarce resources available on embedded platforms, compared to fully fledged deep learning workstations.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation&lt;br&gt;
Paper: &lt;a href=&quot;https://arxiv.org/abs/1606.02147&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1606.02147&lt;/a&gt;&lt;br&gt;
Code: &lt;a href=&quot;https://github.com/e-lab/ENet-training&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/e-lab/ENet-training&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Simple Does It</title>
    <link href="http://lijiancheng0614.github.io/2017/06/05/2017_06_05_simple_does_it/"/>
    <id>http://lijiancheng0614.github.io/2017/06/05/2017_06_05_simple_does_it/</id>
    <published>2017-06-04T16:00:00.000Z</published>
    <updated>2017-11-12T13:34:30.372Z</updated>
    
    <content type="html"><![CDATA[<p>(CVPR 2017) Simple Does It: Weakly Supervised Instance and Semantic Segmentation<br>Paper: <a href="https://arxiv.org/abs/1603.07485" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1603.07485</a><br>Project Page: <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/weakly-supervised-learning/simple-does-it-weakly-supervised-instance-and-semantic-segmentation/" class="uri" target="_blank" rel="external">https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/weakly-supervised-learning/simple-does-it-weakly-supervised-instance-and-semantic-segmentation/</a></p><a id="more"></a><p>提出递归训练、合理使用GrabCut之类的算法来弱监督实例级的语义分割。</p><p><span class="math inline">\(Box\)</span>：递归训练，初始时使用整个bbox作为分割ground-truth，训练完一轮后加上3个后处理（去掉不在bbox内的预测，恢复IoU过小的bbox，使用DenseCRF过滤）。</p><p><span class="math inline">\(Box^i\)</span>：同Box，除了初始时使用bbox中心20%区域作为分割ground-truth。</p><p><span class="math inline">\(GrabCut+/GrabCut+^i\)</span>：同<span class="math inline">\(Box/Box^i\)</span>，初始时使用修改版GrabCut的结果/多个GrabCut+超过一定阈值的结果。</p><p><span class="math inline">\(MCG \cap GrabCut+\)</span>：同GrabCut+，初始时使用MCG和GrabCut+同时标为前景的像素作为ground-truth。</p><h2 id="introduction">Introduction</h2><p>one of their main weaknesses is that they need a large number of training samples for high quality results.</p><p>contributions:</p><ul><li><p>We explore recursive training of convnets for weakly supervised semantic labelling, discuss how to reach good quality results, and what are the limitations of the approach.</p></li><li><p>We show that state of the art quality can be reached when properly employing GrabCut-like algorithms to generate training labels from given bounding boxes, instead of modifying the segmentation convnet training procedure.</p></li><li><p>We report the best known results when training using bounding boxes only, both using Pascal VOC12 and VOC12+COCO training data, reaching comparable quality with the fully supervised regime.</p></li><li><p>We are the first to show that similar results can be achieved for the weakly supervised instance segmentation task.</p></li></ul><h2 id="related-work">Related work</h2><p><strong>Semantic labelling</strong></p><p><strong>Weakly supervised semantic labelling</strong></p><p><strong>Instance segmentation</strong></p><h2 id="from-boxes-to-semantic-labels">From boxes to semantic labels</h2><p>There are two sources of information: the annotated boxes and priors about the objects.</p><p>We integrate these in the following cues:</p><p><strong>C1 Background</strong>. Since the bounding boxes are expected to be exhaustive, any pixel not covered by a box is labelled as background.</p><p><strong>C2 Object extend</strong>. The box annotations bound the extent of each instance. Assuming a prior on the objects shapes (e.g. oval-shaped objects are more likely than thin bar or full rectangular objects), the box also gives information on the expected object area. We employ this size information during training.</p><p><strong>C3 Objectness</strong>. Two priors typically used are spatial continuity and having a contrasting boundary with the background. In general we can harness priors about object shape by using segment proposal techniques [35], which are designed to enumerate and rank plausible object shapes in an area of the image.</p><h3 id="box-baselines">Box baselines</h3><p>Given an annotated bounding box and its class label, we label all pixels inside the box with such given class. If two boxes overlap, we assume the smaller one is in front.</p><p><strong>Recursive training</strong></p><p>We name this recursive training approach <span class="math inline">\(Naive\)</span>.</p><p>The recursive training is enhanced by de-noising the convnet outputs using extra information from the annotated boxes and object priors. Between each round we improve the labels with three post-processing stages:</p><ol style="list-style-type: decimal"><li><p>Any pixel outside the box annotations is reset to background label (cue C1).</p></li><li><p>If the area of a segment is too small compared to its corresponding bounding box (e.g. IoU &lt; 50%), the<br>box area is reset to its initial label (fed in the first round). This enforces a minimal area (cue C2).</p></li><li><p>As it is common practice among semantic labelling methods, we filter the output of the network to better respect the image boundaries. (We use DenseCRF [20] with the DeepLabv1 parameters [5]). In our weakly supervised scenario, boundary-aware filtering is particularly useful to improve objects delineation (cue C3).</p></li></ol><p>The recursion and these three post-processing stages are crucial to reach good performance. We name this recursive training approach <span class="math inline">\(Box\)</span>, and show an example result in Figure 2.</p><div class="figure"><img src="https://www.mpi-inf.mpg.de/fileadmin/_processed_/csm_teaser_eccv16_subm_e0018b0146.png" alt="Figure 2"><p class="caption">Figure 2</p></div><p>Figure 2: Example results of using only rectangle segments and recursive training (using convnet predictions as supervision for the next round).</p><p><strong>Ignore regions</strong></p><p>We also consider a second variant <span class="math inline">\(Box^i\)</span> that, instead of using filled rectangles as initial labels, we fill in the 20% inner region, and leave the remaining inner area of the bounding box as ignore regions.</p><p>Following cues C2 and C3 (shape and spatial continuity priors), the 20% inner box region should have higher chances of overlapping with the corresponding object, reducing the noise in the generated input labels.</p><h3 id="box-driven-segments">Box-driven segments</h3><h4 id="grabcut-baselines">GrabCut baselines</h4><p>We propose to use a modified version of GrabCut, which we call <span class="math inline">\(GrabCut+\)</span>, where HED boundaries [43] are used as pairwise term instead of the typical RGB colour difference.</p><p>Similar to <span class="math inline">\(Box^i\)</span>, we also consider a <span class="math inline">\(GrabCut+^i\)</span> variant.</p><p>For each annotated box we generate multiple (∼ 150) perturbed GrabCut+ outputs. If 70% of the segments mark the pixel as foreground, the pixel is set to the box object class. If less than 20% of the segments mark the pixels as foreground, the pixel is set as background, otherwise it is marked as ignore. The perturbed outputs are generated by jittering the box coordinates (±5%) as well as the size of the outer background region considered by GrabCut (from 10% to 60%).</p><h4 id="adding-objectness">Adding objectness</h4><p>As final stage the MCG algorithm includes a ranking based on a decision forest trained over the Pascal VOC 2012 dataset.</p><p>Given a box annotation, we pick the highest overlapping proposal as a corresponding segment.</p><p>Inside the annotated boxes, we mark as foreground pixels where both MCG and GrabCut+ agree; the remaining ones are marked as ignore. We denote this approach as <span class="math inline">\(MCG \cap GrabCut+\)</span> or <span class="math inline">\(M \cap G+\)</span> for short.</p><div class="figure"><img src="fig3.png" alt="Figure 3"><p class="caption">Figure 3</p></div><p>Figure 3: Example of the different segmentations obtained starting from a bounding box annotation. Grey/pink/magenta indicate different object classes, white is background, and ignore regions are beige. <span class="math inline">\(M \cap G+\)</span> denotes <span class="math inline">\(MCG \cap GrabCut+\)</span>.</p><h2 id="semantic-labelling-results">Semantic labelling results</h2><h3 id="experimental-setup">Experimental setup</h3><p><strong>Datasets</strong></p><p>Pascal VOC12 segmentation benchmark: 1464 training, 1449 validation, and 1456 test images. augmented<br>set of 10582 training images.</p><p>we use additional training images from the COCO [25] dataset. We only consider images that contain any of the 20 Pascal classes and (following [48]) only objects with a bounding box area larger than 200 pixels. After this filtering, 99 310 images remain (from training and validation sets), which are added to our training set.</p><p><strong>Evaluation</strong></p><p>We use the “comp6” evaluation protocol.</p><p>The performance is measured in terms of pixel intersectionover-union averaged across 21 classes (mIoU).</p><p><strong>Implementation details</strong></p><p>For all our experiments we use the DeepLab-LargeFOV network, using the same train and test parameters as [5].</p><p>We use a mini-batch of 30 images for SGD and initial learning rate of 0:001, which is divided by 10 after a 2k/20k iterations (for Pascal/COCO). At test time, we apply DenseCRF [20].</p><h3 id="main-results">Main results</h3><p><strong>Box results</strong></p><p><strong>Box-driven segment results</strong></p><table><thead><tr class="header"><th>Method</th><th>val.mIoU</th></tr></thead><tbody><tr class="odd"><td>Fast-RCNN</td><td>44.3</td></tr><tr class="even"><td>GT Boxes</td><td>62.2</td></tr><tr class="odd"><td>Box</td><td>61.2</td></tr><tr class="even"><td>Box^i</td><td>62.7</td></tr><tr class="odd"><td>MCG</td><td>62.6</td></tr><tr class="even"><td>GrabCut+</td><td>63.4</td></tr><tr class="odd"><td><span class="math inline">\(GrabCut+^i\)</span></td><td>64.3</td></tr><tr class="even"><td><span class="math inline">\(M \cap G+\)</span></td><td>65.7</td></tr><tr class="odd"><td>DeepLab_ours</td><td>69.1</td></tr></tbody></table><p>Table 1: Weakly supervised semantic labelling results for our baselines. Trained using Pascal VOC12 bounding boxes alone, validation set results. DeepLabours indicates our fully supervised result.</p><div class="figure"><img src="https://www.mpi-inf.mpg.de/fileadmin/inf/d2/khoreva/res.png" alt="Figure 5"><p class="caption">Figure 5</p></div><p>Figure 5: Qualitative results on VOC12. Visually, the results from our weakly supervised method <span class="math inline">\(M \cap G+\)</span> are hardly distinguishable from the fully supervised ones.</p><h3 id="additional-results">Additional results</h3><p><strong>Boundaries supervision</strong></p><p><strong>Different convnet results</strong></p><h2 id="from-boxes-to-instance-segmentation">From boxes to instance segmentation</h2><h2 id="instance-segmentation-results">Instance segmentation results</h2><p><strong>Experimental setup</strong></p><p>For our experiments we use a re-implementation of the DeepMask [33] architecture, and additionally we repurpose a DeepLabv2 VGG-16 network [6] for the instance segmentation task, which we name <span class="math inline">\(DeepLab_{BOX}\)</span>.</p><p><strong>Baselines</strong></p><p><strong>Analysis</strong></p><h2 id="conclusion">Conclusion</h2><p>We showed that when carefully employing the available cues, recursive training using only rectangles as input can be surprisingly effective (<span class="math inline">\(Box^i\)</span>). Even more, when using box-driven segmentation techniques and doing a good balance between accuracy and recall in the noisy training segments, we can reach state of the art performance without modifying the segmentation network training procedure (<span class="math inline">\(M \cap G+\)</span>).</p><p>In future work we would like to explore co-segmentation ideas (treating the set of annotations as a whole), and consider even weaker forms of supervision.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(CVPR 2017) Simple Does It: Weakly Supervised Instance and Semantic Segmentation&lt;br&gt;
Paper: &lt;a href=&quot;https://arxiv.org/abs/1603.07485&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1603.07485&lt;/a&gt;&lt;br&gt;
Project Page: &lt;a href=&quot;https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/weakly-supervised-learning/simple-does-it-weakly-supervised-instance-and-semantic-segmentation/&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/weakly-supervised-learning/simple-does-it-weakly-supervised-instance-and-semantic-segmentation/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Multiple Human Parsing</title>
    <link href="http://lijiancheng0614.github.io/2017/06/02/2017_06_02_Multiple_Human_Parsing/"/>
    <id>http://lijiancheng0614.github.io/2017/06/02/2017_06_02_Multiple_Human_Parsing/</id>
    <published>2017-06-01T16:00:00.000Z</published>
    <updated>2017-06-06T17:40:30.902Z</updated>
    
    <content type="html"><![CDATA[<p>Towards Real World Human Parsing: Multiple-Human Parsing in the Wild<br>Paper: <a href="https://arxiv.org/pdf/1705.07206.pdf" class="uri" target="_blank" rel="external">https://arxiv.org/pdf/1705.07206.pdf</a></p><a id="more"></a><p>提出多人语义分割数据集：4980张图片（训练/验证/测试：3000/1000/980），每张包含2-16人，18个语义标签。</p><p>多人分割模型MH-Parser包含5个组件：<br>Representation learner （FCN提特征）<br>Global parser （用特征生成分割图）<br>Candidate nominator （RPN生成bbox）<br>Local parser （使用特征和bbox生成局部分割）<br>Global-local aggregator （结合全局和局部信息得到最终每个人的分割）</p><p>we introduce the Multiple-Human Parsing (MHP) dataset, which contains multiple persons in a real world scene per single image.</p><p>The MHP dataset contains various numbers of persons (from 2 to 16) per image with 18 semantic classes for each parsing annotation. Persons appearing in the MHP images present sufficient variations in pose, occlusion and interaction.</p><p>To tackle the multiple-human parsing problem, we also propose a novel Multiple-Human Parser (MH-Parser), which considers both the global context and local cues for each person in the parsing process.</p><h2 id="introduction">Introduction</h2><p>all the human parsing datasets only contain one person per image, while usually multiple persons appear simultaneously in a realistic scene.</p><p>Previous work on human parsing mainly focuses on the problem of parsing in controlled and simplified conditions.</p><p>simultaneous presence of multiple persons.</p><p>we tackle the problem of person detection and human parsing simultaneously so that both the global information and the local information are employed.</p><p>contributions:</p><ul><li><p>We introduce the multiple-human parsing problem that extends the research scope of human parsing and matches real world scenarios better in various applications.</p></li><li><p>We construct a new large-scale benchmark, named Multiple-Human Parsing (MHP) dataset, to advance the development of relevant techniques.</p></li><li><p>We propose a novel MH-Parser model for multiple-human parsing, which integrates global context as well as local cues for human parsing and significantly outperforms the naive “detect-and-parse” approach.</p></li></ul><h2 id="related-work">Related work</h2><p><strong>Human parsing</strong></p><p><strong>Instance-aware object segmentation</strong></p><h2 id="the-mhp-dataset">The MHP dataset</h2><p>this is the first large scale dataset focusing on multiple-human parsing.</p><p>4980 images, each image contains 2 to 16 humans, totally there are 14969 person level annotations.</p><h3 id="image-collection-and-annotation-methodology">Image collection and annotation methodology</h3><p>we manually specify several underlying relationships (e.g., family, couple, team, etc.), and several possible scenes (e.g., sports, conferences, banquets, etc.)</p><p>The first task is manually counting the number of foreground persons and duplicating each image into several copies according to that number.</p><p>the second is to assign the fine-grained pixel-wise label for each instance.</p><h3 id="dataset-statistics">Dataset statistics</h3><p>training/validation/test: 3000/1000/980 (randomly choose)</p><p>The images in the MHP dataset contain diverse human numbers, appearances, viewpoints and relationships (see Figure 1).</p><h2 id="multiple-human-parsing-methods">Multiple-Human Parsing Methods</h2><h3 id="mh-parser">MH-Parser</h3><div class="figure"><img src="fig3.png"></div><p>The proposed MH-Parser has five components:</p><ul><li><p>Representation learner</p><p>We use a trunk network to learn rich and discriminative representations. we preserve the spatial information of the image by employing fully convolutional neural networks.</p><p>images and annotations =&gt; representations</p></li><li><p>Global parser</p><p>capture the global information of the whole image. The global parser takes the representation from the representation learner and generates a semantic parsing map of the whole image.</p><p>representations =&gt; a semantic parsing map of the whole image</p></li><li><p>Candidate nominator</p><p>We use a candidate nominator to generate local regions of interest. The candidate nominator consists of a Region Proposal Network (RPN).</p><p>representations =&gt; candidate box</p></li><li><p>Local parser</p><p>give a fine-grained prediction of the semantic parsing labels for each person in the image.</p><p>representations, candidate box =&gt; semantic parsing labels for each person</p></li><li><p>Global-local aggregator</p><p>leverages both the global and local information when performing the parsing task of each person.</p><p>the hidden representations from both the local parser and the global parser =&gt; a set of semantic parsing predictions for each candidate box</p></li></ul><h3 id="detect-and-parse-baseline">Detect-and-parse baseline</h3><p>In the detection stage, we use the representation learner and the candidate nominator as the detection model.</p><p>In the parsing stage, we use the representation learner and the local prediction as the parsing model.</p><h2 id="experiments">Experiments</h2><h3 id="performance-evaluation">Performance evaluation</h3><p>The goal of multiple-human parsing is to accurately detect the persons in one image and generate semantic category predictions for each pixel in the detected regions.</p><p><strong>Mean average precision based on pixel (<span class="math inline">\(mAP^p\)</span>)</strong></p><p>we adopt pixel-level IOU of different semantic categories on a person.</p><p><strong>Percentage of correctly segmented body parts (PCP)</strong></p><p>evaluate how well different semantic categories on a human are segmented.</p><p><strong>Global Mean IOU</strong></p><p>evaluates how well the overall parsing predictions match the overall global parsing labels.</p><h3 id="implementation-details">Implementation details</h3><ul><li><p>representation learner</p><p>adopt a residual network [19] with 50 layers, contains all the layers in a standard residual network except the fully connected layers.</p><p>input: an image with the shorter side resized to 600 pixels and the longer side no larger than 1000 pixels</p><p>output: 1/16 of the spatial dimension of the input image</p></li><li><p>global parser</p><p>add a deconvolution layer after the representation learner.</p><p>output: a feature map with spatial dimension 1/8 of the input image</p></li><li><p>candidate nominator</p><p>use region proposal network (RPN) to generate region proposals.</p><p>output: region proposals</p></li><li><p>local parser</p><p>based on the region after Region of Interest (ROI) pooling from the representation learner and the size after pooling is 40.</p></li><li><p>global-local aggregator</p><p>the local part is from the hidden layer in the local parser, and the global part uses the feature after ROI pooling from the hidden layer of the global parser with the same pooled size.</p></li></ul><p>The network is optimized with one image per batch and the optimizer used is Adam [20].</p><h3 id="experimental-analysis">Experimental analysis</h3><h4 id="overall-performance-evaluation">Overall performance evaluation</h4><p>RL stands for the representation learner, G means the global parser, L denotes the local parser, A for aggregator.</p><h4 id="qualitative-comparison">Qualitative comparison</h4><p>We can see that the MH-Parser captures more fine-grained details compared to the global parser, as some categories with a small number of pixels are accurately predicted.</p><h2 id="conclusion-and-future-work">Conclusion and future work</h2><p>In this paper, we introduced the multiple-human parsing problem and a new large-scale MHP dataset for developing and evaluating multiple-human parsing models.</p><p>We also proposed a novel MH-Parser algorithm to address this new challenging problem and performed detailed evaluations of the proposed method with different baselines on the new benchmark dataset.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Towards Real World Human Parsing: Multiple-Human Parsing in the Wild&lt;br&gt;
Paper: &lt;a href=&quot;https://arxiv.org/pdf/1705.07206.pdf&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/1705.07206.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Deeplab-Semi</title>
    <link href="http://lijiancheng0614.github.io/2017/06/01/2017_06_01_Deeplab-Semi/"/>
    <id>http://lijiancheng0614.github.io/2017/06/01/2017_06_01_Deeplab-Semi/</id>
    <published>2017-05-31T16:00:00.000Z</published>
    <updated>2017-06-07T15:49:44.653Z</updated>
    
    <content type="html"><![CDATA[<p>(ICCV 2015) Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation<br>Paper: <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf" class="uri" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf</a><br>Project Page: <a href="http://liangchiehchen.com/projects/DeepLab_Models.html" class="uri" target="_blank" rel="external">http://liangchiehchen.com/projects/DeepLab_Models.html</a><br>Code: <a href="https://bitbucket.org/deeplab/deeplab-public/" class="uri" target="_blank" rel="external">https://bitbucket.org/deeplab/deeplab-public/</a></p><a id="more"></a><p>提出使用图像级别标签或bounding box来进行弱监督或半监督学习语义分割。</p><p>基础网络是Deeplab，对只有图像级别标签的，使用EM去弱监督指导分割（E步骤：对每个位置的预测结果，若有该图像级别标签，则加上预设定的潜在参数<span class="math inline">\(b_l\)</span>）。</p><p>Bbox-Rect: 使用bbox里的像素作为分割的正样本（重叠时选面积最小的bbox）。</p><p>Bbox-Seg：使用DenseCRF预处理bbox的前景和背景（中心区域a%的像素为前景），前景标上bbox的标签。</p><p>Bbox-EM-Fixed：使用EM去精细化估计的分割图。</p><p>We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets.</p><p>We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings.</p><h2 id="introduction">Introduction</h2><p>A key bottleneck in building this class of DCNN-based segmentation models is that they typically require pixel-level annotated images during training.</p><p>According to [24], collecting bounding boxes around each class instance in the image is about 15 times faster/cheaper than labeling images at the pixel level.</p><p>Most existing approaches for training semantic segmentation models from this kind of very weak labels use multiple instance learning (MIL) techniques.</p><p>We develop novel online Expectation-Maximization (EM) methods for training DCNN semantic segmentation models from weakly annotated data.</p><p><strong>Contributions</strong></p><ol style="list-style-type: decimal"><li><p>We present EM algorithms for training with image-level or bounding box annotation, applicable to both the weakly-supervised and semi-supervised settings.</p></li><li><p>We show that our approach achieves excellent performance when combining a small number of pixel-level annotated images with a large number of image-level or bounding box annotated images, nearly matching the results achieved when all training images have pixel-level annotations.</p></li><li><p>We show that combining weak or strong annotations across datasets yields further improvements. In particular, we reach 73.9% IOU performance on PASCAL VOC 2012 by combining annotations from the PASCAL and MS-COCO datasets.</p></li></ol><h2 id="related-work">Related work</h2><h2 id="proposed-methods">Proposed Methods</h2><p>We build on the DeepLab model for semantic image segmentation proposed in [5].</p><p>We encode the set of image-level labels by <span class="math inline">\(z\)</span>, with <span class="math inline">\(z_l = 1\)</span>, if the <span class="math inline">\(l\)</span>-th label is present anywhere in the image.</p><h3 id="pixel-level-annotations">Pixel-level annotations</h3><p>In the fully supervised case illustrated in Fig. 1, the objective function is</p><p><span class="math display">\[J(\theta) = \log P(y \mid x; \theta) = \sum_{m = 1}^M \log P(y_m \mid x; \theta)\]</span></p><h3 id="image-level-annotations">Image-level annotations</h3><p>When only image-level annotation is available, we can observe the image values <span class="math inline">\(x\)</span> and the image-level labels <span class="math inline">\(z\)</span>, but the pixel-level segmentations <span class="math inline">\(y\)</span> are latent variables. We have the following probabilistic graphical model:</p><p><span class="math display">\[P(x, y, z; \theta) = P(x) \left(\Pi_{m = 1}^M P(y_m \mid x; \theta) \right) P (z \mid y)\]</span></p><p>We pursue an EM-approach in order to learn the model parameters <span class="math inline">\(\theta\)</span> from training data.</p><p>the expected complete-data log-likelihood given the previous parameter estimate <span class="math inline">\(\theta &#39;\)</span> is</p><p><span class="math display">\[Q(\theta; \theta &#39;) = \sum_y P(y \mid x, z; \theta &#39;) \log P(y \mid x; \theta) \approx \log P(\hat{y} \mid x; \theta)\]</span></p><p>where we adopt a hard-EM approximation, estimating in the E-step of the algorithm the latent segmentation by</p><p><span class="math display">\[\begin{align*}\hat{y} &amp; = \operatorname*{argmax}_y P(y \mid x; \theta &#39;) P(z \mid y) \\        &amp; = \operatorname*{argmax}_y \log P(y \mid x; \theta &#39;) + \log P(z \mid y) \\        &amp; = \operatorname*{argmax}_y \left(\sum_{m = 1}^M f_m(y_m \mid x; \theta &#39;) + \log P(z \mid y) \right) \\\end{align*}\]</span></p><p>In the M-step of the algorithm, we optimize <span class="math inline">\(Q(\theta; \theta &#39;)\)</span> by mini-batch SGD similarly to (1), treating <span class="math inline">\(\hat{y}\)</span> as ground truth segmentation.</p><p><strong>EM-Fixed</strong></p><p><span class="math display">\[\hat{y}_m = \operatorname*{argmax}_{y_m} \hat{f_m}(y_m) = f_m (y_m \mid x; \theta &#39;) + \phi (y_m, z)\]</span></p><p>We assume that</p><p><span class="math display">\[\phi(y_m = l, z) =\begin{cases}b_l &amp; \text{if} z_l = 1 \\0   &amp; \text{if} z_l = 0\end{cases}\]</span></p><p>We set the parameters <span class="math inline">\(b_l = b_{fg}\)</span>, if <span class="math inline">\(l &gt; 0\)</span> and <span class="math inline">\(b_0 = b_{bg}\)</span>, with <span class="math inline">\(b_{fg} &gt; b_{bg} &gt; 0\)</span>.</p><p><strong>EM-Adapt</strong></p><p>we assume that <span class="math inline">\(\log P(z \mid y) = \phi(y, z) + (\text{const})\)</span>, where <span class="math inline">\(\phi(y, z)\)</span> takes the form of a cardinality potential [23, 32, 35]. In particular, we encourage at least a <span class="math inline">\(\rho_l\)</span> portion of the image area to be assigned to class <span class="math inline">\(l\)</span>, if <span class="math inline">\(z_l = 1\)</span>, and enforce that no pixel is assigned to class <span class="math inline">\(l\)</span>, if <span class="math inline">\(z_l = 0\)</span>. We set the parameters <span class="math inline">\(\rho_l = \rho_{fg}\)</span>, if <span class="math inline">\(l &gt; 0\)</span> and <span class="math inline">\(\rho_0 = \rho_{bg}\)</span>.</p><p><strong>EM vs. MIL</strong></p><p>While this approach (MIL) has worked well for image classification tasks [28, 29], it is less suited for segmentation as it does not promote full object coverage.</p><h3 id="bounding-box-annotations">Bounding Box Annotations</h3><p><strong>Bbox-Rect</strong></p><p>amounts to simply considering each pixel within the bounding box as positive example for the respective object class.</p><p>Ambiguities are resolved by assigning pixels that belong to multiple bounding boxes to the one that has the smallest area.</p><p><strong>Bbox-Seg</strong></p><p>The bounding boxes fully surround objects but also contain background pixels that contaminate the training set with false positive examples for the respective object classes.</p><p>we perform automatic foreground/background segmentation.</p><p>we use the same CRF as in DeepLab. More specifically, we constrain the center area of the bounding box (<span class="math inline">\(\alpha \%\)</span> of pixels within the box) to be foreground, while we constrain pixels outside the bounding box to be background.</p><div class="figure"><img src="fig3.png" alt="Figure 3"><p class="caption">Figure 3</p></div><p>Figure 3. DeepLab model training from bounding boxes. (Bbox-Seg)</p><div class="figure"><img src="fig4.png" alt="Figure 4"><p class="caption">Figure 4</p></div><p>Figure 4. Estimated segmentation from bounding box annotation.</p><p><strong>Bbox-EM-Fixed</strong></p><p>an EM algorithm that allows us to refine the estimated segmentation maps throughout training. The method is a variant of the EMFixed algorithm in Sec. 3.2, in which we boost the present foreground object scores only within the bounding box area.</p><h3 id="mixed-strong-and-weak-annotations">Mixed strong and weak annotations</h3><p>we bundle to each mini-batch a fixed proportion of strongly/weakly annotated images, and employ our EM algorithm in estimating at each iteration the latent semantic segmentations for the weakly annotated images.</p><div class="figure"><img src="fig5.png" alt="Figure 5"><p class="caption">Figure 5</p></div><p>Figure 5. DeepLab model training on a union of full (strong labels) and image-level (weak labels) annotations.</p><h2 id="experimental-evaluation">Experimental Evaluation</h2><h3 id="experimental-protocol">Experimental Protocol</h3><p><strong>Datasets</strong></p><p>PASCAL VOC 2012 segmentation benchmark</p><p>20 foreground object classes and one background class.</p><p>1464 (train), 1449 (val), and 1456 (test) images for training, validation, and test</p><p>10582 (train aug) and 12031 (trainval aug) images.</p><p>MS-COCO 2014</p><p>123287 images in its trainval set.</p><p>80 foreground object classes and one background class</p><p><strong>Reproducibility</strong></p><p><strong>Weak annotations</strong></p><p>The image-level labels are easily generated by summarizing the pixel-level annotations</p><p>the bounding box annotations are produced by drawing rectangles tightly containing each object instance (PASCAL VOC 2012 also provides instance-level annotations)</p><p><strong>Network architectures</strong></p><p>two DCNN architectures of [5], with parameters initialized from the VGG-16 ImageNet [11] pretrained model of [34]</p><p>large FOV (224×224), small FOV (128×128)</p><p><strong>Training</strong></p><p>We employ our proposed training methods to learn the DCNN component of the DeepLab-CRF model of [5]. For SGD, we use a mini-batch of 20-30 images and initial learning rate of 0.001 (0.01 for the final classifier layer), multiplying the learning rate by 0.1 after a fixed number of iterations. We use momentum of 0.9 and a weight decay of 0.0005. Fine-tuning our network on PASCAL VOC 2012 takes about 12 hours on a NVIDIA Tesla K40 GPU.</p><h3 id="pixel-level-annotations-1">Pixel-level annotations</h3><h3 id="image-level-annotations-1">Image-level annotations</h3><table><thead><tr class="header"><th>Method</th><th>#Strong</th><th>#Weak</th><th>val IOU</th></tr></thead><tbody><tr class="odd"><td>EM-Fixed (Weak)</td><td>-</td><td>10,582</td><td>20.8</td></tr><tr class="even"><td>EM-Adapt (Weak)</td><td>-</td><td>10,582</td><td>38.2</td></tr><tr class="odd"><td>EM-Fixed (Semi)</td><td>200</td><td>10,382</td><td>47.6</td></tr><tr class="even"><td>EM-Fixed (Semi)</td><td>500</td><td>10,082</td><td>56.9</td></tr><tr class="odd"><td>EM-Fixed (Semi)</td><td>750</td><td>9,832</td><td>59.8</td></tr><tr class="even"><td>EM-Fixed (Semi)</td><td>1,000</td><td>9,582</td><td>62.0</td></tr><tr class="odd"><td>EM-Fixed (Semi)</td><td>1,464</td><td>5,000</td><td>63.2</td></tr><tr class="even"><td>EM-Fixed (Semi)</td><td>1,464</td><td>9,118</td><td>64.6</td></tr><tr class="odd"><td>Strong</td><td>1,464</td><td>-</td><td>62.5</td></tr><tr class="even"><td>Strong</td><td>10,582</td><td>-</td><td>67.6</td></tr></tbody></table><p>Table 1. VOC 2012 val performance for varying number of pixel-level (strong) and image-level (weak) annotations (Sec. 4.3).</p><table><thead><tr class="header"><th>Method</th><th>#Strong</th><th>#Weak</th><th>test IOU</th></tr></thead><tbody><tr class="odd"><td>MIL-FCN</td><td>-</td><td>10k</td><td>25.7</td></tr><tr class="even"><td>MIL-sppxl</td><td>-</td><td>760k</td><td>35.8</td></tr><tr class="odd"><td>MIL-obj</td><td>BING</td><td>760k</td><td>37.0</td></tr><tr class="even"><td>MIL-seg</td><td>MCG</td><td>760k</td><td>40.6</td></tr><tr class="odd"><td>EM-Adapt (Weak)</td><td>-</td><td>12k</td><td>39.6</td></tr><tr class="even"><td>EM-Fixed (Semi)</td><td>1.4k</td><td>10k</td><td>66.2</td></tr><tr class="odd"><td>EM-Fixed (Semi)</td><td>2.9k</td><td>9k</td><td>68.5</td></tr><tr class="even"><td>Strong</td><td>12k</td><td>-</td><td>70.3</td></tr></tbody></table><p>Table 2. VOC 2012 test performance for varying number of pixel-level (strong) and image-level (weak) annotations (Sec. 4.3).</p><h3 id="bounding-box-annotations-1">Bounding box annotations</h3><table><thead><tr class="header"><th>Method</th><th>#Strong</th><th>#Box</th><th>val IOU</th></tr></thead><tbody><tr class="odd"><td>Bbox-Rect (Weak)</td><td>-</td><td>10,582</td><td>52.5</td></tr><tr class="even"><td>Bbox-EM-Fixed (Weak)</td><td>-</td><td>10,582</td><td>54.1</td></tr><tr class="odd"><td>Bbox-Seg (Weak)</td><td>-</td><td>10,582</td><td>60.6</td></tr><tr class="even"><td>Bbox-Rect (Semi)</td><td>1,464</td><td>9,118</td><td>62.1</td></tr><tr class="odd"><td>Bbox-EM-Fixed (Semi)</td><td>1,464</td><td>9,118</td><td>64.8</td></tr><tr class="even"><td>Bbox-Seg (Semi)</td><td>1,464</td><td>9,118</td><td>65.1</td></tr><tr class="odd"><td>Strong</td><td>1,464</td><td>-</td><td>62.5</td></tr><tr class="even"><td>Strong</td><td>10,582</td><td>-</td><td>67.6</td></tr></tbody></table><p>Table 3. VOC 2012 val performance for varying number of pixel-level (strong) and bounding box (weak) annotations (Sec. 4.4).</p><table><thead><tr class="header"><th>Method</th><th>#Strong</th><th>#Box</th><th>test IOU</th></tr></thead><tbody><tr class="odd"><td>BoxSup</td><td>MCG</td><td>10k</td><td>64.6</td></tr><tr class="even"><td>BoxSup</td><td>1.4k (+MCG)</td><td>9k</td><td>66.2</td></tr><tr class="odd"><td>Bbox-Rect (Weak)</td><td>-</td><td>12k</td><td>54.2</td></tr><tr class="even"><td>Bbox-Seg (Weak)</td><td>-</td><td>12k</td><td>62.2</td></tr><tr class="odd"><td>Bbox-Seg (Semi)</td><td>1.4k</td><td>10k</td><td>66.6</td></tr><tr class="even"><td>Bbox-EM-Fixed (Semi)</td><td>1.4k</td><td>10k</td><td>66.6</td></tr><tr class="odd"><td>Bbox-Seg (Semi)</td><td>2.9k</td><td>9k</td><td>68.0</td></tr><tr class="even"><td>Bbox-EM-Fixed (Semi)</td><td>2.9k</td><td>9k</td><td>69.0</td></tr><tr class="odd"><td>Strong</td><td>12k</td><td>-</td><td>70.3</td></tr></tbody></table><p>Table 4. VOC 2012 test performance for varying number of pixel-level (strong) and bounding box (weak) annotations (Sec. 4.4).</p><h3 id="exploiting-annotations-across-datasets">Exploiting Annotations Across Datasets</h3><h3 id="qualitative-segmentation-results">Qualitative Segmentation Results</h3><h2 id="conclusions">Conclusions</h2><p>The paper has explored the use of weak or partial annotation in training a state of art semantic image segmentation model. Extensive experiments on the challenging PASCAL VOC 2012 dataset have shown that:</p><ol style="list-style-type: decimal"><li><p>Using weak annotation solely at the image-level seems insufficient to train a high-quality segmentation model.</p></li><li><p>Using weak bounding-box annotation in conjunction with careful segmentation inference for images in the training set suffices to train a competitive model.</p></li><li><p>Excellent performance is obtained when combining a small number of pixel-level annotated images with a large number of weakly annotated images in a semi-supervised setting, nearly matching the results achieved when all training images have pixel-level annotations.</p></li><li><p>Exploiting extra weak or strong annotations from other datasets can lead to large improvements.</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(ICCV 2015) Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation&lt;br&gt;
Paper: &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf&lt;/a&gt;&lt;br&gt;
Project Page: &lt;a href=&quot;http://liangchiehchen.com/projects/DeepLab_Models.html&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://liangchiehchen.com/projects/DeepLab_Models.html&lt;/a&gt;&lt;br&gt;
Code: &lt;a href=&quot;https://bitbucket.org/deeplab/deeplab-public/&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://bitbucket.org/deeplab/deeplab-public/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>BoxSup</title>
    <link href="http://lijiancheng0614.github.io/2017/05/31/2017_05_31_BoxSup/"/>
    <id>http://lijiancheng0614.github.io/2017/05/31/2017_05_31_BoxSup/</id>
    <published>2017-05-30T16:00:00.000Z</published>
    <updated>2017-06-07T15:48:13.090Z</updated>
    
    <content type="html"><![CDATA[<p>(ICCV 2015) BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation<br>Paper: <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" class="uri" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf</a></p><a id="more"></a><p>提出生成区域proposal和训练CNN交替进行的方法，使用bounding box来部分代替mask进行图像语义分割的训练。</p><p>对只有bounding box ground-truth的样本，用Multiscale Combinatorial Grouping (MCG)生成分割mask的候选，并优化label选一个与bbox平均交集最大的mask作为监督信息。</p><p>对只有segmentation ground-truth的样本，直接更新FCN的参数。</p><p>Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data.</p><p>we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks.</p><h2 id="introduction">Introduction</h2><p>pixel-level mask annotations are time-consuming, frustrating, and in the end commercially expensive to obtain.</p><p>Though these box-level annotations are less precise than pixel-level masks, their amount may help improve training deep networks for semantic segmentation.</p><p>In addition, current leading approaches have not fully utilized the detailed pixel-level annotations.</p><p>In this work, we investigate bounding box annotations as an alternative or extra source of supervision to train convolutional networks for semantic segmentation.</p><p>We resort to region proposal methods [4, 33, 2] to generate candidate segmentation masks.</p><p>The convolutional network is trained under the supervision of these approximate masks.</p><p>We extensively evaluate our method, called “BoxSup”, on the PASCAL segmentation benchmarks [9, 26].</p><p>we may save expensive labeling effort by using bounding box annotations dominantly.</p><p>Our error analysis reveals that a BoxSup model trained with a large set of boxes effectively increases the object recognition accuracy (the accuracy in the middle of an object), and its improvement on object boundaries is secondary.</p><h2 id="related-work">Related Work</h2><h2 id="baseline">Baseline</h2><p>we adopt our implementation of FCN refined by CRF [6] as the mask-supervised baseline.</p><h2 id="approach">Approach</h2><h3 id="segment-proposals-for-supervised-training">Segment Proposals for Supervised Training</h3><p>To harness the bounding boxes annotations, it is desired to estimate segmentation masks from them.</p><p>We propose to generate a set of candidate segments using region proposal methods (e.g., Selective Search [33]) due to their nice properties:</p><ol style="list-style-type: decimal"><li><p>region proposal methods have high recall rates [2] of having a good candidate in the proposal pool.</p></li><li><p>region proposal methods generate candidates of greater variance, which provide a kind of data augmentation [20] for network training.</p></li></ol><p>the region proposal is only used for networking training.</p><h3 id="formulation">Formulation</h3><p>As a pre-processing, we use a region proposal method to generate segmentation masks.</p><p>We adopt Multiscale Combinatorial Grouping (MCG) [2] by default</p><p>The proposal candidate masks are fixed throughout the training procedure. But during training, each candidate mask will be assigned a label which can be a semantic category or background. The labels assigned to the masks will be updated.</p><p>With a ground-truth bounding box annotation, we expect it to pick out a candidate mask that overlaps the box as much as possible</p><p><span class="math display">\[\varepsilon_o = \frac{1}{N} \sum_S (1 - IoU(B, S)) \delta(l_B, l_S)\]</span></p><p>With the candidate masks and their estimated semantic labels, we can supervise the deep convolutional network as in Eqn.(1).</p><p><span class="math display">\[\varepsilon_r = \sum_p e(X_\theta(p), l_S(p))\]</span></p><p>We minimize an objective function that combines the above two terms:</p><p><span class="math display">\[\min_{θ,\{l_S\}} \sum_i (\varepsilon_o + \lambda \varepsilon_r)\]</span></p><p>Here the summation <span class="math inline">\(\sum_i\)</span> runs over all training images, and <span class="math inline">\(\lambda = 3\)</span> is a fixed weighting parameter.</p><h3 id="training-algorithm">Training Algorithm</h3><div class="figure"><img src="fig1.png"></div><p>Next we propose a greedy iterative solution to find a local optimum.</p><p>we only consider the case in which one ground-truth bounding box can “activate” (i.e., assign a non-background label to) one and only one candidate.</p><p>the optimization procedure may be trapped in poor local optima.</p><p>we further adopt a random sampling method to select the candidate segment for each ground-truth bounding box. Instead of selecting the single segment with the smallest cost <span class="math inline">\(E_o + \lambda E_r\)</span>, we randomly sample a segment from the first <span class="math inline">\(k\)</span> segments with the smallest costs.</p><p>With the semantic labeling <span class="math inline">\(\{l_S\}\)</span> of all candidate segments fixed, we update the network parameters <span class="math inline">\(\theta\)</span>. In this case, the problem becomes the FCN problem.</p><p>We iteratively perform the above two steps, fixing one set of variables and solving for the other set. For each iteration, we update the network parameters using one training epoch (i.e., all training images are visited once), and after that we update the segment labeling of all images.</p><p>The labeling <span class="math inline">\(l(p)\)</span> is given by candidate proposals as above if a sample only has ground-truth boxes, and is simply assigned as the true label if a sample has ground-truth masks.</p><h2 id="experiments">Experiments</h2><h3 id="experiments-on-pascal-voc-2012">Experiments on PASCAL VOC 2012</h3><p><strong>Comparisons of Supervision Strategies</strong></p><p>This indicates that in practice we can avoid the expensive mask labeling effort by using only bounding boxes, with small accuracy loss.</p><p>This means that we can greatly reduce the labeling effort by dominantly using bounding box annotations.</p><p><strong>Error Analysis</strong></p><p>The error in semantic segmentation can be roughly thought of as two types:</p><ol style="list-style-type: lower-roman"><li><p>recognition error that is due to confusions of recognizing object categories</p></li><li><p>boundary error that is due to misalignments of pixel-level labels on object boundaries.</p></li></ol><p>To analyze the error, we separately evaluate the performance on the boundary regions and interior regions.</p><p>correctly recognizing the interior may also help improve the boundaries (e.g., due to the CRF post-processing).</p><p>So the improvement of the extra boxes on the boundary regions is secondary.</p><p><strong>Comparisons of Estimated Masks for Supervision</strong></p><p>This indicates the importance of the mask quality for supervision.</p><p><strong>Comparisons of Region Proposals</strong></p><p>our method effectively makes use of the higher quality segmentation masks to train a better network.</p><p><strong>Comparisons on the Test Set</strong></p><p><strong>Exploiting Boxes in PASCAL VOC 2007</strong></p><p><strong>Baseline Improvement</strong></p><p>To show the potential of our BoxSup method in parallel with improvements on the baseline, we use a simple test-time augmentation to boost our results.</p><h3 id="experiments-on-pascal-context">Experiments on PASCAL-CONTEXT</h3><p>To train a BoxSup model for this dataset, we first use the box annotations from all 80 object categories in the COCO dataset to train the FCN (using VGG-16).</p><h2 id="conclusion">Conclusion</h2><p>The proposed BoxSup method can effectively harness bounding box annotations to train deep networks for semantic segmentation.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(ICCV 2015) BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation&lt;br&gt;
Paper: &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
</feed>
