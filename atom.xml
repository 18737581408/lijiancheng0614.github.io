<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lijiancheng0614</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lijiancheng0614.github.io/"/>
  <updated>2018-06-17T09:53:58.954Z</updated>
  <id>http://lijiancheng0614.github.io/</id>
  
  <author>
    <name>Jiancheng Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Game Theory Week 7 Coalitional Games</title>
    <link href="http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_7/"/>
    <id>http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_7/</id>
    <published>2018-06-14T16:00:00.000Z</published>
    <updated>2018-06-17T09:53:58.954Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 7: Coalitional Games</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/7" class="uri" target="_blank" rel="external">https://www.coursera.org/learn/game-theory-1/home/week/7</a></p><a id="more"></a><h2 id="coalitional-game-theory-taste">7-1 Coalitional Game Theory: Taste</h2><p>举例：政治伙伴、商业合作联盟、建筑团队</p><h2 id="coalitional-game-theory-definitions">7-2 Coalitional Game Theory: Definitions</h2><ul><li><p>Transferable utility assumption:</p><p>可传递效用假设：</p><ol style="list-style-type: decimal"><li><p>the payoffs to a coalition may be freely redistributed among its members</p><p>联盟的收益可以在其成员之间自由重新分配</p></li><li><p>satisfied whenever there is a universal currency that is used for exchange in the system</p><p>满足只要有用于系统交换的通用货币</p></li><li><p>means that each coalition can be assigned a single value as its payoff</p><p>意味着每个联盟可以分配一个单一的价值作为其收益</p></li></ol></li><li><p>Coalitional game with transferable utility</p><p>带可传递效用的联盟型博弈（合作博弈 Cooperative game）</p><p>a pair <span class="math inline">\((N, v)\)</span>, where</p><ul><li><p><span class="math inline">\(N\)</span> is a finite set of players, indexed by i</p></li><li><p><span class="math inline">\(v : 2^N \mapsto \mathbb{R}\)</span> associates with each coalition <span class="math inline">\(S \subseteq N\)</span> a real-valued payoff <span class="math inline">\(v(S)\)</span> that the coalition’s members can distribute among themselves. We assume that <span class="math inline">\(v(\emptyset) = 0\)</span></p></li></ul></li><li><p>Superadditive game</p><p>超可加的博弈</p><p>A game <span class="math inline">\(G = (N, v)\)</span> is <strong>superadditive</strong> if for all <span class="math inline">\(S, T \subset N\)</span>, if <span class="math inline">\(S \cap T = \emptyset\)</span>, then <span class="math inline">\(v(S \cup T) \ge v(S) + v(T)\)</span></p></li></ul><h2 id="the-shapley-value">7-3 The Shapley Value</h2><p><span class="math inline">\(\psi(N, v)\)</span> is a vector of payoffs to each agent, explaining how they divide the payoff of the grand coalition, <span class="math inline">\(\psi_i(N, v)\)</span> is <span class="math inline">\(i\)</span>’s payoff.</p><ol style="list-style-type: decimal"><li><p>Axiom (Symmetry)</p><p>For any <span class="math inline">\(v\)</span>, if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are interchangeable then <span class="math inline">\(\psi_i(N, v) = \psi_j(N, v)\)</span></p></li><li><p>Axiom (Dummy player)</p><p>For any <span class="math inline">\(v\)</span>, if <span class="math inline">\(i\)</span> is a dummy player then <span class="math inline">\(\psi_i(N, v) = v({i})\)</span></p></li><li><p>Axiom (Additivity)</p><p>For any two <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>, <span class="math inline">\(\psi_i(N, v_1 + v_2) = \psi_i(N, v_1) + \psi_i(N, v_2)\)</span> for each <span class="math inline">\(i\)</span>, where the game <span class="math inline">\((N, v_1 + v_2)\)</span> is defined by <span class="math inline">\((v_1 + v_2)(S) = v_1(S) + v_2(S)\)</span> for every coalition <span class="math inline">\(S\)</span></p></li></ol><ul><li><p>Shapley value</p><p>Given a coalitional game <span class="math inline">\((N, v)\)</span>, the <strong>Shapley value</strong> divides payoffs among players according to:</p><p><span class="math display">\[\phi_i(N, v) = \frac{1}{N!} \sum_{S \subseteq N \backslash \{i\}} \lvert S \rvert ! (\lvert N \rvert - \lvert S \rvert - 1)! \left[ v(S \cup \{i\}) - v(S) \right]\]</span></p></li><li><p>Theorem</p><p>Given a coalitional game <span class="math inline">\((N, v)\)</span>, there is a unique payoff division <span class="math inline">\(x(v) = \phi(N, v)\)</span> that divides the full payoff of the grand coalition and that satisfies the Symmetry, Dummy player and Additivity axioms: the Shapley Value</p></li></ul><h2 id="the-core">7-4 The Core</h2><ul><li><p>Core</p><p>核心</p><p>A payoff vector <span class="math inline">\(x\)</span> is in the <strong>core</strong> of a coalitional game <span class="math inline">\((N, v)\)</span> if and only if</p><p><span class="math display">\[\forall S \subseteq N, \sum_{i \in S} x_i \ge v(S)\]</span></p></li></ul><p>The core can be empty, and not unique.</p><p>核心可以为空，也可以不唯一。</p><ul><li><p>Simple game</p><p>简单博弈</p><p>A game <span class="math inline">\(G = (N, v)\)</span> is <strong>simple</strong> if for all <span class="math inline">\(S \subset N\)</span>, <span class="math inline">\(v(S) \in {0, 1}\)</span></p></li><li><p>Veto player</p><p>否决选手</p><p>A player <span class="math inline">\(i\)</span> is a <strong>veto player</strong> if <span class="math inline">\(v(N \backslash \{i\}) = 0\)</span></p></li><li><p>Theorem</p><p>In a simple game the core is empty iff there is no veto player.</p><p>在一个简单博弈中，当且仅当没有否决选手，核心为空。</p><p>If there are veto players, the core consists of all payoff vectors in which the nonveto players get 0.</p><p>如果有否决选手，则核心由所有非否决权者收益为 0 的分配向量组成。</p></li></ul><p>Example: Airport Game</p><ul><li><p>Convex game</p><p>凸博弈</p><p>A game <span class="math inline">\(G = (N, v)\)</span> is <strong>convex</strong> if for all <span class="math inline">\(S, T \subset N\)</span>, <span class="math inline">\(v(S \cup T) \ge v(S) + v(T) - v(S \cap T)\)</span></p></li><li><p>Theorem</p><p>Every convex game has a nonempty core.</p><p>每个凸博弈都有非空核心。</p></li><li><p>Theorem</p><p>In every convex game, the Shapley value is in the core.</p><p>对于每个凸博弈，Shapley 值都在核心中。</p></li></ul><h2 id="comparing-the-core-and-shapley-value-in-an-example">7-5 Comparing the Core and Shapley value in an Example</h2><p>UN security council: represent it as a cooperative game</p><p>举例：联合国安理会（可以看作合作博弈）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 7: Coalitional Games&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/7&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/7&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 6 Bayesian Games</title>
    <link href="http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_6/"/>
    <id>http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_6/</id>
    <published>2018-06-14T16:00:00.000Z</published>
    <updated>2018-06-17T10:06:46.367Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 6: Bayesian Games</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/6" class="uri" target="_blank" rel="external">https://www.coursera.org/learn/game-theory-1/home/week/6</a></p><a id="more"></a><h2 id="bayesian-games-taste">6-1 Bayesian Games: Taste</h2><p>例子：拍卖</p><h2 id="bayesian-games-first-definition">6-2 Bayesian Games: First Definition</h2><ul><li><p>Bayesian Game: Information Sets</p><p>贝叶斯博弈</p><p>A <strong>Bayesian game</strong> is a tuple <span class="math inline">\((N, G, P, I)\)</span> where</p><ul><li><p><span class="math inline">\(N\)</span> is a set of agents</p></li><li><p><span class="math inline">\(G\)</span> is a set of games with <span class="math inline">\(N\)</span> agents each such that if <span class="math inline">\(g, g&#39; \in G\)</span> then for each agent <span class="math inline">\(i \in N\)</span> the strategy space in <span class="math inline">\(g\)</span> is identical to the strategy space in <span class="math inline">\(g&#39;\)</span></p></li><li><p><span class="math inline">\(P \in \prod (G)\)</span> is a common prior over games, where <span class="math inline">\(\prod(G)\)</span> is the set of all probability distributions over <span class="math inline">\(G\)</span></p></li><li><p><span class="math inline">\(I = (I_1, \cdots, I_N)\)</span> is a set of partitions of <span class="math inline">\(G\)</span>, one for each agent</p></li></ul></li></ul><h2 id="bayesian-games-first-defintion-yoav">6-2 Bayesian Games: First Defintion (yoav)</h2><h2 id="bayesian-games-second-definition">6-3 Bayesian Games: Second Definition</h2><ul><li><p>Directly represent uncertainty over utility function using the notion of <strong>epistemic type</strong>.</p></li><li><p>Bayesian Game: Epistemic Types</p><p>A <strong>Bayesian game</strong> is a tuple <span class="math inline">\((N, A, \Theta, p, u)\)</span> where</p><ul><li><p><span class="math inline">\(N\)</span> is a set of agents</p></li><li><p><span class="math inline">\(A = (A_1, \cdots, A_n)\)</span>, where <span class="math inline">\(A_i\)</span> is the set of actions available to player <span class="math inline">\(i\)</span></p></li><li><p><span class="math inline">\(\Theta = (\Theta_1, \cdots, \Theta_n)\)</span>, where <span class="math inline">\(\Theta_i\)</span> is the type space of player <span class="math inline">\(i\)</span></p></li><li><p><span class="math inline">\(p : \Theta \mapsto [0, 1]\)</span> is the common prior over types</p></li><li><p><span class="math inline">\(u = (u_1, \cdots, u_n)\)</span>, where <span class="math inline">\(u_i : A \times \Theta \mapsto \mathbb{R}\)</span> is the utility function for player <span class="math inline">\(i\)</span></p></li></ul></li></ul><h2 id="analyzing-bayesian-games">6-4 Analyzing Bayesian Games</h2><ul><li><p>Expected Utility</p><p>期望效用</p><p>3 standard notions:</p><ol style="list-style-type: decimal"><li><p>ex-ante</p><p>the agent know nothing about anyone’s actual type</p><p>事前：该代理不知道其它代理的实际类型</p></li><li><p>interim</p><p>an agent knows her own type but not the types of the other agents</p><p>中期：该代理知道自己的类型，但不知道其它代理的类型</p></li><li><p>ex-post</p><p>the agent knows all agents’ types</p><p>事后：该代理知道所有代理的类型</p></li></ol></li></ul><p>Given a Bayesian game <span class="math inline">\((N, A, \Theta, p, u)\)</span>, where <span class="math inline">\(i\)</span>’s type is <span class="math inline">\(\theta_i\)</span> and where the agents’ strategies are given by the mixed strategy profile <span class="math inline">\(s\)</span></p><ul><li><p>Interim expected utility</p><p><span class="math display">\[EU_i(s \mid \theta_i) = \sum_{\theta_{-i} \in \Theta_{-i}} p(\theta_{-i} \mid \theta_i) \sum_{a \in A}  \left( \prod_{j \in N} s_j(a_j \mid \theta_j) \right) u_i(a, \theta_{-i}, \theta_i)\]</span></p></li><li><p>Ex-ante expected utility</p><p><span class="math display">\[EU_i(s) = \sum_{\theta_{-i} \in \Theta_{-i}} p(\theta_{-i}) EU_i(s \mid \theta_i)\]</span></p></li><li><p>Ex-post expected utility</p><p><span class="math display">\[EU_i(s, \theta) = \sum_{a \in A} \left( \prod_{j \in N} s_j(a_j \mid \theta_j) \right) u_i(a, \theta)\]</span></p></li><li><p>Best response</p><p><span class="math display">\[BR_i(s_{-i}) = \text{argmax}_{s&#39;_i} EU_i(s&#39;_i, s_{-i} \mid \theta_i)\]</span></p><p>if <span class="math inline">\(\forall \theta_i \in \Theta_i, p(\theta_i) \gt 0\)</span></p><p><span class="math display">\[BR_i(s_{-i}) = \text{argmax}_{s&#39;_i} EU_i(s&#39;_i, s_{-i}) = \text{argmax}_{s&#39;_i} \sum_{\theta_i} EU_i(s&#39;_i, s_{-i} \mid \theta_i)\]</span></p></li><li><p>Bayesian equilibrium / Bayes-Nash equilibrium</p><p><span class="math display">\[\forall i, s_i \in BR_i(s_{-i})\]</span></p></li></ul><h2 id="analyzing-bayesian-games-another-example">6-5 Analyzing Bayesian Games: Another Example</h2><p>例子：A Sheriff’s Dilemma</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 6: Bayesian Games&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/6&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/6&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 2 Mixed-Strategy Nash Equilibrium</title>
    <link href="http://lijiancheng0614.github.io/2018/06/14/2018_06_14_Game_Theory_Week_2/"/>
    <id>http://lijiancheng0614.github.io/2018/06/14/2018_06_14_Game_Theory_Week_2/</id>
    <published>2018-06-13T16:00:00.000Z</published>
    <updated>2018-06-17T10:10:13.803Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 2: Mixed-Strategy Nash Equilibrium</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/2" class="uri" target="_blank" rel="external">https://www.coursera.org/learn/game-theory-1/home/week/2</a></p><a id="more"></a><h2 id="mixed-strategies-and-nash-equilibrium-i">2-1 Mixed Strategies and Nash Equilibrium (I)</h2><p>举例：设置检查站，攻击与防守</p><h2 id="mixed-strategies-and-nash-equilibrium-ii">2-2 Mixed Strategies and Nash Equilibrium (II)</h2><p>Define a <strong>strategy</strong> <span class="math inline">\(s_i\)</span> for agent <span class="math inline">\(i\)</span> as any probability distribution over the actions <span class="math inline">\(A_i\)</span>.</p><ul><li><p><strong>pure strategy</strong>: only one action is played with positive probability</p></li><li><p><strong>mixed strategy</strong>: more than one action is played with positive probability</p><p>these actions are called the <strong>support</strong> of the mixed strategy</p></li></ul><p><span class="math inline">\(S_i\)</span>: all strategies for <span class="math inline">\(i\)</span></p><p><span class="math inline">\(S = S_1 \times \cdots \times S_n\)</span>: all strategy profiles</p><ul><li>expected utility</li></ul><p><span class="math display">\[u_i(s) = \sum_{a \in A} u_i(a) Pr(a \mid s)\]</span></p><p><span class="math display">\[Pr(a \mid s) = \prod_{j \in N} s_j(a_j)\]</span></p><ul><li><p>Theorem (Nash, 1950) 定理（纳什，1950）</p><p>Every finite game has a Nash equilibrium.</p><p>每一个有限博弈都有纳什均衡。</p></li></ul><h2 id="computing-mixed-nash-equilibrium">2-3 Computing Mixed Nash Equilibrium</h2><p>概率计算</p><h2 id="hardness-beyond-2x2-games---basic">2-4 Hardness Beyond 2x2 Games - Basic</h2><h2 id="hardness-beyond-2x2-games---advanced">2-4 Hardness Beyond 2x2 Games - Advanced</h2><h2 id="example-mixed-strategy-nash">2-5 Example: Mixed Strategy Nash</h2><h2 id="data-professional-sports-and-mixed-strategies">2-6 Data: Professional Sports and Mixed Strategies</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 2: Mixed-Strategy Nash Equilibrium&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/2&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/2&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 1 Introduction and Overview</title>
    <link href="http://lijiancheng0614.github.io/2018/06/11/2018_06_11_Game_Theory_Week_1/"/>
    <id>http://lijiancheng0614.github.io/2018/06/11/2018_06_11_Game_Theory_Week_1/</id>
    <published>2018-06-10T16:00:00.000Z</published>
    <updated>2018-06-15T08:46:44.423Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 1: Introduction and Overview</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/1" class="uri" target="_blank" rel="external">https://www.coursera.org/learn/game-theory-1/home/week/1</a></p><a id="more"></a><h2 id="game-theory-intro---tcp-backoff">1-1 Game Theory Intro - TCP Backoff</h2><p>从 TCP 协议中的退避机制（backoff mechanisn）引出博弈：</p><p>A game in general is any interaction between two or more people where the outcomes of the interaction depend on what everybody does and everybody has different levels of happiness for the different outcomes.</p><p>博弈就是两或多人间的互动，其中互动的结果取决于每个人的行为，并且每个人对于不同的结果都会有不同的愉悦度。</p><h2 id="self-interested-agents-and-utility-theory">1-2 Self-Interested Agents and Utility Theory</h2><p>utility function is, a mathematical measure that tells you how much the agent likes or does not like a given situation.</p><p>收益函数是指：一个数学的评估方法，用于决定一个代理对于特定情况的喜恶程度。</p><p>And the decision theoretic approach which is what underlies modern game theory, says that you’re going to try to act in the way that maximizes your expected or average utility.</p><p>对于这种喜好的理论决定方法，就是现代博弈论的基础。这个基础就是，每个人都试图将期望效益最大化。</p><h2 id="defining-games">1-3 Defining Games</h2><ul><li><p>Normal Form (a.k.a. Matrix Form, Strategic Form)</p><p>Finite, n-person normal form game: <span class="math inline">\(&lt;N, A, u&gt;\)</span></p><ul><li><p>Players</p><p><span class="math inline">\(N = \{1, \cdots, n\}\)</span></p></li><li><p>Actions</p><p>Action set for player <span class="math inline">\(i\)</span>: <span class="math inline">\(A_i\)</span></p></li><li><p>Payoffs</p><p>Utility function or Payoff function for player <span class="math inline">\(i\)</span>: <span class="math inline">\(u_i: A \mapsto \mathbb{R}\)</span></p></li></ul></li><li><p>Extensive Form</p><ul><li><p>Timing</p></li><li><p>Information</p></li></ul></li></ul><h2 id="examples-of-games">1-4 Examples of Games</h2><ul><li><p>Games of Cooperation</p></li><li><p>Coordination Game</p></li><li><p>General Games</p></li></ul><h2 id="nash-equilibrium-intro">1-5 Nash Equilibrium Intro</h2><ul><li><p>Keynes Beauty Contest Game</p></li><li><p>Keynes Beauty Contest Game: The Stylized Version</p></li></ul><h2 id="strategic-reasoning">1-6 Strategic Reasoning</h2><p>Nash Equilibrium:</p><p>纳什均衡：</p><ul><li><p>A consistent list of actions</p></li><li><p>Each player’s action maximizes his or her payoff given the actions of the others</p><p>给定其他人的行为，每个玩家的行为都会最大化其收益</p></li><li><p>A self-consistent or stable profile</p></li></ul><h2 id="best-response-and-nash-equilibrium">1-7 Best Response and Nash Equilibrium</h2><p>Let <span class="math inline">\(a_{-i} = &lt;a_1, \cdots, a_{i - 1}, a_{i + 1}, \cdots, a_n&gt;\)</span></p><p>then <span class="math inline">\(a = (a_{-i}, a_i)\)</span></p><ul><li><p>Best Response</p><p><span class="math inline">\(a_i^* \in BR(a_{-i}) \iff \forall a_i \in A_i, u_i(a_i^*, a_{-i}) \ge u_i(a_i, a_{-i})\)</span></p></li><li><p>Nash Equilibrium</p><p><span class="math inline">\(a = &lt;a_1, \cdots, a_n&gt;\)</span> is a (“pure strategy”) Nash equilibrium iff <span class="math inline">\(\forall i, a_i \in BR(a_{-i})\)</span></p></li></ul><h2 id="nash-equilibrium-of-example-games">1-8 Nash Equilibrium of Example Games</h2><h2 id="dominant-strategies">1-9 Dominant Strategies</h2><p>2 strategies <span class="math inline">\(s_i\)</span>, <span class="math inline">\(s&#39;_i\)</span></p><p>the set of all possible strategy profiles for the other players: <span class="math inline">\(S_{-i}\)</span></p><ul><li><p><span class="math inline">\(s_i\)</span> strictly dominates <span class="math inline">\(s&#39;_i\)</span></p><p>if <span class="math inline">\(\forall s_{-i} \in S_{-i}, u_i(s_i, s_{-i}) \gt u_i(s&#39;_i, s_{-i})\)</span></p></li><li><p><span class="math inline">\(s_i\)</span> very weakly dominates <span class="math inline">\(s&#39;_i\)</span></p><p>if <span class="math inline">\(\forall s_{-i} \in S_{-i}, u_i(s_i, s_{-i}) \ge u_i(s&#39;_i, s_{-i})\)</span></p></li><li><p>dominant 主导</p><p>If one strategy dominates all others.</p></li><li><p>A strategy profile consisting of dominant strategies for every player must be a Nash equilibrium.</p></li></ul><h2 id="pareto-optimality">1-10 Pareto Optimality</h2><ul><li><p><span class="math inline">\(o\)</span> Pareto-dominate <span class="math inline">\(o&#39;\)</span></p><p>one outcome <span class="math inline">\(o\)</span> is at least as good for every agent as another outcome <span class="math inline">\(o&#39;\)</span>.</p></li><li><p><span class="math inline">\(o^*\)</span> is Pareto Optimality 帕累托最优</p><p>if there is no other outcome that Pareto-dominates it.</p></li></ul><p>Every game has at least 1 Pareto-optimal outcome, and can have more than one Pareto-optimal outcome.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 1: Introduction and Overview&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/1&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/1&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>文本自动摘要</title>
    <link href="http://lijiancheng0614.github.io/2018/05/12/2018_05_12_text_summarization/"/>
    <id>http://lijiancheng0614.github.io/2018/05/12/2018_05_12_text_summarization/</id>
    <published>2018-05-11T16:00:00.000Z</published>
    <updated>2018-06-11T16:22:18.514Z</updated>
    
    <content type="html"><![CDATA[<p>文本自动摘要（自动文摘）Text Summarization 指自动地从原始文档中提取摘要，摘要是全面准确地反映该文档中心内容的简单连贯的短文。</p><a id="more"></a><h2 id="应用">应用</h2><p>学术文献、 会议记录、 电影剧本、学生反馈、软件代码、 直播文字</p><h2 id="评价指标">评价指标</h2><h3 id="人工评价">人工评价</h3><p>时间成本太高，效率太低</p><h3 id="自动评价">自动评价</h3><p>给定参考摘要作为标准答案，通过制定一些规则来给生产的摘要打分。</p><p>ROUGE 系统（Recall-Oriented Understudy for Gisting Evaluation）：将待审的摘要和参考摘要的 n 元组共现统计量作为评价依据，通过一系列标准进行打分。</p><p>包括 ROUGE-N (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4), ROUGE-L, ROUGE-W，ROUGE-S, ROUGE-SU</p><h2 id="方法">方法</h2><h3 id="抽取式摘要-extraction-based-summarization">抽取式摘要 Extraction-based summarization</h3><p>从原文中找到一些关键的句子，组合成一篇摘要。</p><ol style="list-style-type: decimal"><li><p>基于特征</p><p>统计句子包含的关键词数量、关键词位置、句子长度、句子位置等。</p><p>方法：TextTeaser</p><p>论文：</p><ul><li><p>(IBM Journal 1958) The Automatic Creation of Literature Abstracts</p></li><li><p>(Journal of the ACM 1969) New Methods in Automatic Extracting</p></li><li><p>(SIGIR 2001) Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis</p></li></ul></li><li><p>基于图排序</p><p>将文档的每句话作为节点，句子之间的相似度作为边权，构建图模型，计算每个句子的得分。</p><p>方法：LexRank, TextRank</p><p>论文：</p><ul><li><p>(JAIR 2004) LexRank: Graph-based Lexical Centrality as Salience in Text Summarization</p></li><li><p>(EMNLP 2004) TextRank: Bringing Order into Texts</p></li></ul></li><li><p>神经网络</p><p>方法：Attention Model, RNN, CNN</p><p>论文：</p><ul><li>(ACL 2016) Neural Summarization by Extracting Sentences and Words</li></ul></li></ol><h3 id="综合式摘要-abstractive-summarization">综合式摘要 Abstractive Summarization</h3><p>理解原文并用简洁文本表达。</p><p>方法：</p><ul><li><p>Encoder-Decoder 框架</p><p>Encoder 是将输入序列表示成一个带有语义的向量，通常使用 LSTM、GRU 等 RNN 模型，复杂的也有 BiRNN、BiRNN with LSTM、BiRNN with GRU、多层RNN等模型。</p><p>Decoder 是以 Encoder 输出的向量作为输入，并输出目标文本序列，本质上是一个语言模型，通常使用 Recurrent Neural Network Language Model (RNNLM)，同样也会用 LSTM、GRU 等模型。</p></li><li><p>Attention Mechanism</p><p>Encoder 输出的向量更多地表示输入序列中最后一个单词的意思，因此加入注意力机制有助于该向量更多地关注其中重要的单词。</p></li><li><p>整体思路</p><ol style="list-style-type: decimal"><li><p>将自动文摘问题构造成 seq2seq 问题，一种做法是将某段文本的第一个句子作为输入，headlines 作为输出，变成 headlines generative 问题。</p></li><li><p>选择大规模语料库作为数据集。</p></li><li><p>选择合适的 Encoder。</p></li><li><p>选择合适的 Decoder。</p></li><li><p>设计合适的 attention model。</p></li><li><p>设计 copy net。由于测试时部分词汇可能不在训练的单词表里，因此需要用 copy net 将输入的词 copy 到最终输出。</p></li></ol></li></ul><p>论文：</p><ol style="list-style-type: decimal"><li><p>(EMNLP 2015) A Neural Attention Model for Abstractive Sentence Summarization</p></li><li><p>(ICLR 2018) A Deep Reinforced Model for Abstractive Summarization</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本自动摘要（自动文摘）Text Summarization 指自动地从原始文档中提取摘要，摘要是全面准确地反映该文档中心内容的简单连贯的短文。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>DuReader</title>
    <link href="http://lijiancheng0614.github.io/2018/05/02/2018_05_02_DuReader/"/>
    <id>http://lijiancheng0614.github.io/2018/05/02/2018_05_02_DuReader/</id>
    <published>2018-05-01T16:00:00.000Z</published>
    <updated>2018-06-09T05:16:37.058Z</updated>
    
    <content type="html"><![CDATA[<p>DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</p><p>Paper: <a href="https://arxiv.org/abs/1711.05073" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1711.05073</a></p><p>Page: <a href="http://ai.baidu.com/broad/subordinate?dataset=dureader" class="uri" target="_blank" rel="external">http://ai.baidu.com/broad/subordinate?dataset=dureader</a></p><p>Code: <a href="https://github.com/baidu/DuReader/" class="uri" target="_blank" rel="external">https://github.com/baidu/DuReader/</a></p><a id="more"></a><p>DuReader，一个新的大型开放中文机器阅读理解数据集。</p><p>DuReader 与以前的 MRC 数据集相比有三个优势：</p><ol style="list-style-type: decimal"><li><p>数据来源：问题和文档均基于百度搜索和百度知道; 答案是手动生成的。</p></li><li><p>问题类型：它为更多的问题类型提供了丰富的注释，特别是是非类和观点类问题。</p></li><li><p>规模：包含 200K 个问题，420K 个答案和 1M 个文档; 是目前最大的中文 MRC 数据集。</p></li></ol><p>This paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC.</p><p>DuReader has three advantages over previous MRC datasets:</p><ol style="list-style-type: decimal"><li><p>data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated.</p></li><li><p>question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community.</p></li><li><p>scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far.</p></li></ol><h2 id="introduction">Introduction</h2><table><thead><tr class="header"><th>Dataset</th><th>Lang</th><th>#Que.</th><th>#Docs</th><th>Source of Que.</th><th>Source of Docs</th><th>Answer Type</th></tr></thead><tbody><tr class="odd"><td>CNN/DM</td><td>EN</td><td>1.4M</td><td>300K</td><td>Synthetic cloze</td><td>News</td><td>Fill in entity</td></tr><tr class="even"><td>HLF-RC</td><td>ZH</td><td>100K</td><td>28K</td><td>Synthetic cloze</td><td>Fairy/News</td><td>Fill in word</td></tr><tr class="odd"><td>CBT</td><td>EN</td><td>688K</td><td>108</td><td>Synthetic cloze</td><td>Children’s books</td><td>Multi. choices</td></tr><tr class="even"><td>RACE</td><td>EN</td><td>870K</td><td>50K</td><td>English exam</td><td>English exam</td><td>Multi. choices</td></tr><tr class="odd"><td>MCTest</td><td>EN</td><td>2K</td><td>500</td><td>Crowd-sourced</td><td>Fictional stories</td><td>Multi. choices</td></tr><tr class="even"><td>NewsQA</td><td>EN</td><td>100K</td><td>10K</td><td>Crowd-sourced</td><td>CNN</td><td>Span of words</td></tr><tr class="odd"><td>SQuAD</td><td>EN</td><td>100K</td><td>536</td><td>Crowd-sourced</td><td>Wiki.</td><td>Span of words</td></tr><tr class="even"><td>SearchQA</td><td>EN</td><td>140K</td><td>6.9M</td><td>QA site</td><td>Web doc.</td><td>Span of words</td></tr><tr class="odd"><td>TrivaQA</td><td>EN</td><td>40K</td><td>660K</td><td>Trivia websites</td><td>Wiki./Web doc.</td><td>Span/substring of words</td></tr><tr class="even"><td>NarrativeQA</td><td>EN</td><td>46K</td><td>1.5K</td><td>Crowd-sourced</td><td>Book&amp;movie</td><td>Manual summary</td></tr><tr class="odd"><td>MS-MARCO</td><td>EN</td><td>100K</td><td>200K</td><td>User logs</td><td>Web doc.</td><td>Manual summary</td></tr><tr class="even"><td>DuReader</td><td>ZH</td><td>200k</td><td>1M</td><td>User logs</td><td>Web doc./CQA</td><td>Manual summary</td></tr></tbody></table><p>表 1: 机器阅读理解数据集对比</p><h2 id="pilot-study">Pilot Study</h2><table><colgroup><col width="17%"><col width="41%"><col width="41%"></colgroup><thead><tr class="header"><th>Examples</th><th>Fact</th><th>Opinion</th></tr></thead><tbody><tr class="odd"><td>Entity</td><td>iphone哪天发布</td><td>2017最好看的十部电影</td></tr><tr class="even"><td>-</td><td>On which day will iphone be released</td><td>Top 10 movies of 2017</td></tr><tr class="odd"><td>Description</td><td>消防车为什么是红的</td><td>丰田卡罗拉怎么样</td></tr><tr class="even"><td>-</td><td>Why are firetrucks red</td><td>How is Toyota Carola</td></tr><tr class="odd"><td>YesNo</td><td>39.5度算高烧吗</td><td>学围棋能开发智力吗</td></tr><tr class="even"><td>-</td><td>Is 39.5 degree a high fever</td><td>Does learning to play go improve intelligence</td></tr></tbody></table><p>表 2: 中文六类问题的例子</p><h2 id="scaling-up-from-the-pilot-to-dureader">Scaling up from the Pilot to DuReader</h2><h3 id="data-collection-and-annotation">Data Collection and Annotation</h3><h4 id="data-collection">Data Collection</h4><p>DuReader 的样本可用四元组表示： <span class="math inline">\(\{q, t, D, A\}\)</span>，其中 <span class="math inline">\(q\)</span> 是问题，<span class="math inline">\(t\)</span> 是问题类型，<span class="math inline">\(D\)</span> 是相关文档集合，<span class="math inline">\(A\)</span> 是由人类标注产生的答案集合。</p><p>The DuReader is a sequence of 4-tuples: <span class="math inline">\(\{q, t, D, A\}\)</span>, where <span class="math inline">\(q\)</span> is a question, <span class="math inline">\(t\)</span> is a question type, <span class="math inline">\(D\)</span> is a set of relevant documents, and <span class="math inline">\(A\)</span> is an answer set produced by human annotators.</p><h4 id="question-type-annotation">Question Type Annotation</h4><h4 id="answer-annotation">Answer Annotation</h4><p>众包</p><p>Crowd-sourcing</p><h4 id="quality-control">Quality Control</h4><h4 id="training-development-and-test-sets">Training, Development and Test Sets</h4><table><thead><tr class="header"><th>数量</th><th>训练集</th><th>开发集</th><th>测试集</th></tr></thead><tbody><tr class="odd"><td>问题</td><td>181K</td><td>10K</td><td>10K</td></tr><tr class="even"><td>文档</td><td>855K</td><td>45K</td><td>46K</td></tr><tr class="odd"><td>答案</td><td>376K</td><td>20K</td><td>21K</td></tr></tbody></table><p>The training, development and test sets consist of 181K, 10K and 10K questions, 855K, 45K and 46K documents, 376K, 20K and 21K answers, respectively.</p><h3 id="dureader-is-relatively-challenging">DuReader is (Relatively) Challenging</h3><p>challenges:</p><ol style="list-style-type: decimal"><li><p>The number of answers.</p><div class="figure"><img src="figure1.png"></div><p>图 1. 答案数量分布</p></li><li><p>The edit distance.</p><p>人类生成的答案和源文档之间的差异很大。</p><p>the difference between the human generated answers and the source documents is large.</p></li><li><p>The document length.</p><p>问题平均 4.8 词，答案平均 69.6 词，文档平均 396 词。</p><p>In DuReader, questions tend to be short (4.8 words on average) compared to answers (69.6 words), and answers tend to be short compared to documents (396 words on average).</p></li></ol><h2 id="experiments">Experiments</h2><h3 id="baseline-systems">Baseline Systems</h3><ol style="list-style-type: decimal"><li><p>从每个文件中选择一个最相关的段落</p></li><li><p>在选定的段落中应用最先进的 MRC 模型</p></li></ol><p>our designed systems have two steps:</p><ol style="list-style-type: decimal"><li><p>select one most related paragraph from each document</p></li><li><p>apply the state-of-the-art MRC models on the selected paragraphs</p></li></ol><h3 id="paragraph-selection">Paragraph Selection</h3><p>在训练阶段，我们从文档中选择与人类生成答案重叠最大的段落作为最相关段落。</p><p>In training stage, we select one paragraph from a document as the most relevant one, if the paragraph has the largest overlap with human generated answer.</p><p>在测试阶段，由于我们没有人类生成答案，我们选择与问题重叠最大的段落作为最相关段落。</p><p>In testing stage, since we have no human generated answer, we select the most relevant paragraph that has the largest overlap with the corresponding question.</p><h3 id="answer-span-selection">Answer Span Selection</h3><ul><li><p>Match-LSTM</p><p>要在段落中找到答案，它会按顺序遍历段落，并动态地将注意力加权问题表示与段落的每个标记进行匹配。</p><p>最后，使用答案指针层来查找段落中的答案范围。</p><p>To find an answer in a paragraph, it goes through the paragraph sequentially and dynamically aggregates the matching of an attention-weighted question representation to each token of the paragraph.</p><p>Finally, an answer pointer layer is used to find an answer span in the paragraph.</p></li><li><p>BiDAF</p><p>它使用上下文对问题的关注和问题对上下文的关注，以突出问题和上下文中的重要部分。</p><p>之后，使用注意流层来融合所有有用的信息，以获得每个位置的向量表示。</p><p>It uses both context-to-question attention and question-to-context attention in order to highlight the important parts in both question and context.</p><p>After that, the so-called attention flow layer is used to fuse all useful information in order to get a vector representation for each position.</p></li></ul><h3 id="results-and-analysis">Results and Analysis</h3><p>评价方法：BLEU-4, Rouge-L</p><p>We evaluate the reading comprehension task via character-level BLEU-4 and Rouge-L.</p><table><thead><tr class="header"><th>Systems</th><th>Baidu Search</th><th>-</th><th>Baidu Zhidao</th><th>-</th><th>All</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Selected Paragraph</td><td>15.8</td><td>22.6</td><td>16.5</td><td>38.3</td><td>16.4</td><td>30.2</td></tr><tr class="odd"><td>Match-LSTM</td><td>23.1</td><td>31.2</td><td>42.5</td><td>48.0</td><td>31.9</td><td>39.2</td></tr><tr class="even"><td>BiDAF</td><td>23.1</td><td>31.1</td><td>42.2</td><td>47.5</td><td>31.8</td><td>39.0</td></tr><tr class="odd"><td>Human</td><td>55.1</td><td>54.4</td><td>57.1</td><td>60.7</td><td>56.1</td><td>57.4</td></tr></tbody></table><p>Table 6: Performance of typical MRC systems on the DuReader.</p><table><thead><tr class="header"><th>Question type</th><th>Description</th><th>-</th><th>Entity</th><th>-</th><th>YesNo</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Match-LSTM</td><td>32.8</td><td>40.0</td><td>29.5</td><td>38.5</td><td>5.9</td><td>7.2</td></tr><tr class="odd"><td>BiDAF</td><td>32.6</td><td>39.7</td><td>29.8</td><td>38.4</td><td>5.5</td><td>7.5</td></tr><tr class="even"><td>Human</td><td>58.1</td><td>58.0</td><td>44.6</td><td>52.0</td><td>56.2</td><td>57.4</td></tr></tbody></table><p>Table 8: Performance on various question types.</p><h3 id="opinion-aware-evaluation">Opinion-aware Evaluation</h3><table><thead><tr class="header"><th>Question type</th><th>Fact</th><th>-</th><th>Opinion</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Opinion-unaware</td><td>6.3</td><td>8.3</td><td>5.0</td><td>7.1</td></tr><tr class="odd"><td>Opinion-aware</td><td>12.0</td><td>13.9</td><td>8.0</td><td>8.9</td></tr></tbody></table><p>Table 9: Performance of opinion-aware model on YesNo questions.</p><h3 id="discussion">Discussion</h3><h2 id="conclusion">Conclusion</h2><p>提出了 DuReader 数据集，提供了几个 baseline。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1711.05073&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1711.05073&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Page: &lt;a href=&quot;http://ai.baidu.com/broad/subordinate?dataset=dureader&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ai.baidu.com/broad/subordinate?dataset=dureader&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/baidu/DuReader/&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/baidu/DuReader/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>XPS 15 安装 Windows 10 + Ubuntu 双系统</title>
    <link href="http://lijiancheng0614.github.io/2018/04/29/2018_04_29_XPS15_Win10_Ubuntu/"/>
    <id>http://lijiancheng0614.github.io/2018/04/29/2018_04_29_XPS15_Win10_Ubuntu/</id>
    <published>2018-04-28T16:00:00.000Z</published>
    <updated>2018-04-30T15:35:42.981Z</updated>
    
    <content type="html"><![CDATA[<p>在 Dell XPS 15 上安装 Windows 10 + Ubuntu 双系统，主要把默认安装的 Windows 修改到 AHCI 模式下。</p><a id="more"></a><h2 id="安装环境">安装环境</h2><p>以下为成功安装的环境：</p><ul><li><p>Dell XPS 15</p></li><li><p>默认安装的 Windows 10</p></li><li><p>Ubuntu 16.04</p></li></ul><h2 id="安装过程">安装过程</h2><h3 id="把默认安装的-windows-修改到-ahci-模式下">把默认安装的 Windows 修改到 AHCI 模式下</h3><ol style="list-style-type: decimal"><li><p>在 Windows 下，以<strong>安全模式</strong>重启</p><blockquote><p>一种进入安全模式的方法：运行 <code>msconfig</code>，选择 <code>安全模式</code>，重启<br>其它方法：<a href="https://support.microsoft.com/zh-cn/help/12376/windows-10-start-your-pc-in-safe-mode" class="uri" target="_blank" rel="external">https://support.microsoft.com/zh-cn/help/12376/windows-10-start-your-pc-in-safe-mode</a></p></blockquote></li><li><p>重启，按 F2 进入启动设置</p><p>进入 System configuration &gt; SATA operation，把默认的 <code>RAID On</code> 选项改成 <code>AHCI</code></p></li><li><p>重启，等待 Windows 自动修复即可</p><blockquote><p>如在 <code>msconfig</code> 中修改了以安全模式进入，可修改回来</p></blockquote></li></ol><h3 id="安装-ubuntu">安装 Ubuntu</h3><ol style="list-style-type: decimal"><li><p>开机按 F2 进入启动设置</p><p>进入 General &gt; Boot Sequence，选择 <code>Boot List Option</code> 里的 <code>UEFI</code> 选项</p><p>进入 Secure Boot &gt; Secure Boot，选择 <code>Secure Boot Enable</code> 里的 <code>Disabled</code> 选项</p><blockquote><p>如需详细步骤见 <a href="http://www.dell.com/support/article/cn/zh/cndhs1/sln301754/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%88%B4%E5%B0%94pc%E4%B8%8A%E5%AE%89%E8%A3%85ubuntu%E5%92%8Cwindows8%E6%88%9610%E4%BD%9C%E4%B8%BA%E5%8F%8C%E5%BC%95%E5%AF%BC?lang=zh" target="_blank" rel="external">http://www.dell.com/support/article/cn/zh/cndhs1/sln301754/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%88%B4%E5%B0%94pc%E4%B8%8A%E5%AE%89%E8%A3%85ubuntu%E5%92%8Cwindows8%E6%88%9610%E4%BD%9C%E4%B8%BA%E5%8F%8C%E5%BC%95%E5%AF%BC?lang=zh</a></p></blockquote></li><li><p>在 Windows 下，进入磁盘管理，压缩卷预留一些空间以安装 Ubuntu</p><blockquote><p>如需详细步骤见 <a href="https://support.microsoft.com/zh-cn/help/944248" class="uri" target="_blank" rel="external">https://support.microsoft.com/zh-cn/help/944248</a><br>但不需要创建新分区，得到未分配空间即可<br><img src="https://support.microsoft.com/Library/Images/2474497.jpg"></p></blockquote></li><li><p>下载 Ubuntu 安装包，制作安装 Ubuntu 的 U盘启动盘，如使用 UltroISO 刻录</p><blockquote><p>下载地址 <a href="https://www.ubuntu.com/download/desktop" class="uri" target="_blank" rel="external">https://www.ubuntu.com/download/desktop</a><br>如得到 <code>ubuntu-16.04-desktop-amd64.iso</code></p></blockquote></li><li><p>重启，按 F12，选择 U盘启动</p></li><li><p>安装 Ubuntu</p><blockquote><p>基本一直按下一步<br>安装详细步骤见 <a href="https://tutorials.ubuntu.com/tutorial/tutorial-install-ubuntu-desktop" class="uri" target="_blank" rel="external">https://tutorials.ubuntu.com/tutorial/tutorial-install-ubuntu-desktop</a></p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Dell XPS 15 上安装 Windows 10 + Ubuntu 双系统，主要把默认安装的 Windows 修改到 AHCI 模式下。&lt;/p&gt;
    
    </summary>
    
      <category term="Default" scheme="http://lijiancheng0614.github.io/categories/Default/"/>
    
    
      <category term="Ubuntu" scheme="http://lijiancheng0614.github.io/tags/Ubuntu/"/>
    
      <category term="Windows" scheme="http://lijiancheng0614.github.io/tags/Windows/"/>
    
  </entry>
  
  <entry>
    <title>修改TensorFlow-DeepLab</title>
    <link href="http://lijiancheng0614.github.io/2018/03/16/2018_03_16_TensorFlow-DeepLab/"/>
    <id>http://lijiancheng0614.github.io/2018/03/16/2018_03_16_TensorFlow-DeepLab/</id>
    <published>2018-03-15T16:00:00.000Z</published>
    <updated>2018-03-18T16:34:25.531Z</updated>
    
    <content type="html"><![CDATA[<p>代码仓库：<a href="https://github.com/lijiancheng0614/tensorflow_deeplab" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/tensorflow_deeplab</a></p><p>修改TensorFlow DeepLab，添加一些方便使用或新的功能。</p><a id="more"></a><h2 id="中文">中文</h2><p>使用方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/lijiancheng0614/tensorflow_deeplab deeplab</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`</div><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`/models/research/slim</div></pre></td></tr></table></figure><h3 id="在eval.py中添加gpu_allow_growth参数">在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><code>eval.py</code></li></ul><h3 id="在train.py中添加gpu_allow_growth参数">在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><code>train.py</code></li></ul><h3 id="在train.py中添加max_to_keep参数">在<code>train.py</code>中添加<code>max_to_keep</code>参数</h3><p>在<code>train.py</code>中添加<code>max_to_keep</code>参数，默认为<code>5</code>，即保留最后5个checkpoint。如为<code>0</code>则保留所有的checkpoint。</p><p>修改文件：</p><ul><li><code>train.py</code></li></ul><h2 id="english">English</h2><p>Usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/lijiancheng0614/tensorflow_deeplab deeplab</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`</div><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`/models/research/slim</div></pre></td></tr></table></figure><h3 id="add-gpu_allow_growth-parameter-in-eval.py">Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="add-gpu_allow_growth-parameter-in-train.py">Add <code>gpu_allow_growth</code> parameter in <code>train.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>train.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="add-max_to_keep-parameter-in-train.py">Add <code>max_to_keep</code> parameter in <code>train.py</code></h3><p>Add <code>max_to_keep</code> parameter in <code>train.py</code>, default value is <code>5</code> which means the 5 most recent checkpoint files are kept. If <code>0</code>, all checkpoint files are kept.</p><p>Modified files:</p><ul><li><code>train.py</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码仓库：&lt;a href=&quot;https://github.com/lijiancheng0614/tensorflow_deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lijiancheng0614/tensorflow_deeplab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;修改TensorFlow DeepLab，添加一些方便使用或新的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow DeepLab进行语义分割</title>
    <link href="http://lijiancheng0614.github.io/2018/03/13/2018_03_13_TensorFlow-DeepLab/"/>
    <id>http://lijiancheng0614.github.io/2018/03/13/2018_03_13_TensorFlow-DeepLab/</id>
    <published>2018-03-12T16:00:00.000Z</published>
    <updated>2018-03-18T16:31:58.168Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p><p>使用 TensorFlow DeepLab 进行语义分割</p><a id="more"></a><div class="figure"><img src="https://raw.githubusercontent.com/tensorflow/models/master/research/deeplab/g3doc/img/vis1.png"></div><h2 id="准备">准备</h2><ol start="0" style="list-style-type: decimal"><li><p>文件结构</p><p>这里以 PASCAL VOC 2012 为例，参考官方推荐的文件结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">deeplab/datasets/pascal_voc_seg</div><div class="line">├── exp</div><div class="line">│   └── train_on_train_set</div><div class="line">│       ├── eval</div><div class="line">│       │   └── events.out.tfevents....</div><div class="line">│       ├── export</div><div class="line">│       │   └── frozen_inference_graph.pb</div><div class="line">│       ├── train</div><div class="line">│       │   ├── checkpoint</div><div class="line">│       │   ├── events.out.tfevents....</div><div class="line">│       │   ├── graph.pbtxt</div><div class="line">│       │   ├── model.ckpt-0.data-00000-of-00001</div><div class="line">│       │   ├── model.ckpt-0.index</div><div class="line">│       │   ├── model.ckpt-0.meta</div><div class="line">│       │   └── ...</div><div class="line">│       └── vis</div><div class="line">│           ├── graph.pbtxt</div><div class="line">│           ├── raw_segmentation_results</div><div class="line">│           └── segmentation_results</div><div class="line">├── init_models</div><div class="line">│   └── deeplabv3_pascal_train_aug</div><div class="line">│       ├── frozen_inference_graph.pb</div><div class="line">│       ├── model.ckpt.data-00000-of-00001</div><div class="line">│       └── model.ckpt.index</div><div class="line">├── tfrecord</div><div class="line">│   ├── ....tfrecord</div><div class="line">│   └── ...</div><div class="line">└── VOCdevkit</div><div class="line">    └── VOC2012</div><div class="line">        ├── Annotations</div><div class="line">        ├── ImageSets</div><div class="line">        │   ├── Action</div><div class="line">        │   ├── Layout</div><div class="line">        │   ├── Main</div><div class="line">        │   └── Segmentation</div><div class="line">        ├── JPEGImages</div><div class="line">        ├── SegmentationClass</div><div class="line">        ├── SegmentationClassRaw</div><div class="line">        └── SegmentationObject</div></pre></td></tr></table></figure></li><li><p>安装 TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/install/</a> ，安装 TensorFlow v1.5.0 或更新的版本。</p><p>如果操作系统、GPU 型号、Python 版本号等配置跟官方一致，可直接使用官网提供的安装包安装。</p><blockquote><p>编译源码时注意 bazel 可能并不能总是获取 <code>$LD_LIBRARY_PATH</code>，如有报错，可以尝试添加参数 <code>action_env</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bazel build --config=opt --config=cuda tensorflow/tools/pip_package:build_pip_package --action_env=&quot;LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;&quot;</div></pre></td></tr></table></figure></p></blockquote></li><li><p>配置 TensorFlow Models</p><ul><li>下载 TensorFlow Models</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div></pre></td></tr></table></figure><ul><li>添加 <code>$PYTHONPATH</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/research/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/research/</span></div><div class="line">python deeplab/model_test.py</div></pre></td></tr></table></figure><p>若成功，显示<code>OK</code>。</p></li><li><p>准备数据</p><p>这里以 <code>PASCAL VOC 2012</code> 为例。</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md</a></p><p>运行以下代码即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/</span></div><div class="line">sh download_and_convert_voc2012.sh</div></pre></td></tr></table></figure><p>实际上，该脚本执行了以下操作：</p><ul><li>下载并解压</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/</span></div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</div><div class="line">tar -xf VOCtrainval_11-May-2012.tar</div></pre></td></tr></table></figure><ul><li>移除 ground-truth 中的 colormap</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/</span></div><div class="line">PASCAL_ROOT=<span class="string">"pascal_voc_seg/VOCdevkit/VOC2012"</span></div><div class="line">SEG_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/SegmentationClass"</span></div><div class="line">SEMANTIC_SEG_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/SegmentationClassRaw"</span></div><div class="line">python ./remove_gt_colormap.py \</div><div class="line">    --original_gt_folder=<span class="string">"<span class="variable">$&#123;SEG_FOLDER&#125;</span>"</span> \</div><div class="line">    --output_dir=<span class="string">"<span class="variable">$&#123;SEMANTIC_SEG_FOLDER&#125;</span>"</span></div></pre></td></tr></table></figure><ul><li>生成 TFRecord</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/</span></div><div class="line">OUTPUT_DIR=<span class="string">"pascal_voc_seg/tfrecord"</span></div><div class="line">mkdir -p <span class="string">"<span class="variable">$&#123;OUTPUT_DIR&#125;</span>"</span></div><div class="line">IMAGE_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/JPEGImages"</span></div><div class="line">LIST_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/ImageSets/Segmentation"</span></div><div class="line">python ./build_voc2012_data.py \</div><div class="line">    --image_folder=<span class="string">"<span class="variable">$&#123;IMAGE_FOLDER&#125;</span>"</span> \</div><div class="line">    --semantic_segmentation_folder=<span class="string">"<span class="variable">$&#123;SEMANTIC_SEG_FOLDER&#125;</span>"</span> \</div><div class="line">    --list_folder=<span class="string">"<span class="variable">$&#123;LIST_FOLDER&#125;</span>"</span> \</div><div class="line">    --image_format=<span class="string">"jpg"</span> \</div><div class="line">    --output_dir=<span class="string">"<span class="variable">$&#123;OUTPUT_DIR&#125;</span>"</span></div></pre></td></tr></table></figure></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型（ <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</a> ），</p><p>这里以 <code>deeplabv3_pascal_train_aug_2018_01_04</code> 以例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/</span></div><div class="line">mkdir init_models</div><div class="line"><span class="built_in">cd</span> init_models</div><div class="line">wget http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz</div><div class="line">tar zxf ssd_mobilenet_v1_coco_11_06_2017.tar.gz</div></pre></td></tr></table></figure></li></ol><h2 id="训练">训练</h2><p>如果使用现有模型进行预测则不需要训练。</p><ol style="list-style-type: decimal"><li><p>训练</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">mkdir -p logs/</div><div class="line">now=$(date +<span class="string">"%Y%m%d_%H%M%S"</span>)</div><div class="line">python ../../../../train.py \</div><div class="line">    --logtostderr \</div><div class="line">    --train_split=<span class="string">"train"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --train_crop_size=513 \</div><div class="line">    --train_crop_size=513 \</div><div class="line">    --train_batch_size=4 \</div><div class="line">    --training_number_of_steps=10 \</div><div class="line">    --fine_tune_batch_norm=<span class="literal">false</span> \</div><div class="line">    --tf_initial_checkpoint=<span class="string">"../../init_models/deeplabv3_pascal_train_aug/model.ckpt"</span> \</div><div class="line">    --train_logdir=<span class="string">"train/"</span> \</div><div class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> 2&gt;&amp;1 | tee logs/train_<span class="variable">$now</span>.txt &amp;</div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh train.sh</code> 即可训练。</p></li><li><p>验证</p><p>可一边训练一边验证，注意使用其它的GPU或合理分配显存。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/eval.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">python ../../../../eval.py \</div><div class="line">    --logtostderr \</div><div class="line">    --eval_split=<span class="string">"val"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --eval_crop_size=513 \</div><div class="line">    --eval_crop_size=513 \</div><div class="line">    --checkpoint_dir=<span class="string">"train/"</span> \</div><div class="line">    --eval_logdir=<span class="string">"eval/"</span> \</div><div class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> &amp;</div><div class="line">    <span class="comment"># --max_number_of_evaluations=1 &amp;</span></div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>CUDA_VISIBLE_DEVICES=&quot;1&quot; sh eval.sh</code> 即可验证（这里指定了第二个 GPU）。</p></li><li><p>可视化 log</p><p>可一边训练一边可视化训练的 log，访问 <code>http://localhost:6006/</code> 即可看到 loss 等的变化。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></div><div class="line">tensorboard --logdir train/</div></pre></td></tr></table></figure><p>可视化验证的 log，可看到 <code>miou_1.0</code> 的变化，这里指定了另一个端口。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></div><div class="line">tensorboard --logdir <span class="built_in">eval</span>/ --port 6007</div></pre></td></tr></table></figure><p>或同时可视化训练与验证的log：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></div><div class="line">tensorboard --logdir .</div></pre></td></tr></table></figure></li><li><p>可视化分割结果</p><p>可一边训练一边可视化分割结果。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/vis.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">python ../../../../vis.py \</div><div class="line">    --logtostderr \</div><div class="line">    --vis_split=<span class="string">"val"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --vis_crop_size=513 \</div><div class="line">    --vis_crop_size=513 \</div><div class="line">    --checkpoint_dir=<span class="string">"train/"</span> \</div><div class="line">    --vis_logdir=<span class="string">"vis/"</span> \</div><div class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> &amp;</div><div class="line">    <span class="comment"># --max_number_of_evaluations=1 &amp;</span></div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh vis.sh</code> 即可生成分割结果，<code>vis/segmentation_results/</code> 里有彩色化的分割结果，<code>vis/raw_segmentation_results/</code> 里有原始的分割结果。</p></li></ol><h2 id="测试">测试</h2><ol style="list-style-type: decimal"><li><p>导出模型</p><p>训练完成后得到一些 checkpoint 文件在 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train/</code> 中，如：</p><ul><li>graph.pbtxt</li><li>model.ckpt-1000.data-00000-of-00001</li><li>model.ckpt-1000.info</li><li>model.ckpt-1000.meta</li></ul><p>其中 meta 文件保存了 graph 和 metadata，ckpt 文件保存了网络的 weights。</p><p>而进行预测时只需模型和权重，不需要 metadata，故可使用官方提供的脚本生成推导图。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/export_model.sh</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">python ../../../../export_model.py \</div><div class="line">    --logtostderr \</div><div class="line">    --checkpoint_path=<span class="string">"train/model.ckpt-<span class="variable">$1</span>"</span> \</div><div class="line">    --export_path=<span class="string">"export/frozen_inference_graph-<span class="variable">$1</span>.pb"</span> \</div><div class="line">    --model_variant=<span class="string">"xception_65"</span> \</div><div class="line">    --atrous_rates=6 \</div><div class="line">    --atrous_rates=12 \</div><div class="line">    --atrous_rates=18 \</div><div class="line">    --output_stride=16 \</div><div class="line">    --decoder_output_stride=4 \</div><div class="line">    --num_classes=21 \</div><div class="line">    --crop_size=513 \</div><div class="line">    --crop_size=513 \</div><div class="line">    --inference_scales=1.0</div></pre></td></tr></table></figure><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh export_model.sh 1000</code> 即可导出模型 <code>export/frozen_inference_graph-1000.pb</code>。</p></li><li><p>测试图片</p><ul><li><p>运行 <code>deeplab_demo.ipynb</code> 并修改其中的各种路径即可。</p></li><li><p>或自写 inference 脚本，如 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/infer.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">'../../../../utils/'</span>)</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> get_dataset_colormap</div><div class="line"></div><div class="line">LABEL_NAMES = np.asarray([</div><div class="line">    <span class="string">'background'</span>, <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>, <span class="string">'bottle'</span>, <span class="string">'bus'</span>,</div><div class="line">    <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>, <span class="string">'cow'</span>, <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>, <span class="string">'motorbike'</span>,</div><div class="line">    <span class="string">'person'</span>, <span class="string">'pottedplant'</span>, <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tv'</span></div><div class="line">])</div><div class="line"></div><div class="line">FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), <span class="number">1</span>)</div><div class="line">FULL_COLOR_MAP = get_dataset_colormap.label_to_color_image(FULL_LABEL_MAP)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepLabModel</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Class to load deeplab model and run inference."""</span></div><div class="line"></div><div class="line">    INPUT_TENSOR_NAME = <span class="string">'ImageTensor:0'</span></div><div class="line">    OUTPUT_TENSOR_NAME = <span class="string">'SemanticPredictions:0'</span></div><div class="line">    INPUT_SIZE = <span class="number">513</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_path)</span>:</span></div><div class="line">        <span class="string">"""Creates and loads pretrained deeplab model."""</span></div><div class="line">        self.graph = tf.Graph()</div><div class="line">        <span class="keyword">with</span> open(model_path) <span class="keyword">as</span> fd:</div><div class="line">            graph_def = tf.GraphDef.FromString(fd.read())</div><div class="line">        <span class="keyword">with</span> self.graph.as_default():</div><div class="line">            tf.import_graph_def(graph_def, name=<span class="string">''</span>)</div><div class="line">        self.sess = tf.Session(graph=self.graph)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, image)</span>:</span></div><div class="line">        <span class="string">"""Runs inference on a single image.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            image: A PIL.Image object, raw input image.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            resized_image: RGB image resized from original input image.</span></div><div class="line"><span class="string">            seg_map: Segmentation map of `resized_image`.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        width, height = image.size</div><div class="line">        resize_ratio = <span class="number">1.0</span> * self.INPUT_SIZE / max(width, height)</div><div class="line">        target_size = (int(resize_ratio * width), int(resize_ratio * height))</div><div class="line">        resized_image = image.convert(<span class="string">'RGB'</span>).resize(target_size,</div><div class="line">                                                    Image.ANTIALIAS)</div><div class="line">        batch_seg_map = self.sess.run(</div><div class="line">            self.OUTPUT_TENSOR_NAME,</div><div class="line">            feed_dict=&#123;</div><div class="line">                self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]</div><div class="line">            &#125;)</div><div class="line">        seg_map = batch_seg_map[<span class="number">0</span>]</div><div class="line">        <span class="keyword">return</span> resized_image, seg_map</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_segmentation</span><span class="params">(image, seg_map)</span>:</span></div><div class="line">    plt.figure()</div><div class="line"></div><div class="line">    plt.subplot(<span class="number">221</span>)</div><div class="line">    plt.imshow(image)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.title(<span class="string">'input image'</span>)</div><div class="line"></div><div class="line">    plt.subplot(<span class="number">222</span>)</div><div class="line">    seg_image = get_dataset_colormap.label_to_color_image(</div><div class="line">        seg_map, get_dataset_colormap.get_pascal_name()).astype(np.uint8)</div><div class="line">    plt.imshow(seg_image)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.title(<span class="string">'segmentation map'</span>)</div><div class="line"></div><div class="line">    plt.subplot(<span class="number">223</span>)</div><div class="line">    plt.imshow(image)</div><div class="line">    plt.imshow(seg_image, alpha=<span class="number">0.7</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.title(<span class="string">'segmentation overlay'</span>)</div><div class="line"></div><div class="line">    unique_labels = np.unique(seg_map)</div><div class="line">    ax = plt.subplot(<span class="number">224</span>)</div><div class="line">    plt.imshow(</div><div class="line">        FULL_COLOR_MAP[unique_labels].astype(np.uint8),</div><div class="line">        interpolation=<span class="string">'nearest'</span>)</div><div class="line">    ax.yaxis.tick_right()</div><div class="line">    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])</div><div class="line">    plt.xticks([], [])</div><div class="line">    ax.tick_params(width=<span class="number">0</span>)</div><div class="line"></div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</div><div class="line">        print(<span class="string">'Usage: python &#123;&#125; image_path model_path'</span>.format(sys.argv[<span class="number">0</span>]))</div><div class="line">        exit()</div><div class="line"></div><div class="line">    image_path = sys.argv[<span class="number">1</span>]</div><div class="line">    model_path = sys.argv[<span class="number">2</span>]</div><div class="line">    model = DeepLabModel(model_path)</div><div class="line">    orignal_im = Image.open(image_path)</div><div class="line">    resized_im, seg_map = model.run(orignal_im)</div><div class="line">    vis_segmentation(resized_im, seg_map)</div></pre></td></tr></table></figure><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</span></div><div class="line">python infer.py \</div><div class="line">    ../../../../g3doc/img/image1.jpg \</div><div class="line">    <span class="built_in">export</span>/frozen_inference_graph.pb</div></pre></td></tr></table></figure><p>运行结果：</p><div class="figure"><img src="plot_image1.png"></div></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/deeplab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用 TensorFlow DeepLab 进行语义分割&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab-v3+</title>
    <link href="http://lijiancheng0614.github.io/2018/02/27/2018_02_27_DeepLab-v3+/"/>
    <id>http://lijiancheng0614.github.io/2018/02/27/2018_02_27_DeepLab-v3+/</id>
    <published>2018-02-26T16:00:00.000Z</published>
    <updated>2018-03-18T16:39:07.972Z</updated>
    
    <content type="html"><![CDATA[<p>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</p><p>Paper: <a href="https://arxiv.org/abs/1802.02611" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1802.02611</a></p><p>Blog: <a href="https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html" class="uri" target="_blank" rel="external">https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html</a></p><p>Code: <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p><a id="more"></a><p>在 DeepLab-v3 上添加 decoder 细化分割结果（尤其是物体边界），且使用 depthwise separable convolution 加速。</p><p>DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries.</p><p>We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network.</p><h2 id="introduction">Introduction</h2><p>考虑语义分割的两种类型：</p><ol style="list-style-type: decimal"><li><p>空间金字塔池化模块：通过池化不同分辨率的特征，捕获丰富的上下文信息</p></li><li><p>编码器解码器结构：能够获得锐利的物体边界</p></li></ol><p>we consider two types of neural networks for semantic segmentation:</p><ol style="list-style-type: decimal"><li><p>spatial pyramid pooling module: captures rich contextual information by pooling features at different resolution</p></li><li><p>encoder-decoder structure: is able to obtain sharp object boundaries.</p></li></ol><p>空间金字塔池化模块计算密集，因为提取输出特征只是输入大小的 8 倍甚至 4 倍。</p><p>it is computationally prohibitive to extract output feature maps that are 8, or even 4 times smaller than the input resolution.</p><p>编码器解码器结构在编码器路径中计算更快（因为没有扩散特征）。</p><p>encoder-decoder models lend themselves to faster computation (since no features are dilated) in the encoder path and gradually recover sharp object boundaries in the decoder path.</p><p>结合二者优点，提出加入多尺度上下文信息来丰富编码器解码器网络中的编码器模块。</p><p>Attempting to combine the advantages from both methods, we propose to enrich the encoder module in the encoder-decoder networks by incorporating the multi-scale contextual information.</p><p>采用 Xception 模型在速度和精度上都有所提升</p><p>show improvement in terms of both speed and accuracy by adapting the Xception model</p><p>contributions:</p><ol style="list-style-type: decimal"><li><p>propose a novel encoder-decoder structure which employs DeepLabv3 as a powerful encoder module.</p></li><li><p>can arbitrarily control the resolution of extracted encoder features by atrous convolution to trade-off precision and runtime, which is not possible with existing encoder-decoder models.</p></li><li><p>adapt the Xception model for the segmentation task and apply depthwise separable convolution to both<br>ASPP module and decoder module, resulting in a faster and stronger encoder-decoder network.</p></li><li><p>attains a new state-of-art performance on PASCAL VOC 2012 dataset.</p></li><li><p>make our Tensorflow-based implementation of the proposed model publicly available.</p></li></ol><h2 id="related-work">Related Work</h2><p>Spatial pyramid pooling</p><p>Encoder-decoder</p><p>Depthwise separable convolution</p><h2 id="methods">Methods</h2><h3 id="encoder-decoder-with-atrous-convolution">Encoder-Decoder with Atrous Convolution</h3><ul><li><p>Atrous convolution</p></li><li><p>Depthwise separable convolution</p></li><li><p>DeepLabv3 as encoder</p></li><li><p>Proposed decoder</p><p>In the work of DeepLabv3, the features are bilinearly upsampled by a factor of 16, which could be considered a naive decoder module.</p><p>However, this naive decoder module may not successfully recover object segmentation details.</p><p>We thus propose a simple yet effective decoder module, as illustrated in Fig. 2.</p><ol style="list-style-type: decimal"><li><p>The encoder features are first bilinearly upsampled by a factor of 4</p></li><li><p>then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution</p><p>apply another 1 × 1 convolution on the low-level features to reduce the number of channels</p></li><li><p>apply a few 3 × 3 convolutions to refine the features</p></li><li><p>another bilinear upsampling by a factor of 4</p></li></ol><p>解码器结构：</p><ol style="list-style-type: decimal"><li><p>编码器特征双线性插值上采样 4 倍</p></li><li><p>然后与具有相同空间分辨率的相应低级特征合并</p><p>作用一个 1 x 1 卷积在低级特征上以减少通道数</p></li><li><p>经过一些 3 x 3 的卷积以精炼特征</p></li><li><p>再双线性插值上采样 4 倍</p></li></ol></li></ul><div class="figure"><img src="http://liangchiehchen.com/fig/deeplabv3plus.png"></div><p>Figure 2. Our proposed DeepLabv3+ extends DeepLabv3 by employing a encoder-decoder structure. The encoder module encodes multiscale contextual information by applying atrous convolution at multiple scales, while the simple yet effective decoder module refines the segmentation results along object boundaries.</p><h3 id="modified-aligned-xception">Modified Aligned Xception</h3><p>The Xception model has shown promising image classification results on ImageNet with fast computation.</p><p>More recently, the MSRA team modifies the Xception model (called Aligned Xception) and further pushes the performance in the task of object detection.</p><div class="figure"><img src="figure3.png"></div><p>Figure 3. The Xception model is modified as follows: (1) more layers (same as MSRA’s modification except the changes in Entry flow), (2) all the max pooling operations are replaced by depthwise separable convolutions with striding, and (3) extra batch normalization and ReLU are added after each 3 × 3 depthwise convolution, similar to MobileNet.</p><p>a few more changes:</p><p>一些改进：</p><ol style="list-style-type: decimal"><li><p>不修改入口流的网络结构，为了快速计算和存储效率</p></li><li><p>替代最大池化操作为深度可分离卷积，这使我们能够应用多孔分离卷积在任意分辨率提取特征（另一种选择是延长 arous 算法到最大池化操作）</p></li><li><p>在 3 x 3 的深度可分离卷积后添加额外的 BN 和 ReLU 激活</p></li><li><p>do not modify the entry flow network structure for fast computation and memory efficiency</p></li><li><p>all max pooling operations are replaced by depthwise separable convolution with striding, which enables us to apply atrous separable convolution to extract feature maps at an arbitrary resolution (another option is to extend the atrous algorithm to max pooling operations)</p></li><li><p>extra batch normalization and ReLU activation are added after each 3 × 3 depthwise convolution</p></li></ol><h2 id="experimental-evaluation">Experimental Evaluation</h2><h3 id="decoder-design-choices">Decoder Design Choices</h3><p>考虑三个地方进行不同的设计：</p><ol style="list-style-type: decimal"><li><p>用来减少编码器模块的底层特征图的通道的 1 × 1 卷积</p></li><li><p>得到清晰的分割结果的 3 × 3 卷积</p></li><li><p>使用哪些编码器低级特征</p></li></ol><p>we consider three places for different design choices:</p><ol style="list-style-type: decimal"><li><p>the 1 × 1 convolution used to reduce the channels of the low-level feature map from the encoder module</p><p>实验表明，48 个 channel 的 1 x 1 卷积效果最好</p></li><li><p>the 3 × 3 convolution used to obtain sharper segmentation results</p><p>实验表明，2 个 256 channel 的 3 x 3 卷积效果最好</p></li><li><p>what encoder low-level features should be used</p><p>实验表明，只使用 Conv2 的特征效果最好</p></li></ol><h3 id="resnet-101-as-network-backbone">ResNet-101 as Network Backbone</h3><ul><li><p>Baseline</p></li><li><p>Adding decoder</p></li><li><p>Coarser feature maps</p></li></ul><div class="figure"><img src="table3.png"></div><p>Table 3. Inference strategy on the PASCAL VOC 2012 val set when using ResNet-101 as feature extractor. train OS: The output stride used during training. eval OS: The output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multi-scale inputs during evaluation. Flip: Adding left-right flipped inputs.</p><h3 id="xception-as-network-backbone">Xception as Network Backbone</h3><ul><li><p>ImageNet pretraining</p></li><li><p>Baseline</p></li><li><p>Adding decoder</p></li><li><p>Using depthwise separable convolution</p></li><li><p>Pretraining on COCO</p></li><li><p>Pretraining on JFT</p></li><li><p>Test set results</p></li><li><p>Qualitative results</p></li><li><p>Failure mode</p></li></ul><div class="figure"><img src="table5.png"></div><p>Table 5. Inference strategy on the PASCAL VOC 2012 val set when using modified Xception as feature extractor. train OS: The output stride used during training. eval OS: The output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multi-scale inputs during evaluation. Flip: Adding left-right flipped inputs. SC: Adopting depthwise separable convolution for both ASPP and decoder modules. COCO: Models pretrained on MS-COCO dataset. JFT: Models pretrained on JFT dataset.</p><h3 id="improvement-along-object-boundaries">Improvement along Object Boundaries</h3><p>employing the proposed decoder for both ResNet-101 and Xception network backbones improves the performance compared to the naive bilinear upsampling.</p><h2 id="conclusion">Conclusion</h2><p>Our proposed model “DeepLabv3+” employs the encoderdecoder structure where DeepLabv3 is used to encode the rich contextual information and a simple yet effective decoder module is adopted to recover the object boundaries. One could also apply the atrous convolution to extract the encoder features at an arbitrary resolution, depending on the available computation resources. We also explore the Xception model and atrous separable convolution to make the proposed model faster and stronger. Finally, our experimental results show that the proposed model sets a new state-of-the-art performance on the PASCAL VOC 2012 semantic image segmentation benchmark.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1802.02611&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Blog: &lt;a href=&quot;https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/deeplab&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://lijiancheng0614.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>代码评审 Code Review</title>
    <link href="http://lijiancheng0614.github.io/2018/01/12/2018_01_12_code_review/"/>
    <id>http://lijiancheng0614.github.io/2018/01/12/2018_01_12_code_review/</id>
    <published>2018-01-11T16:00:00.000Z</published>
    <updated>2018-03-07T16:30:12.350Z</updated>
    
    <content type="html"><![CDATA[<p>介绍几个轻量级代码检查工具，包括代码静态检查，整理代码等。</p><a id="more"></a><p>无论是自己一个人写代码，还是与其他人合作写代码，都希望能有一份高质量的代码，以便别人或未来的自己可读、可维护和可扩展。</p><p>于是往往我们需要代码评审（Code Review）。正式的代码评审已经有不少书籍介绍且与开发环境有关，这里只介绍几个轻量级代码检查工具，方便日常开发过程中提高自己的代码质量。</p><h2 id="python">Python</h2><h3 id="pylint">Pylint</h3><p><a href="https://www.pylint.org/" class="uri" target="_blank" rel="external">https://www.pylint.org/</a></p><p>Pylint 是一个 Python 源代码分析工具，可以分析代码错误，查找不符合代码风格标准（Pylint 默认使用的代码风格是 <a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="external">PEP 8</a>）和有潜在问题的代码。</p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install pylint</div></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pylint 1.py</div></pre></td></tr></table></figure><p>安装直接用 pip 就可以了，使用则是输入代码文件路径就可以了，它会输出存在不同级别问题的代码的所在位置，还能评分。更多的用法可以参考官方网站。</p><h3 id="yapf">yapf</h3><p><a href="https://github.com/google/yapf" class="uri" target="_blank" rel="external">https://github.com/google/yapf</a></p><p>yapf 是一个 Google 开源的 Python 代码格式化工具。可以格式化代码，统一缩进、换行、符号等格式。</p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install yapf</div></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yapf 1.py &gt; out.py</div></pre></td></tr></table></figure><p>安装直接用 pip 就可以了，使用则是输入代码文件路径就可以了，它会输出美化后的代码，因此可以重定向至一个文件。更多的用法可以参考官方网站。</p><h2 id="c">C++</h2><h3 id="cpplint">cpplint</h3><p><a href="https://github.com/cpplint/cpplint" class="uri" target="_blank" rel="external">https://github.com/cpplint/cpplint</a></p><p>cpplint 是一个遵循 Google C++ 风格指南的 C++ 静态代码检索工具。</p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install cpplint</div></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cpplint 1.cpp</div></pre></td></tr></table></figure><p>安装直接用 pip 就可以了，使用则是输入代码文件路径就可以了，它会输出存在不同级别问题的代码的所在位置。更多的用法可以参考官方网站。</p><h3 id="astyle">AStyle</h3><p><a href="http://astyle.sourceforge.net/" class="uri" target="_blank" rel="external">http://astyle.sourceforge.net/</a></p><p>AStyle 即 Artistic Style，是一个支持 C, C++, C++/CLI，Objective‑C, C# 和 Java 的格式化工具。可以格式化代码，统一缩进、换行、符号等格式。</p><p>注意 AStyle 风格与 Google C++ 风格不一样。</p><p>安装：</p><p>到 <a href="https://sourceforge.net/projects/astyle/" class="uri" target="_blank" rel="external">https://sourceforge.net/projects/astyle/</a> 点“Download”按钮即可根据当前平台下载相应安装包，根据说明安装即可。</p><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">astyle 1.cpp</div></pre></td></tr></table></figure><p>使用则是输入代码文件路径就可以了，它会输出美化后的代码，并把原代码备份至 <code>1.cpp.orig</code>。更多的用法可以参考官方网站。</p><h2 id="sublime-插件">Sublime 插件</h2><p>以上工具均在 Sublime 中能找到插件，只需 Install Package 安装即可。之后可在编码时实现一键格式化或一键检查。</p><p><a href="http://www.sublimelinter.com/en/stable/" target="_blank" rel="external">SublimeLinter</a> 里包含各种语言的 linter。</p><p><a href="https://github.com/jason-kane/PyYapf" target="_blank" rel="external">PyYapf</a> 是 Python yapf 格式化工具。</p><p><a href="https://github.com/timonwong/SublimeAStyleFormatter" target="_blank" rel="external">SublimeAStyleFomatter</a> 是 C++ AStyle 格式化工具。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍几个轻量级代码检查工具，包括代码静态检查，整理代码等。&lt;/p&gt;
    
    </summary>
    
      <category term="Develop" scheme="http://lijiancheng0614.github.io/categories/Develop/"/>
    
    
      <category term="Python" scheme="http://lijiancheng0614.github.io/tags/Python/"/>
    
      <category term="C++" scheme="http://lijiancheng0614.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>句子分类</title>
    <link href="http://lijiancheng0614.github.io/2018/01/08/2018_01_08_sentence_classification/"/>
    <id>http://lijiancheng0614.github.io/2018/01/08/2018_01_08_sentence_classification/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-03-05T16:12:10.894Z</updated>
    
    <content type="html"><![CDATA[<p>学习句子分类，使用深度学习的方法对句子数据集进行分类。</p><a id="more"></a><h2 id="问题">问题</h2><p>句子分类（Sentence Classification）是指给定一个句子，标注预先设定的若干类别中的一个类别。</p><p>句子分类包括情感分析（Sentiment Analysis）、问题分类（Question<br>Classification）等任务。情感分析又称倾向性分析、意见抽取（Opinion extraction）、意见挖掘（Opinion mining）、情感挖掘（Sentiment mining）、主观分析（Subjectivity analysis），它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从评论文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向。</p><h2 id="应用">应用</h2><p>了解对电影、商品、Twitter 等的褒贬评价，以此来改善产品和服务、发现竞争对手的优劣势、预测股票走势等。</p><h2 id="数据集">数据集</h2><table><thead><tr class="header"><th>Data</th><th>c</th><th>l</th><th>N</th><th>|V|</th><th>|V_pre|</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>MR</td><td>2</td><td>20</td><td>10662</td><td>18765</td><td>16448</td><td>CV</td></tr><tr class="even"><td>SST-1</td><td>5</td><td>18</td><td>11855</td><td>17836</td><td>16262</td><td>2210</td></tr><tr class="odd"><td>SST-2</td><td>2</td><td>19</td><td>9613</td><td>16185</td><td>14838</td><td>1821</td></tr><tr class="even"><td>Subj</td><td>2</td><td>23</td><td>10000</td><td>21323</td><td>17913</td><td>CV</td></tr><tr class="odd"><td>TREC</td><td>6</td><td>10</td><td>5952</td><td>9592</td><td>9125</td><td>500</td></tr><tr class="even"><td>CR</td><td>2</td><td>19</td><td>3775</td><td>5340</td><td>5046</td><td>CV</td></tr><tr class="odd"><td>MPQA</td><td>2</td><td>3</td><td>10606</td><td>6246</td><td>6083</td><td>CV</td></tr></tbody></table><ul><li><p>MR: Movie reviews 电影评论，每条评论包含一个句子。<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p></li><li><p>SST-1: Stanford Sentiment Treebank，MR 的扩展但划分了 train/dev/test 集合并提供 5 个细粒度标签（非常积极的，积极的，中性的，负面的，非常消极的）。</p></li><li><p>SST-2: 与 SST-1 一样但移除中性评论并用二进制标签。<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p></li><li><p>Subj: Subjectivity 主观性数据集，任务是将句子分类为主观或客观的。<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p></li><li><p>TREC: TREC question dataset TREC 问题数据集，任务是将一个问题分成 6 类（关于人、位置、数字信息等）。<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p></li><li><p>CR: Customer reviews 各种产品的客户评论，任务是预测正面/负面评论。<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p></li><li><p>MPQA: MPQA 数据集意见极性检测任务。<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p></li></ul><h2 id="方法">方法</h2><p>通常会把任务拆分成几个子任务：</p><ol style="list-style-type: decimal"><li><p>分词</p><p>把句子根据意思分成多个词，有时可能还需要去掉停用词、了解词性、转换成词向量等操作。</p></li><li><p>提取特征</p><p>有时我们不会直接使用分词后的多个词来直接分类，这时需要提取特征来方便分类。</p><p>常用特征：TF-IDF、LDA、LSI</p></li><li><p>构建分类器</p><p>输入特征或词向量等，通过一些模型，对该句子进行分类。</p></li></ol><h3 id="naive-bayes">Naive Bayes</h3><p>NBSVM: Naive Bayes SVM</p><p>MNB: Multinomial Naive Bayes <a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p><p>combine-skip</p><p>combine-skip + NB <a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>NBSVM</td><td>79.4</td><td>-</td><td>-</td><td>93.2</td><td>-</td><td>81.8</td><td>86.3</td></tr><tr class="even"><td>MNB</td><td>79.0</td><td>-</td><td>-</td><td>93.6</td><td>-</td><td>80.0</td><td>86.3</td></tr><tr class="odd"><td>combine-skip</td><td>76.5</td><td>-</td><td>-</td><td>93.6</td><td>92.2</td><td>80.1</td><td>87.1</td></tr><tr class="even"><td>combine-skip+NB</td><td>80.4</td><td>-</td><td>-</td><td>93.6</td><td>-</td><td>81.3</td><td>87.5</td></tr></tbody></table><h3 id="rnn">RNN</h3><p>RCNN: Recurrent Convolutional Neural Networks <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p><p>S-LSTM: Long Short-Term Memory Over Recursive Structures <a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p><p>LSTM: Long Short-Term Memory</p><p>BLSTM: Bidirectional Long Short-Term Memory</p><p>Tree-LSTM: Tree-structured Long Short-Term Memory <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p><p>LSTMN: Long Short-Term Memory-Network <a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p><p>Multi-Task: Recurrent Neural Network for Text Classification with Multi-Task Learning <a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p><p>BLSTM-Att: Bidirectional Long Short-Term Memory, attention-based model</p><p>BLSTM-2DPooling: Bidirectional Long Short-Term Memory Networks with Two-Dimensional Max Pooling</p><p>BLSTM-2DCNN: Bidirectional Long Short-Term Memory Networks with 2D convolution <a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>RCNN</td><td>-</td><td>47.21</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>S-LSTM</td><td>-</td><td>-</td><td>81.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="odd"><td>LSTM</td><td>-</td><td>46.4</td><td>84.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>BLSTM</td><td>-</td><td>49.1</td><td>87.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="odd"><td>Tree-LSTM</td><td>-</td><td>51.0</td><td>88.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>LSTMN</td><td>-</td><td>49.3</td><td>87.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="odd"><td>Multi-Task</td><td>-</td><td>49.6</td><td>87.9</td><td>94.1</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>BLSTM</td><td>80.0</td><td>49.1</td><td>87.6</td><td>92.1</td><td>93.0</td><td>-</td><td>-</td></tr><tr class="odd"><td>BLSTM-Att</td><td>81.0</td><td>49.8</td><td>88.2</td><td>93.5</td><td>93.8</td><td>-</td><td>-</td></tr><tr class="even"><td>BLSTM-2DPooling</td><td>81.5</td><td>50.5</td><td>88.3</td><td>93.7</td><td>94.8</td><td>-</td><td>-</td></tr><tr class="odd"><td>BLSTM-2DCNN</td><td>82.3</td><td>52.4</td><td>89.5</td><td>94.0</td><td>96.1</td><td>-</td><td>-</td></tr></tbody></table><h3 id="cnn">CNN</h3><p>DCNN: Dynamic Convolutional Neural Network <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p><p>CNN-non-static: Convolutional Neural Networks, the pretrained vectors are fine-tuned for each task</p><p>CNN-multichannel: Convolutional Neural Networks with two sets of word vectors <a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p><p>TBCNN: Tree-based Convolutional Neural Network <a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p><p>Molding-CNN: Molding Convolutional Neural Networks <a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a></p><p>CNN-Ana: Non-static GloVe+word2vec CNN <a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a></p><p>MVCNN: Multichannel Variable-Size Convolution <a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a></p><p>DSCNN: Dependency Sensitive Convolutional Neural Networks <a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>DCNN</td><td>-</td><td>48.5</td><td>86.8</td><td>-</td><td>93.0</td><td>-</td><td>-</td></tr><tr class="even"><td>CNN-non-static</td><td>81.5</td><td>48.0</td><td>87.2</td><td>93.4</td><td>93.6</td><td>84.3</td><td>89.5</td></tr><tr class="odd"><td>CNN-multichannel</td><td>81.1</td><td>47.4</td><td>88.1</td><td>93.2</td><td>92.2</td><td>85.0</td><td>89.4</td></tr><tr class="even"><td>TBCNN</td><td>-</td><td>51.4</td><td>87.9</td><td>-</td><td>96.0</td><td>-</td><td>-</td></tr><tr class="odd"><td>Molding-CNN</td><td>-</td><td>51.2</td><td>88.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>CNN-Ana</td><td>81.02</td><td>45.98</td><td>85.45</td><td>93.66</td><td>91.37</td><td>84.65</td><td>89.55</td></tr><tr class="odd"><td>MVCNN</td><td>-</td><td>49.6</td><td>89.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>DSCNN</td><td>81.5</td><td>49.7</td><td>89.1</td><td>93.2</td><td>95.4</td><td>-</td><td>-</td></tr></tbody></table><h3 id="others">Others</h3><p>RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia <a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></p><p>AdaSent: self-adaptive hierarchical sentence model <a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a></p><p>RNTN: Recursive Neural Tensor Network <a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a></p><p>DRNN: Deep Recursive Neural Networks <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a></p><table><thead><tr class="header"><th>Model</th><th>MR</th><th>SST-1</th><th>SST-2</th><th>Subj</th><th>TREC</th><th>CR</th><th>MPQA</th></tr></thead><tbody><tr class="odd"><td>RAE</td><td>77.7</td><td>43.2</td><td>82.4</td><td>-</td><td>-</td><td>-</td><td>86.4</td></tr><tr class="even"><td>AdaSent</td><td>83.1</td><td>-</td><td>-</td><td>95.5</td><td>92.4</td><td>86.3</td><td>93.3</td></tr><tr class="odd"><td>RNTN</td><td>-</td><td>45.7</td><td>85.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr class="even"><td>DRNN</td><td>-</td><td>49.8</td><td>86.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><h2 id="参考">参考</h2><div class="footnotes"><hr><ol><li id="fn1"><p>(ACL 2005) Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" class="uri" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><a href="#fnref1">↩</a></p></li><li id="fn2"><p>(EMNLP 2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank <a href="https://nlp.stanford.edu/sentiment/" class="uri" target="_blank" rel="external">https://nlp.stanford.edu/sentiment/</a><a href="#fnref2">↩</a></p></li><li id="fn3"><p>(ACL 2004) A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data" class="uri" target="_blank" rel="external">http://www.cs.cornell.edu/people/pabo/movie-review-data</a><a href="#fnref3">↩</a></p></li><li id="fn4"><p>(ACL 2002) Learning Question Classifiers <a href="http://cogcomp.org/Data/QA/QC/" class="uri" target="_blank" rel="external">http://cogcomp.org/Data/QA/QC/</a><a href="#fnref4">↩</a></p></li><li id="fn5"><p>(SIGKDD 2004) Mining and Summarizing Customer Reviews <a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html" class="uri" target="_blank" rel="external">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a><a href="#fnref5">↩</a></p></li><li id="fn6"><p>(Language Resources and Evaluation 2005) Annotating Expressions Of Opinions And Emotions In Language <a href="http://mpqa.cs.pitt.edu/" class="uri" target="_blank" rel="external">http://mpqa.cs.pitt.edu/</a><a href="#fnref6">↩</a></p></li><li id="fn7"><p>(ACL 2012) Baselines and Bigrams: Simple, Good Sentiment and Topic Classification<a href="#fnref7">↩</a></p></li><li id="fn8"><p>(NIPS 2015) Skip-Thought Vectors<a href="#fnref8">↩</a></p></li><li id="fn9"><p>(AAAI 2015) Recurrent Convolutional Neural Networks for Text Classification<a href="#fnref9">↩</a></p></li><li id="fn10"><p>(ICML 2015) Long Short-Term Memory Over Recursive Structures<a href="#fnref10">↩</a></p></li><li id="fn11"><p>(ACL 2015) Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks<a href="#fnref11">↩</a></p></li><li id="fn12"><p>(EMNLP2016) Long Short-Term Memory-Networks for Machine Reading<a href="#fnref12">↩</a></p></li><li id="fn13"><p>(IJCAI 2016) Recurrent Neural Network for Text Classification with Multi-Task Learning<a href="#fnref13">↩</a></p></li><li id="fn14"><p>(COLING 2016) Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling<a href="#fnref14">↩</a></p></li><li id="fn15"><p>(ACL 2014) A Convolutional Neural Network for Modelling Sentences<a href="#fnref15">↩</a></p></li><li id="fn16"><p>(EMNLP 2014) Convolutional Neural Networks for Sentence Classification<a href="#fnref16">↩</a></p></li><li id="fn17"><p>(EMNLP 2015) Discriminative Neural Sentence Modeling by Tree-Based Convolution<a href="#fnref17">↩</a></p></li><li id="fn18"><p>(EMNLP 2015) Molding CNNs for text: non-linear, non-consecutive convolutions<a href="#fnref18">↩</a></p></li><li id="fn19"><p>(IJCNLP 2017) A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification<a href="#fnref19">↩</a></p></li><li id="fn20"><p>(CoNLL 2015) Multichannel Variable-Size Convolution for Sentence Classification<a href="#fnref20">↩</a></p></li><li id="fn21"><p>(NAACL 2016) Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents<a href="#fnref21">↩</a></p></li><li id="fn22"><p>(EMNLP 2011) Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions<a href="#fnref22">↩</a></p></li><li id="fn23"><p>(IJCAI 2015) Self-adaptive hierarchical sentence model<a href="#fnref23">↩</a></p></li><li id="fn24"><p>(EMNLP 2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank<a href="#fnref24">↩</a></p></li><li id="fn25"><p>(NIPS 2014) Deep Recursive Neural Networks for Compositionality in Language<a href="#fnref25">↩</a></p></li></ol></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习句子分类，使用深度学习的方法对句子数据集进行分类。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>神经网络权值量化</title>
    <link href="http://lijiancheng0614.github.io/2017/12/10/2017_12_10_network_weights_quantization/"/>
    <id>http://lijiancheng0614.github.io/2017/12/10/2017_12_10_network_weights_quantization/</id>
    <published>2017-12-09T16:00:00.000Z</published>
    <updated>2018-03-01T16:22:11.971Z</updated>
    
    <content type="html"><![CDATA[<p>对神经网络的权值进行量化，使模型大小变小，运行速度变快，且准确率与原来相近。</p><a id="more"></a><p>参考 <a href="https://www.tensorflow.org/performance/quantization" class="uri" target="_blank" rel="external">https://www.tensorflow.org/performance/quantization</a></p><h2 id="什么是量化">什么是量化</h2><p>把网络权值从高精度转化成低精度（32位浮点数 float32 转化成 8位定点数 int8 或二值化为 1 bit），但模型准确率等指标与原来相近，模型大小变小，运行速度加快。</p><h2 id="为什么量化">为什么量化</h2><p>量化可以看作是噪声的一种来源，所以量化后的模型效果与原来相近。</p><ul><li><p>优点</p><ol style="list-style-type: decimal"><li><p>模型变小，运行速度变快。</p></li><li><p>int8 只需 float32 内存带宽的25％，可以更好使用缓存并避免 RAM 访问出现瓶颈。</p></li><li><p>每个时钟周期执行更多的 SIMD 操作。</p></li><li><p>如有加速8位计算的 DSP 芯片则更快。</p></li></ol></li><li><p>缺点</p><p>效果稍差。</p></li></ul><h2 id="如何量化">如何量化</h2><p>先训练模型，再进行量化，测试时使用量化后的模型。</p><ul><li><p>训练</p><p>一般使用 float32 来训练模型效果较好（特别是反向传播和梯度需要浮点来表示）</p></li><li><p>量化</p><ol style="list-style-type: decimal"><li><p>加入量化和反量化操作（如一种量化操作为根据该层权值的最大值和最小值映射到 8位区间）</p><p>如下图 1 变成图 2</p><div class="figure"><img src="https://www.tensorflow.org/images/quantization0.png"></div><div class="figure"><img src="https://www.tensorflow.org/images/quantization1.png"></div></li><li><p>把相应的运算转化为量化的运算（实现 8位版本的卷积、矩阵乘法等）</p></li><li><p>删除相邻的 反量化-量化 操作</p><p>如下图</p><div class="figure"><img src="https://www.tensorflow.org/images/quantization2.png"></div></li></ol></li><li><p>测试</p><p>使用量化后的模型来预测</p></li></ul><h2 id="参考">参考</h2><ol style="list-style-type: decimal"><li><p>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1606.06160</a> <a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net" class="uri" target="_blank" rel="external">https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net</a></p></li><li><p>(ECCV 2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks <a href="https://arxiv.org/abs/1603.05279" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1603.05279</a> <a href="https://github.com/allenai/XNOR-Net" class="uri" target="_blank" rel="external">https://github.com/allenai/XNOR-Net</a></p></li><li><p>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1 <a href="https://arxiv.org/abs/1602.02830" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.02830</a> <a href="https://github.com/MatthieuCourbariaux/BinaryNet" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryNet</a></p></li><li><p>BinaryConnect: Training Deep Neural Networks with binary weights during propagations <a href="https://arxiv.org/abs/1511.00363" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1511.00363</a> <a href="https://github.com/MatthieuCourbariaux/BinaryConnect" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryConnect</a></p></li><li><p>(CVPR 2016) Quantized Convolutional Neural Networks for Mobile Devices <a href="https://github.com/jiaxiang-wu/quantized-cnn" class="uri" target="_blank" rel="external">https://github.com/jiaxiang-wu/quantized-cnn</a></p></li><li><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对神经网络的权值进行量化，使模型大小变小，运行速度变快，且准确率与原来相近。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>神经网络剪枝</title>
    <link href="http://lijiancheng0614.github.io/2017/11/12/2017_11_12_network_pruning/"/>
    <id>http://lijiancheng0614.github.io/2017/11/12/2017_11_12_network_pruning/</id>
    <published>2017-11-11T16:00:00.000Z</published>
    <updated>2018-03-13T15:31:14.301Z</updated>
    
    <content type="html"><![CDATA[<p>对神经网络（主要是CNN）进行剪枝，使模型运行速度变快，大小变小，且准确率与原来相近。</p><a id="more"></a><h2 id="如何剪枝">如何剪枝</h2><h3 id="移除滤波器">移除滤波器</h3><p>参考论文 <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>，对所有滤波器（filters）计算L1范数，移除值较小的滤波器。</p><ul><li><p>优点</p><p>模型变小，运行速度变快。</p></li><li><p>缺点</p><p>依然保留部分冗余的连接。</p></li></ul><ol style="list-style-type: decimal"><li><p>普通卷积</p><div class="figure"><img src="pruning_filters.png"></div><p>输入的特征图：<span class="math inline">\(x_i \in \mathbb{R}^{n_i \times h_i \times w_i}\)</span></p><p>输出的特征图：<span class="math inline">\(x_{i + 1} \in \mathbb{R}^{n_{i + 1} \times h_{i + 1} \times w_{i + 1}}\)</span></p><p>不考虑 bias，参数维度：<span class="math inline">\(n_i \times n_{i + 1} \times k_h \times k_w\)</span>，即有 <span class="math inline">\(n_{i + 1}\)</span> 个 3D 滤波器 <span class="math inline">\(\mathbb{F}_{i, j} \in \mathbb{R}^{n_i \times k_h \times k_w}\)</span></p><p>计算每个滤波器的 L1 值，取最小的若干个移除：<span class="math inline">\(n_{i + 1}\)</span> -&gt; <span class="math inline">\(n&#39;_{i + 1}\)</span></p><p>这会影响后续层（卷积 / 全连接 / Batch Normalization 等）的输入：</p><p>如后续卷积层的参数维度为 <span class="math inline">\(n&#39;_{i + 1} \times n_{i + 2} \times k_h \times k_w\)</span></p></li><li><p>Depthwise 卷积</p><p>Depthwise 卷积参数维度为 <span class="math inline">\(1 \times n_i \times k_h \times k_w\)</span></p><p>后续的 Pointwise 卷积参数维度为 <span class="math inline">\(n_i \times n_{i + 1} \times 1 \times 1\)</span></p><p>应与后续的 Pointwise 卷积一起计算 L1：即使用 <span class="math inline">\(dw[0, :, :, :] \cdot pw[:, i, :, :]\)</span></p></li></ol><h3 id="移除连接">移除连接</h3><p>参考论文 <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>，移除权值小于一定阈值的连接。</p><ul><li><p>优点</p><p>模型变小，运行速度变快。</p><p>能尽可能去掉冗余的连接。</p></li><li><p>缺点</p><p>需要设计更稀疏的格式保存模型，否则模型不变小也不加速。</p></li></ul><h3 id="合并-batch-normalization">合并 Batch Normalization</h3><p>Batch Normalization 的参数可以合并到上一个卷积/全连接的参数中</p><p>如设卷积的参数为 <span class="math inline">\(W\)</span>, <span class="math inline">\(b\)</span>，则卷积可表示为 <span class="math inline">\(y = Wx + b\)</span></p><p>Batch Normalization 的参数为 scale, bias, mean, variance</p><p>Batch Normalization 可表示为 <span class="math inline">\(y = \frac{scale}{\sqrt{variance + \varepsilon}} \cdot x + \left( bias - \frac{scale \cdot mean}{\sqrt{variance + \varepsilon}} \right)\)</span></p><p>Batch Normalization 的参数合并后卷积的参数为</p><p><span class="math inline">\(W&#39; = W \cdot \frac{scale}{\sqrt{variance + \varepsilon}}\)</span></p><p><span class="math inline">\(b&#39; = (b - mean) \cdot \frac{scale}{\sqrt{variance + \varepsilon}} + bias\)</span></p><h2 id="剪枝策略">剪枝策略</h2><ol style="list-style-type: decimal"><li><p>逐层剪枝比一次性剪枝效果好</p></li><li><p>每层剪枝比例应根据敏感度分析去删减</p></li><li><p>移除滤波器时，计算L1移除值较小的比随机移除、其它计算方法效果好</p></li><li><p>剪枝后进行 finetune 比 train from scratch 效果好</p></li><li><p>剪枝后固定较为敏感的层的权值再训练的效果比较好</p></li></ol><h2 id="参考">参考</h2><div class="footnotes"><hr><ol><li id="fn1"><p>(ICLR 2017) Pruning Filters for Efficient Convnets <a href="https://arxiv.org/abs/1608.08710" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1608.08710</a><a href="#fnref1">↩</a></p></li><li id="fn2"><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a><a href="#fnref2">↩</a></p></li><li id="fn3"><p>(NIPS 2015) Learning both Weights and Connections for Efficient Neural Networks <a href="https://arxiv.org/abs/1506.02626" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1506.02626</a><a href="#fnref3">↩</a></p></li></ol></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对神经网络（主要是CNN）进行剪枝，使模型运行速度变快，大小变小，且准确率与原来相近。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>神经网络模型压缩与加速</title>
    <link href="http://lijiancheng0614.github.io/2017/10/01/2017_10_01_network_compression_and_acceleration/"/>
    <id>http://lijiancheng0614.github.io/2017/10/01/2017_10_01_network_compression_and_acceleration/</id>
    <published>2017-09-30T16:00:00.000Z</published>
    <updated>2018-03-01T16:39:33.488Z</updated>
    
    <content type="html"><![CDATA[<p>介绍神经网络（主要是CNN）模型压缩与加速的常见方法</p><a id="more"></a><p>目标：模型运行速度尽可能快，大小尽可能小，准确率尽可能保持不变</p><h2 id="模型压缩">模型压缩</h2><h3 id="改变网络结构">改变网络结构</h3><h4 id="使用特定结构">使用特定结构</h4><p>如 ShuffleNet, MobileNet, Xception, SqueezeNet</p><ul><li><p>MobileNet</p><p>把普通卷积操作分成两部分</p><ul><li><p>Depthwise Convolution</p><p>计算量 <span class="math inline">\(D_K \cdot D_K \cdot M \cdot D_F \cdot D_F\)</span></p></li><li><p>Pointwise Convolution</p><p>计算量 <span class="math inline">\(M \cdot N \cdot D_F \cdot D_F\)</span></p></li></ul><p>上面两步合称Depthwise Separable Convolution</p><p>与原卷积计算量之比 <span class="math inline">\(\frac{D_K \cdot D_K \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F}{D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F} = \frac{1}{N} + \frac{1}{D_K^2}\)</span></p><div class="figure"><img src="MobileNet.png"></div></li></ul><p>参考：</p><ol style="list-style-type: decimal"><li><p>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices <a href="https://arxiv.org/abs/1707.01083" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1707.01083</a></p></li><li><p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications <a href="https://arxiv.org/abs/1704.04861" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a> <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md</a></p></li><li><p>Xception: Deep Learning with Depthwise Separable Convolutions <a href="https://arxiv.org/abs/1610.02357" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1610.02357</a></p></li><li><p>(ICLR 2017) Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters and &lt;0.5MB Model Size <a href="https://arxiv.org/abs/1602.07360" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.07360</a> <a href="https://github.com/DeepScale/SqueezeNet" class="uri" target="_blank" rel="external">https://github.com/DeepScale/SqueezeNet</a></p></li></ol><h4 id="剪枝">剪枝</h4><p>裁剪连接、滤波器</p><p>权值稀疏化</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(ICLR 2017) Pruning Filters for Efficient Convnets <a href="https://arxiv.org/abs/1608.08710" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1608.08710</a></p></li><li><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a></p></li><li><p>(NIPS 2015) Learning both Weights and Connections for Efficient Neural Networks <a href="https://arxiv.org/abs/1506.02626" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1506.02626</a></p></li></ol><h4 id="蒸馏">蒸馏</h4><p>用一个性能好的大网络来教小网络学习，使小网络具备跟大网络一样的性能，但参数规模小</p><p>训练小模型 (distilled model) 的目标函数由两部分组成：</p><ol style="list-style-type: decimal"><li>与大模型的softmax输出的交叉熵，称为软目标</li><li>与groundtruth的交叉熵</li></ol><p>训练的损失为上述两项损失的加权和</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(ICLR 2017) Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer <a href="https://arxiv.org/abs/1612.03928" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1612.03928</a></p></li><li><p>(NIPSW 2014) Distilling the Knowledge in a Neural Network <a href="https://arxiv.org/abs/1503.02531" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1503.02531</a></p></li></ol><h3 id="只改变权值">只改变权值</h3><h4 id="量化">量化</h4><p>把网络权值从高精度转化成低精度（32位浮点数 float32 转化成 8位定点数 int8 或二值化为 1 bit），但模型准确率等指标与原来相近，模型大小变小，运行速度加快。</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients <a href="https://arxiv.org/abs/1606.06160" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1606.06160</a> <a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net" class="uri" target="_blank" rel="external">https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net</a></p></li><li><p>(ECCV 2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks <a href="https://arxiv.org/abs/1603.05279" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1603.05279</a> <a href="https://github.com/allenai/XNOR-Net" class="uri" target="_blank" rel="external">https://github.com/allenai/XNOR-Net</a></p></li><li><p>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1 <a href="https://arxiv.org/abs/1602.02830" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.02830</a> <a href="https://github.com/MatthieuCourbariaux/BinaryNet" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryNet</a></p></li><li><p>BinaryConnect: Training Deep Neural Networks with binary weights during propagations <a href="https://arxiv.org/abs/1511.00363" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1511.00363</a> <a href="https://github.com/MatthieuCourbariaux/BinaryConnect" class="uri" target="_blank" rel="external">https://github.com/MatthieuCourbariaux/BinaryConnect</a></p></li><li><p>(CVPR 2016) Quantized Convolutional Neural Networks for Mobile Devices <a href="https://github.com/jiaxiang-wu/quantized-cnn" class="uri" target="_blank" rel="external">https://github.com/jiaxiang-wu/quantized-cnn</a></p></li><li><p>(ICLR 2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <a href="https://arxiv.org/abs/1510.00149" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a> <a href="https://github.com/songhan/Deep-Compression-AlexNet" class="uri" target="_blank" rel="external">https://github.com/songhan/Deep-Compression-AlexNet</a></p></li></ol><h4 id="矩阵分解">矩阵分解</h4><p>低秩分解（SVD分解、Tucker分解、Block Term分解）</p><p>原理：权值向量主要分布在一些低秩子空间，用少数基来重构权值矩阵</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(ICCV 2017) Coordinating Filters for Faster Deep Neural Networks <a href="https://arxiv.org/abs/1703.09746" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1703.09746</a> <a href="https://github.com/wenwei202/caffe" class="uri" target="_blank" rel="external">https://github.com/wenwei202/caffe</a></p></li><li><p>(TPAMI 2015) Accelerating Very Deep Convolutional Networks for Classification and Detection <a href="https://arxiv.org/abs/1505.06798" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1505.06798</a></p></li><li><p>(NIPS 2014) Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation <a href="https://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation" class="uri" target="_blank" rel="external">https://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation</a></p></li></ol><h2 id="平台加速">平台加速</h2><h3 id="软件加速">软件加速</h3><h4 id="卷积计算加速">卷积计算加速</h4><ul><li><p>im2col + GEMM：将问题转化为矩阵乘法后使用矩阵运算库</p><p>参考：</p><ol style="list-style-type: decimal"><li>(ICML 2017) MEC: Memory-efficient Convolution for Deep Neural Network <a href="https://arxiv.org/abs/1706.06873" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1706.06873</a></li></ol></li><li><p>FFT变换：时域卷积等于频域相乘，将问题转化为简单的乘法问题</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(BMVC 2015) Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add <a href="https://arxiv.org/abs/1601.06815" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1601.06815</a></p></li><li><p>(ICLR 2015) Fast Convolutional Nets With fbfft: A GPU Performance Evaluation <a href="https://arxiv.org/abs/1412.7580" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1412.7580</a></p></li><li><p>Fast Training of Convolutional Networks through FFTs <a href="https://arxiv.org/abs/1312.5851" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1312.5851</a></p></li></ol></li><li><p>Winograd</p><p>参考：</p><ol style="list-style-type: decimal"><li><p>(CODES 2016) Zero and data reuse-aware fast convolution for deep neural networks on gpu</p></li><li><p>(CVPR 2016) Fast Algorithms for Convolutional Neural Networks <a href="https://arxiv.org/abs/1509.09308" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1509.09308</a></p></li></ol></li></ul><h4 id="不同框架速度不同">不同框架速度不同</h4><ol style="list-style-type: decimal"><li>TensorFlow Mobile, TensorFlow Lite <a href="https://www.tensorflow.org/mobile/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/mobile/</a></li><li>Caffe2 <a href="https://caffe2.ai/" class="uri" target="_blank" rel="external">https://caffe2.ai/</a></li><li>腾讯ncnn（不依赖 BLAS/NNPACK 等计算框架，NEON优化，多核并行） <a href="https://github.com/Tencent/ncnn/" class="uri" target="_blank" rel="external">https://github.com/Tencent/ncnn/</a></li><li>百度mdl（无任何第三方依赖，汇编优化，NEON优化） <a href="https://github.com/baidu/mobile-deep-learning" class="uri" target="_blank" rel="external">https://github.com/baidu/mobile-deep-learning</a></li></ol><h3 id="硬件加速">硬件加速</h3><ol style="list-style-type: decimal"><li><p>多GPU并行</p></li><li><p>多核并行</p></li><li><p>使用硬件提供最优的指令</p><p>如支持arm64则编译64位的库而非32位的库</p><p>如使用支持相应 SIMD (Single Instruction, Multiple Data) 的库</p></li></ol><p>参考：</p><ol style="list-style-type: decimal"><li>(ISCA 2016) EIE: Efficient Inference Engine on Compressed Deep Neural Network <a href="https://arxiv.org/abs/1602.01528" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1602.01528</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍神经网络（主要是CNN）模型压缩与加速的常见方法&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>修改TensorFlow Object Detection API</title>
    <link href="http://lijiancheng0614.github.io/2017/09/07/2017_09_07_TensorFlow-Object-Detection-API/"/>
    <id>http://lijiancheng0614.github.io/2017/09/07/2017_09_07_TensorFlow-Object-Detection-API/</id>
    <published>2017-09-06T16:00:00.000Z</published>
    <updated>2017-09-09T12:47:57.809Z</updated>
    
    <content type="html"><![CDATA[<p>代码仓库：<a href="https://github.com/lijiancheng0614/tensorflow_object_detection" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/tensorflow_object_detection</a></p><p>修改TensorFlow Object Detection API，添加一些方便使用或新的功能。</p><a id="more"></a><h2 id="中文">中文</h2><p>使用时记得添加PYTHONPATH：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><p>以及编译protobuf：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure><h3 id="在train_config中添加from_detection_checkpoint参数">在<code>train_config</code>中添加<code>from_detection_checkpoint</code>参数</h3><p>增加加载模型所有参数和不加载模型的模式。</p><p>原来：</p><p><code>from_detection_checkpoint</code>为<code>bool</code>型：</p><ul><li><p>True: 加载detection模型参数中FeatureExtractor部分。</p></li><li><p>False: 加载classification模型参数。</p></li></ul><p>现在：</p><p><code>from_detection_checkpoint</code>为<code>uint32</code>型：</p><ul><li><p>3: 不加载模型参数（适用于train from scratch的情况。）</p></li><li><p>2: 加载模型所有参数（适用于训练中断后重新加载checkpoint的情况。）</p></li><li><p>1: 加载detection模型参数中FeatureExtractor部分。</p></li><li><p>0: 加载classification模型参数。</p></li></ul><p>修改文件：</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>core/model.py</code></p></li><li><p><code>protos/train.proto</code></p></li><li><p><code>meta_architectures/ssd_meta_arch.py</code></p></li><li><p><code>meta_architectures/faster_rcnn_meta_arch.py</code></p></li></ul><h3 id="更新训练时的summary">更新训练时的summary</h3><p>把histograms和first_clone_scope等summaries去掉。这样训练的event文件变小，方便tensorboard加载。</p><p>修改文件：</p><ul><li><code>trainer.py</code></li></ul><h3 id="在eval.py中添加gpu_allow_growth参数">在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="在train.py中添加gpu_allow_growth参数">在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="在train.config中添加max_to_keep参数">在<code>train.config</code>中添加<code>max_to_keep</code>参数</h3><p>在<code>train_config</code>中添加<code>max_to_keep</code>参数，默认为<code>5</code>，即保留最后5个checkpoint。如为<code>0</code>则保留所有的checkpoint。</p><p>修改文件：</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>train.proto</code></p></li></ul><h3 id="网络添加focalsigmoidclassificationloss">网络添加<code>FocalSigmoidClassificationLoss</code></h3><p><code>model</code>中的<code>loss</code>中的<code>classification_loss</code>可使用<code>focal_sigmoid</code>。</p><p>关于Focal loss，参考 <a href="https://arxiv.org/pdf/1708.02002.pdf" class="uri" target="_blank" rel="external">https://arxiv.org/pdf/1708.02002.pdf</a></p><p>修改文件：</p><ul><li><p><code>core/losses.py</code></p></li><li><p><code>builders/losses_builder.py</code></p></li><li><p><code>protos/losses.proto</code></p></li></ul><h2 id="english">English</h2><p>Remember update PYTHONPATH:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><p>And compile protobuf:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure><h3 id="add-from_detection_checkpoint-parameter-in-train_config">Add <code>from_detection_checkpoint</code> parameter in <code>train_config</code></h3><p>Old:</p><p><code>bool from_detection_checkpoint</code></p><ul><li><p>True: the checkpoint was an object detection model that have the same parameters with the exception of the num_classes parameter.</p></li><li><p>False: the checkpoint was a object classification model.</p></li></ul><p>New:</p><p><code>uint32 from_detection_checkpoint</code></p><ul><li><p>3: don’t load any variables.</p></li><li><p>2: load all variables.</p></li><li><p>1: load feature extractor variables from an object detection model, same as <code>True</code>.</p></li><li><p>0: load feature extractor variables from a object classification model, same as <code>False</code>.</p></li></ul><p>Modified files:</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>core/model.py</code></p></li><li><p><code>protos/train.proto</code></p></li><li><p><code>meta_architectures/ssd_meta_arch.py</code></p></li><li><p><code>meta_architectures/faster_rcnn_meta_arch.py</code></p></li></ul><h3 id="remove-some-summaries-when-training">Remove some summaries when training</h3><p>Remove summaries about histograms and first_clone_scope when training.</p><p>Modified files:</p><ul><li><code>trainer.py</code></li></ul><h3 id="add-gpu_allow_growth-parameter-in-eval.py">Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="add-gpu_allow_growth-parameter-in-train.py">Add <code>gpu_allow_growth</code> parameter in <code>train.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>train.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="add-max_to_keep-parameter-in-train_config">Add <code>max_to_keep</code> parameter in <code>train_config</code></h3><p>Add <code>max_to_keep</code> parameter in <code>train_config</code>, default value is <code>5</code> which means the 5 most recent checkpoint files are kept. If <code>0</code>, all checkpoint files are kept.</p><p>Modified files:</p><ul><li><p><code>trainer.py</code></p></li><li><p><code>protos/train.proto</code></p></li></ul><h3 id="add-focalsigmoidclassificationloss-in-model">Add <code>FocalSigmoidClassificationLoss</code> in <code>model</code></h3><p>In config, <code>model</code> -&gt; <code>loss</code> -&gt; <code>classification_loss</code> can be <code>focal_sigmoid</code>, parameters: anchorwise_output, gamma.</p><p>Reference: <a href="https://arxiv.org/pdf/1708.02002.pdf" class="uri" target="_blank" rel="external">https://arxiv.org/pdf/1708.02002.pdf</a></p><p>Modified files:</p><ul><li><p><code>core/losses.py</code></p></li><li><p><code>builders/losses_builder.py</code></p></li><li><p><code>protos/losses.proto</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码仓库：&lt;a href=&quot;https://github.com/lijiancheng0614/tensorflow_object_detection&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lijiancheng0614/tensorflow_object_detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;修改TensorFlow Object Detection API，添加一些方便使用或新的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow在Android上进行物体检测</title>
    <link href="http://lijiancheng0614.github.io/2017/09/01/2017_09_01_TensorFlow-Android-Detection/"/>
    <id>http://lijiancheng0614.github.io/2017/09/01/2017_09_01_TensorFlow-Android-Detection/</id>
    <published>2017-08-31T16:00:00.000Z</published>
    <updated>2017-09-09T12:43:10.660Z</updated>
    
    <content type="html"><![CDATA[<p>使用TensorFlow Android Inference Interface在Android上进行图像物体检测</p><p>不支持Camera2 API的手机也可以物体检测：<a href="https://github.com/lijiancheng0614/android-TFDetect" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/android-TFDetect</a></p><a id="more"></a><h2 id="从tensorflow源代码编译tensorflow-android-camera-demo">从TensorFlow源代码编译TensorFlow Android Camera Demo</h2><p>参考 <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/" class="uri" target="_blank" rel="external">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/</a></p><h3 id="使用bazel编译">使用Bazel编译</h3><ol style="list-style-type: decimal"><li><p>下载TensorFlow源代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> --recurse-submodules https://github.com/tensorflow/tensorflow.git</div></pre></td></tr></table></figure></li><li><p>准备环境</p><ul><li><p>安装Bazel</p></li><li><p>安装Android NDK，版本应与Bazel配合，见官网对应的版本</p></li><li><p>安装Android SDK</p></li><li><p>编辑<code>tensorflow/WORKSPACE</code>中NDK和SDK的路径</p></li></ul></li><li><p>编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow</span></div><div class="line">bazel build -c opt //tensorflow/examples/android:tensorflow_demo</div></pre></td></tr></table></figure></li><li><p>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk</div></pre></td></tr></table></figure></li></ol><h3 id="使用android-studio和bazel编译">使用Android Studio和Bazel编译</h3><p>修改<code>tensorflow/examples/android/build.gradle</code>中Bazel的路径，直接用Android Studio导入<code>tensorflow/examples/android/</code>目录作为新的Android Studio项目。</p><h2 id="不支持camera2-api的手机">不支持Camera2 API的手机</h2><p>由于部分手机不支持Camera2 API，故需要把调用Camera2 API的代码去掉。</p><p>具体来说，把tracking部分的代码删掉，只做object detection，并更新相应画bounding box的代码，这样速度也有所加快。</p><p>详细代码见：<a href="https://github.com/lijiancheng0614/android-TFDetect" class="uri" target="_blank" rel="external">https://github.com/lijiancheng0614/android-TFDetect</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用TensorFlow Android Inference Interface在Android上进行图像物体检测&lt;/p&gt;
&lt;p&gt;不支持Camera2 API的手机也可以物体检测：&lt;a href=&quot;https://github.com/lijiancheng0614/android-TFDetect&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lijiancheng0614/android-TFDetect&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Android" scheme="http://lijiancheng0614.github.io/tags/Android/"/>
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>用VPS搭建IPv6的VPN</title>
    <link href="http://lijiancheng0614.github.io/2017/08/27/2017_08_27_vps_ipv6/"/>
    <id>http://lijiancheng0614.github.io/2017/08/27/2017_08_27_vps_ipv6/</id>
    <published>2017-08-26T16:00:00.000Z</published>
    <updated>2017-09-08T16:51:42.648Z</updated>
    
    <content type="html"><![CDATA[<p>使用tunnelbroker为VPS创建IPv6隧道。</p><p>使用shadowsocks建立IPv6的VPN。</p><a id="more"></a><p>一般VPS没有IPv6地址，如想有IPv6地址，则可以为VPS创建IPv6隧道，只需要有公网IPv4地址即可。因此其它主机也可以创建隧道。</p><h2 id="隧道申请">隧道申请</h2><p>选择一个隧道服务的提供商，如 <a href="https://www.tunnelbroker.net/" class="uri" target="_blank" rel="external">https://www.tunnelbroker.net/</a></p><p>注册账号并登陆，点击<code>Create Regular Tunnel</code>，在<code>IPv4 Endpoint (Your side)</code>中填入VPS的IPv4地址，在<code>Available Tunnel Servers</code>中选一个较近的Server（这样延迟较小），最后点击<code>Create Tunnel</code>。</p><p>创建完成后，在<code>Main Page</code>中会显示所创建的隧道，点击进入详情。</p><p><code>IPv6 Tunnel</code>即可看到相关信息，点击旁边<code>Example Configurations</code>即可看到不同操作系统的配置。</p><h2 id="vps配置">VPS配置</h2><p>根据VPS的系统执行<code>Example Configurations</code>中的命令，一般就完成了。这里选取几种常见的系统的命令。</p><ul><li><p>Debian/Ubuntu:</p><p>把以下文本添加到<code>/etc/network/interfaces</code>中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">auto he-ipv6</div><div class="line">iface he-ipv6 inet6 v4tunnel</div><div class="line">    address 2001:470:XXX:XXX::2</div><div class="line">    netmask 64</div><div class="line">    endpoint 216.218.221.6</div><div class="line">    local XXX.XXX.XXX.XXX</div><div class="line">    ttl 255</div><div class="line">    gateway 2001:470:XXX:XXX::1</div></pre></td></tr></table></figure></li><li><p>Linux-route2:</p><p>在command中运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">modprobe ipv6</div><div class="line">ip tunnel add he-ipv6 mode sit remote 216.218.221.6 <span class="built_in">local</span> XXX.XXX.XXX.XXX ttl 255</div><div class="line">ip link <span class="built_in">set</span> he-ipv6 up</div><div class="line">ip addr add 2001:470:XXX:XXX::2/64 dev he-ipv6</div><div class="line">ip route add ::/0 dev he-ipv6</div><div class="line">ip -f inet6 addr</div></pre></td></tr></table></figure></li><li><p>Mac OS X:</p><p>在command中运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ifconfig gif0 create</div><div class="line">ifconfig gif0 tunnel XXX.XXX.XXX.XXX 216.218.221.6</div><div class="line">ifconfig gif0 inet6 2001:470:XXX:XXX::2 2001:470:XXX:XXX::1 prefixlen 128</div><div class="line">route -n add -inet6 default 2001:470:XXX:XXX::1</div></pre></td></tr></table></figure></li><li><p>Windows 10:</p><p>在命令行中运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">netsh interface teredo set state disabled</div><div class="line">netsh interface ipv6 add v6v4tunnel interface=IP6Tunnel localaddress=XXX.XXX.XXX.XXX remoteaddress=216.218.221.6</div><div class="line">netsh interface ipv6 add address interface=IP6Tunnel address=2001:470:XXX:XXX::2</div><div class="line">netsh interface ipv6 add route prefix=::/0 interface=IP6Tunnel nexthop=2001:470:XXX:XXX::1</div></pre></td></tr></table></figure><p>可能还需要添加IPv6 DNS服务器，在“网络和共享中心”中，选择其中一个网络，修改属性中的“Internet 协议版本 6（TCP/IPv6）”属性，添加DNS服务器，如<code>2001:470:20::2</code>。</p></li></ul><h2 id="查看ipv6地址">查看IPv6地址</h2><p>根据操作系统，运行<code>ifconfig</code>或<code>ipconfig</code>即可看到IPv6的网络接口，尝试ping或<code>ping6 ipv6.google.com</code>如果成功即创建IPv6隧道成功。</p><p>这样，可以在VPS中访问IPv6的资源。</p><h2 id="搭建ipv6的vpn">搭建IPv6的VPN</h2><p>配置VPN有多种方法，这里以shadowsocks为例。</p><ul><li><p>安装shadowsocks服务端</p><p>根据操作系统不同可能有不同的安装方法，这里以python版为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install shadowsocks</div></pre></td></tr></table></figure></li><li><p>编辑配置文件，如<code>ss-conf-ipv6.json</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;server&quot;:&quot;::&quot;,</div><div class="line">    &quot;server_port&quot;:8388,</div><div class="line">    &quot;local_address&quot;:&quot;127.0.0.1&quot;,</div><div class="line">    &quot;local_port&quot;:1080,</div><div class="line">    &quot;password&quot;:&quot;$PASSWORD&quot;,</div><div class="line">    &quot;timeout&quot;:600,</div><div class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><code>$PASSWORD</code>填入自定义的密码，<code>server</code>不是IPv4时的<code>0.0.0.0</code>而是<code>::</code>，其它参数自行修改。</p></li><li><p>启动shadowsocks</p><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssserver -c ss-conf-ipv6.json -d start</div></pre></td></tr></table></figure></li><li><p>关闭shadowsocks</p><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssserver -d stop</div></pre></td></tr></table></figure></li><li><p>在客户端中运行shadowsocks</p><p>在主机、平板、手机等上安装相应的shadowsocks客户端并填入相关配置即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Server Addr: 2001:470:xxx:xxx::2</div><div class="line">Server Port: 8388</div><div class="line">Password: $PASSWORD</div><div class="line">Encryption: aes-256-cfb</div><div class="line">Proxy Port: 1080</div></pre></td></tr></table></figure><p>这样，客户端可以通过IPv6连接VPN，再通过VPS访问IPv4的资源。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用tunnelbroker为VPS创建IPv6隧道。&lt;/p&gt;
&lt;p&gt;使用shadowsocks建立IPv6的VPN。&lt;/p&gt;
    
    </summary>
    
      <category term="Default" scheme="http://lijiancheng0614.github.io/categories/Default/"/>
    
    
      <category term="VPS" scheme="http://lijiancheng0614.github.io/tags/VPS/"/>
    
      <category term="IPv6" scheme="http://lijiancheng0614.github.io/tags/IPv6/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow Object Detection API进行物体检测</title>
    <link href="http://lijiancheng0614.github.io/2017/08/22/2017_08_22_TensorFlow-Object-Detection-API/"/>
    <id>http://lijiancheng0614.github.io/2017/08/22/2017_08_22_TensorFlow-Object-Detection-API/</id>
    <published>2017-08-21T16:00:00.000Z</published>
    <updated>2017-10-06T12:14:37.165Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/object_detection" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/object_detection</a></p><p>使用TensorFlow Object Detection API进行物体检测</p><a id="more"></a><div class="figure"><img src="https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/g3doc/img/kites_detections_output.jpg"></div><h2 id="准备">准备</h2><ol start="0" style="list-style-type: decimal"><li><p>文件结构</p><p>为了方便查看文件，使用以下文件结构。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">object_detection</div><div class="line">├── checkpoints</div><div class="line">│   └── ssd_mobilenet_v1_coco_11_06_2017</div><div class="line">│       ├── frozen_inference_graph.pb</div><div class="line">│       ├── graph.pbtxt</div><div class="line">│       ├── model.ckpt.data-00000-of-00001</div><div class="line">│       ├── model.ckpt.index</div><div class="line">│       └── model.ckpt.meta</div><div class="line">└── VOC2012</div><div class="line">    ├── data</div><div class="line">    │   ├── VOCdevkit</div><div class="line">    │   │   └── ...</div><div class="line">    │   ├── pascal_label_map.pbtxt</div><div class="line">    │   ├── pascal_train.record</div><div class="line">    │   └── pascal_val.record</div><div class="line">    ├── ssd_mobilenet</div><div class="line">    │   ├── eval_logs</div><div class="line">    │   │   ├── ...</div><div class="line">    │   │   └── events.out.tfevents....</div><div class="line">    │   ├── logs</div><div class="line">    │   │   ├── ...</div><div class="line">    │   │   └── train_....txt</div><div class="line">    │   ├── output</div><div class="line">    │   │   ├── saved_model</div><div class="line">    │   │   │   ├── variables</div><div class="line">    │   │   │   └── saved_model.pb</div><div class="line">    │   │   ├── checkpoint</div><div class="line">    │   │   ├── frozen_inference_graph.pb</div><div class="line">    │   │   ├── model.ckpt.data-00000-of-00001</div><div class="line">    │   │   ├── model.ckpt.index</div><div class="line">    │   │   └── model.ckpt.meta</div><div class="line">    │   ├── train_logs</div><div class="line">    │   │   ├── ...</div><div class="line">    │   │   ├── model.ckpt-0.data-00000-of-00001</div><div class="line">    │   │   ├── model.ckpt-0.index</div><div class="line">    │   │   └── model.ckpt-0.meta</div><div class="line">    │   ├── eval.sh</div><div class="line">    │   ├── ssd_mobilenet_v1.config</div><div class="line">    │   └── train.sh</div><div class="line">    ├── create_pascal_tf_record.py</div><div class="line">    ├── eval.py</div><div class="line">    ├── infer.py</div><div class="line">    └── test.py</div></pre></td></tr></table></figure></li><li><p>安装TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/install/</a></p><p>如在Ubuntu下安装TensorFlow with GPU support, python 2.7版本的TensorFlow 1.2.0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div><div class="line">pip install tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div></pre></td></tr></table></figure></li><li><p>配置TensorFlow Models</p><ul><li>下载TensorFlow Models</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</div></pre></td></tr></table></figure><ul><li>编译protobuf</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure><p>生成若干py文件在<code>object_detection/protos/</code>。</p><ul><li>添加PYTHONPATH</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</div></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/</span></div><div class="line">python object_detection/builders/model_builder_test.py</div></pre></td></tr></table></figure><p>若成功，显示<code>OK</code>。</p></li><li><p>准备数据</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/preparing_inputs.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/preparing_inputs.md</a></p><p>这里以<code>PASCAL VOC 2012</code>为例。</p><ul><li>下载并解压</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection</span></div><div class="line">mkdir -p VOC2012/data</div><div class="line"><span class="built_in">cd</span> VOC2012/data</div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</div><div class="line">tar -xvf VOCtrainval_11-May-2012.tar</div></pre></td></tr></table></figure><ul><li>生成TFRecord</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/</span></div><div class="line">cp ../../create_pascal_tf_record.py .</div><div class="line">cp ../../data/pascal_label_map.pbtxt data/</div><div class="line">python create_pascal_tf_record.py \</div><div class="line">    --label_map_path=data/pascal_label_map.pbtxt \</div><div class="line">    --data_dir=data/VOCdevkit --year=VOC2012 --<span class="built_in">set</span>=train \</div><div class="line">    --output_path=data/pascal_train.record</div><div class="line">python create_pascal_tf_record.py \</div><div class="line">    --label_map_path=data/pascal_label_map.pbtxt \</div><div class="line">    --data_dir=data/VOCdevkit --year=VOC2012 --<span class="built_in">set</span>=val \</div><div class="line">    --output_path=data/pascal_val.record</div></pre></td></tr></table></figure><p>得到<code>data/pascal_train.record</code>和<code>data/pascal_val.record</code>。</p><p>如果需要用自己的数据，则参考<code>create_pascal_tf_record.py</code>编写处理数据生成TFRecord的脚本。可参考 <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md</a></p></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型（ <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md</a> ），这里以<code>ssd_mobilenet_v1_coco</code>以例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/</span></div><div class="line">mkdir checkpoints</div><div class="line"><span class="built_in">cd</span> checkpoints</div><div class="line">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz</div><div class="line">tar zxf ssd_mobilenet_v1_coco_11_06_2017.tar.gz</div></pre></td></tr></table></figure></li></ol><h2 id="训练">训练</h2><p>如果使用现有模型进行预测则不需要训练。</p><ol style="list-style-type: decimal"><li><p>配置</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/configuring_jobs.md" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/configuring_jobs.md</a></p><p>这里使用SSD with MobileNet，把<code>object_detection/samples/configs/ssd_mobilenet_v1_pets.config</code>复制到<code>object_detection/VOC2012/ssd_mobilenet/ssd_mobilenet_v1.config</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012</span></div><div class="line">mkdir ssd_mobilenet</div><div class="line">cp ../samples/configs/ssd_mobilenet_v1_pets.config ssd_mobilenet/ssd_mobilenet_v1.config</div></pre></td></tr></table></figure><p>并进行相应的修改：</p><p>修改第9行为<code>num_classes: 20</code>。</p><p>修改第158行为<code>fine_tune_checkpoint: &quot;../../checkpoints/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt&quot;</code></p><p>修改第177行为<code>input_path: &quot;../data/pascal_train.record&quot;</code></p><p>修改第179行和193行为<code>label_map_path: &quot;../data/pascal_label_map.pbtxt&quot;</code></p><p>修改第191行为<code>input_path: &quot;../data/pascal_val.record&quot;</code></p></li><li><p>训练</p><p>新建<code>object_detection/VOC2012/ssd_mobilenet/train.sh</code>，内容以下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p logs/</div><div class="line">now=$(date +<span class="string">"%Y%m%d_%H%M%S"</span>)</div><div class="line">python ../../train.py \</div><div class="line">    --logtostderr \</div><div class="line">    --pipeline_config_path=ssd_mobilenet_v1.config \</div><div class="line">    --train_dir=train_logs 2&gt;&amp;1 | tee logs/train_<span class="variable">$now</span>.txt &amp;</div></pre></td></tr></table></figure><p>进入<code>object_detection/VOC2012/ssd_mobilenet/</code>，运行<code>./train.sh</code>即可训练。</p></li><li><p>验证</p><p>可一边训练一边验证，注意使用其它的GPU或合理分配显存。</p><p>新建<code>object_detection/VOC2012/ssd_mobilenet/eval.sh</code>，内容以下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p eval_logs</div><div class="line">python ../../eval.py \</div><div class="line">    --logtostderr \</div><div class="line">    --pipeline_config_path=ssd_mobilenet_v1.config \</div><div class="line">    --checkpoint_dir=train_logs \</div><div class="line">    --eval_dir=eval_logs &amp;</div></pre></td></tr></table></figure><p>进入<code>object_detection/VOC2012/ssd_mobilenet/</code>，运行<code>CUDA_VISIBLE_DEVICES=&quot;1&quot; ./eval.sh</code>即可验证（这里指定了第二个GPU）。</p></li><li><p>可视化log</p><p>可一边训练一边可视化训练的log，访问<code>http://localhost:6006/</code>即可看到Loss等的变化。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">tensorboard --logdir train_logs/</div></pre></td></tr></table></figure><p>可视化验证的log，可看到<code>Precision/mAP@0.5IOU</code>的变化以及具体image的预测结果，这里指定了另一个端口。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">tensorboard --logdir eval_logs/ --port 6007</div></pre></td></tr></table></figure><p>或同时可视化训练与验证的log：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">tensorboard --logdir .</div></pre></td></tr></table></figure></li></ol><h2 id="测试">测试</h2><ol style="list-style-type: decimal"><li><p>导出模型</p><p>训练完成后得到一些checkpoint文件在<code>tensorflow/models/object_detection/VOC2012/ssd_mobilenet/train_logs/</code>中，如：</p><ul><li>graph.pbtxt</li><li>model.ckpt-200000.data-00000-of-00001</li><li>model.ckpt-200000.info</li><li>model.ckpt-200000.meta</li></ul><p>其中meta保存了graph和metadata，ckpt保存了网络的weights。</p><p>而进行预测时只需模型和权重，不需要metadata，故可使用官方提供的脚本生成推导图。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/ssd_mobilenet/</span></div><div class="line">mkdir -p output</div><div class="line">CUDA_VISIBLE_DEVICES=<span class="string">"1"</span> python ../../export_inference_graph.py \</div><div class="line">    --input_type image_tensor \</div><div class="line">    --pipeline_config_path ssd_mobilenet_v1.config \</div><div class="line">    --trained_checkpoint_prefix train_logs/model.ckpt-200000 \</div><div class="line">    --output_directory output/</div></pre></td></tr></table></figure></li><li><p>测试图片</p><ul><li><p>运行<code>object_detection_tutorial.ipynb</code>并修改其中的各种路径即可。</p></li><li><p>或自写inference脚本，如<code>tensorflow/models/object_detection/VOC2012/infer.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">'..'</span>)</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> label_map_util</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> visualization_utils <span class="keyword">as</span> vis_util</div><div class="line"></div><div class="line"><span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</div><div class="line">    print(<span class="string">'Usage: python &#123;&#125; test_image_path checkpoint_path'</span>.format(sys.argv[<span class="number">0</span>]))</div><div class="line">    exit()</div><div class="line"></div><div class="line">PATH_TEST_IMAGE = sys.argv[<span class="number">1</span>]</div><div class="line">PATH_TO_CKPT = sys.argv[<span class="number">2</span>]</div><div class="line">PATH_TO_LABELS = <span class="string">'data/pascal_label_map.pbtxt'</span></div><div class="line">NUM_CLASSES = <span class="number">21</span></div><div class="line">IMAGE_SIZE = (<span class="number">18</span>, <span class="number">12</span>)</div><div class="line"></div><div class="line">label_map = label_map_util.load_labelmap(PATH_TO_LABELS)</div><div class="line">categories = label_map_util.convert_label_map_to_categories(</div><div class="line">    label_map, max_num_classes=NUM_CLASSES, use_display_name=<span class="keyword">True</span>)</div><div class="line">category_index = label_map_util.create_category_index(categories)</div><div class="line"></div><div class="line">detection_graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    od_graph_def = tf.GraphDef()</div><div class="line">    <span class="keyword">with</span> tf.gfile.GFile(PATH_TO_CKPT, <span class="string">'rb'</span>) <span class="keyword">as</span> fid:</div><div class="line">        serialized_graph = fid.read()</div><div class="line">        od_graph_def.ParseFromString(serialized_graph)</div><div class="line">        tf.import_graph_def(od_graph_def, name=<span class="string">''</span>)</div><div class="line"></div><div class="line">config = tf.ConfigProto()</div><div class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    <span class="keyword">with</span> tf.Session(graph=detection_graph, config=config) <span class="keyword">as</span> sess:</div><div class="line">        start_time = time.time()</div><div class="line">        print(time.ctime())</div><div class="line">        image = Image.open(PATH_TEST_IMAGE)</div><div class="line">        image_np = np.array(image).astype(np.uint8)</div><div class="line">        image_np_expanded = np.expand_dims(image_np, axis=<span class="number">0</span>)</div><div class="line">        image_tensor = detection_graph.get_tensor_by_name(<span class="string">'image_tensor:0'</span>)</div><div class="line">        boxes = detection_graph.get_tensor_by_name(<span class="string">'detection_boxes:0'</span>)</div><div class="line">        scores = detection_graph.get_tensor_by_name(<span class="string">'detection_scores:0'</span>)</div><div class="line">        classes = detection_graph.get_tensor_by_name(<span class="string">'detection_classes:0'</span>)</div><div class="line">        num_detections = detection_graph.get_tensor_by_name(<span class="string">'num_detections:0'</span>)</div><div class="line">        (boxes, scores, classes, num_detections) = sess.run(</div><div class="line">            [boxes, scores, classes, num_detections],</div><div class="line">            feed_dict=&#123;image_tensor: image_np_expanded&#125;)</div><div class="line">        print(<span class="string">'&#123;&#125; elapsed time: &#123;:.3f&#125;s'</span>.format(time.ctime(), time.time() - start_time))</div><div class="line">        vis_util.visualize_boxes_and_labels_on_image_array(</div><div class="line">            image_np, np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores),</div><div class="line">            category_index, use_normalized_coordinates=<span class="keyword">True</span>, line_thickness=<span class="number">8</span>)</div><div class="line">        plt.figure(figsize=IMAGE_SIZE)</div><div class="line">        plt.imshow(image_np)</div></pre></td></tr></table></figure><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/</span></div><div class="line">python infer.py \</div><div class="line">    ../test_images/image1.jpg \</div><div class="line">    ssd_mobilenet/output/frozen_inference_graph.pb</div></pre></td></tr></table></figure></li></ul></li><li><p>批量测试</p><ul><li><p>运行<code>./eval.sh</code>即可测试<code>train_logs/</code>中<code>train_logs/checkpoint</code>里包含的checkpoint并计算<code>Precision/mAP@0.5IOU</code>。</p></li><li><p>或自写脚本，对每张图片进行预测并把分类得分大于某个阈值的预测结果保存到json文件中，如<code>tensorflow/models/object_detection/VOC2012/test.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">'..'</span>)</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> label_map_util</div><div class="line"></div><div class="line"><span class="keyword">if</span> len(sys.argv) &lt; <span class="number">5</span>:</div><div class="line">    print(<span class="string">'Usage: python &#123;&#125; output_json_path checkpoint_path test_ids_path image_dir'</span>.format(sys.argv[<span class="number">0</span>]))</div><div class="line">    exit()</div><div class="line"></div><div class="line">PATH_OUTPUT = sys.argv[<span class="number">1</span>]</div><div class="line">PATH_TO_CKPT = sys.argv[<span class="number">2</span>]</div><div class="line">PATH_TEST_IDS = sys.argv[<span class="number">3</span>]</div><div class="line">DIR_IMAGE = sys.argv[<span class="number">4</span>]</div><div class="line">PATH_TO_LABELS = <span class="string">'data/pascal_label_map.pbtxt'</span></div><div class="line">NUM_CLASSES = <span class="number">21</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_results</span><span class="params">(boxes, classes, scores, category_index, im_width, im_height,</span></span></div><div class="line"><span class="function"><span class="params">    min_score_thresh=<span class="number">.5</span>)</span>:</span></div><div class="line">    bboxes = list()</div><div class="line">    <span class="keyword">for</span> i, box <span class="keyword">in</span> enumerate(boxes):</div><div class="line">        <span class="keyword">if</span> scores[i] &gt; min_score_thresh:</div><div class="line">            ymin, xmin, ymax, xmax = box</div><div class="line">            bbox = &#123;</div><div class="line">                <span class="string">'bbox'</span>: &#123;</div><div class="line">                    <span class="string">'xmax'</span>: xmax * im_width,</div><div class="line">                    <span class="string">'xmin'</span>: xmin * im_width,</div><div class="line">                    <span class="string">'ymax'</span>: ymax * im_height,</div><div class="line">                    <span class="string">'ymin'</span>: ymin * im_height</div><div class="line">                &#125;,</div><div class="line">                <span class="string">'category'</span>: category_index[classes[i]][<span class="string">'name'</span>],</div><div class="line">                <span class="string">'score'</span>: float(scores[i])</div><div class="line">            &#125;</div><div class="line">            bboxes.append(bbox)</div><div class="line">    <span class="keyword">return</span> bboxes</div><div class="line"></div><div class="line">label_map = label_map_util.load_labelmap(PATH_TO_LABELS)</div><div class="line">categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=<span class="keyword">True</span>)</div><div class="line">category_index = label_map_util.create_category_index(categories)</div><div class="line"></div><div class="line">detection_graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    od_graph_def = tf.GraphDef()</div><div class="line">    <span class="keyword">with</span> tf.gfile.GFile(PATH_TO_CKPT, <span class="string">'rb'</span>) <span class="keyword">as</span> fid:</div><div class="line">        serialized_graph = fid.read()</div><div class="line">        od_graph_def.ParseFromString(serialized_graph)</div><div class="line">        tf.import_graph_def(od_graph_def, name=<span class="string">''</span>)</div><div class="line"></div><div class="line">config = tf.ConfigProto()</div><div class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line"></div><div class="line">test_ids = [line.split()[<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> open(PATH_TEST_IDS)]</div><div class="line">total_time = <span class="number">0</span></div><div class="line">test_annos = dict()</div><div class="line">flag = <span class="keyword">False</span></div><div class="line"><span class="keyword">with</span> detection_graph.as_default():</div><div class="line">    <span class="keyword">with</span> tf.Session(graph=detection_graph, config=config) <span class="keyword">as</span> sess:</div><div class="line">        <span class="keyword">for</span> image_id <span class="keyword">in</span> test_ids:</div><div class="line">            image_path = os.path.join(DIR_IMAGE, image_id + <span class="string">'.jpg'</span>)</div><div class="line">            image = Image.open(image_path)</div><div class="line">            image_np = np.array(image).astype(np.uint8)</div><div class="line">            im_width, im_height, _ = image_np.shape</div><div class="line">            image_np_expanded = np.expand_dims(image_np, axis=<span class="number">0</span>)</div><div class="line">            image_tensor = detection_graph.get_tensor_by_name(<span class="string">'image_tensor:0'</span>)</div><div class="line">            boxes = detection_graph.get_tensor_by_name(<span class="string">'detection_boxes:0'</span>)</div><div class="line">            scores = detection_graph.get_tensor_by_name(<span class="string">'detection_scores:0'</span>)</div><div class="line">            classes = detection_graph.get_tensor_by_name(<span class="string">'detection_classes:0'</span>)</div><div class="line">            num_detections = detection_graph.get_tensor_by_name(<span class="string">'num_detections:0'</span>)</div><div class="line">            start_time = time.time()</div><div class="line">            (boxes, scores, classes, num_detections) = sess.run(</div><div class="line">                [boxes, scores, classes, num_detections],</div><div class="line">                feed_dict=&#123;image_tensor: image_np_expanded&#125;)</div><div class="line">            end_time = time.time()</div><div class="line">            print(<span class="string">'&#123;&#125; &#123;&#125; &#123;:.3f&#125;s'</span>.format(time.ctime(), image_id, end_time - start_time))</div><div class="line">            <span class="keyword">if</span> flag:</div><div class="line">                total_time += end_time - start_time</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                flag = <span class="keyword">True</span></div><div class="line">            test_annos[image_id] = &#123;<span class="string">'objects'</span>: get_results(</div><div class="line">                np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores), category_index,</div><div class="line">                im_width, im_height)&#125;</div><div class="line"></div><div class="line">print(<span class="string">'total time: &#123;&#125;, total images: &#123;&#125;, average time: &#123;&#125;'</span>.format(</div><div class="line">    total_time, len(test_ids), total_time / len(test_ids)))</div><div class="line">test_annos = &#123;<span class="string">'imgs'</span>: test_annos&#125;</div><div class="line">fd = open(PATH_OUTPUT, <span class="string">'w'</span>)</div><div class="line">json.dump(test_annos, fd)</div><div class="line">fd.close()</div></pre></td></tr></table></figure><p>运行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/object_detection/VOC2012/</span></div><div class="line">python test.py \</div><div class="line">    ssd_mobilenet/output/result_annos.json \</div><div class="line">    ssd_mobilenet/output/frozen_inference_graph.pb \</div><div class="line">    data/VOCdevkit/VOC2012/ImageSets/Main/train_val.txt \</div><div class="line">    data/VOCdevkit/VOC2012/JPEGImages/</div></pre></td></tr></table></figure></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/object_detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用TensorFlow Object Detection API进行物体检测&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow-Slim进行图像分类</title>
    <link href="http://lijiancheng0614.github.io/2017/06/29/2017_06_29_TensorFlow-Slim-image-classification/"/>
    <id>http://lijiancheng0614.github.io/2017/06/29/2017_06_29_TensorFlow-Slim-image-classification/</id>
    <published>2017-06-28T16:00:00.000Z</published>
    <updated>2017-11-22T11:28:28.354Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/slim" class="uri" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/slim</a></p><p>使用TensorFlow-Slim进行图像分类</p><a id="more"></a><h2 id="准备">准备</h2><ol style="list-style-type: decimal"><li><p>安装TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="external">https://www.tensorflow.org/install/</a></p><p>如在Ubuntu下安装TensorFlow with GPU support, python 2.7版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div><div class="line">pip install tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl</div></pre></td></tr></table></figure></li><li><p>下载TF-slim图像模型库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span></div><div class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models/</div></pre></td></tr></table></figure></li><li><p>准备数据</p><p>有不少公开数据集，这里以官网提供的<code>Flowers</code>为例。</p><p>官网提供了下载和转换数据的代码，为了理解代码并能使用自己的数据，这里参考官方提供的代码进行修改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/data</div><div class="line">wget http://download.tensorflow.org/example_images/flower_photos.tgz</div><div class="line">tar zxf flower_photos.tgz</div></pre></td></tr></table></figure><p>数据集文件夹结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">flower_photos</div><div class="line">├── daisy</div><div class="line">│   ├── 100080576_f52e8ee070_n.jpg</div><div class="line">│   └── ...</div><div class="line">├── dandelion</div><div class="line">├── LICENSE.txt</div><div class="line">├── roses</div><div class="line">├── sunflowers</div><div class="line">└── tulips</div></pre></td></tr></table></figure><p>由于实际情况中我们自己的数据集并不一定把图片按类别放在不同的文件夹里，故我们生成<code>list.txt</code>来表示图片路径与标签的关系。</p><p>Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">class_names_to_ids = &#123;<span class="string">'daisy'</span>: <span class="number">0</span>, <span class="string">'dandelion'</span>: <span class="number">1</span>, <span class="string">'roses'</span>: <span class="number">2</span>, <span class="string">'sunflowers'</span>: <span class="number">3</span>, <span class="string">'tulips'</span>: <span class="number">4</span>&#125;</div><div class="line">data_dir = <span class="string">'flower_photos/'</span></div><div class="line">output_path = <span class="string">'list.txt'</span></div><div class="line"></div><div class="line">fd = open(output_path, <span class="string">'w'</span>)</div><div class="line"><span class="keyword">for</span> class_name <span class="keyword">in</span> class_names_to_ids.keys():</div><div class="line">    images_list = os.listdir(data_dir + class_name)</div><div class="line">    <span class="keyword">for</span> image_name <span class="keyword">in</span> images_list:</div><div class="line">        fd.write(<span class="string">'&#123;&#125;/&#123;&#125; &#123;&#125;\n'</span>.format(class_name, image_name, class_names_to_ids[class_name]))</div><div class="line"></div><div class="line">fd.close()</div></pre></td></tr></table></figure><p>为了方便后期查看label标签，也可以定义<code>labels.txt</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">daisy</div><div class="line">dandelion</div><div class="line">roses</div><div class="line">sunflowers</div><div class="line">tulips</div></pre></td></tr></table></figure><p>随机生成训练集与验证集：</p><p>Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line">_NUM_VALIDATION = <span class="number">350</span></div><div class="line">_RANDOM_SEED = <span class="number">0</span></div><div class="line">list_path = <span class="string">'list.txt'</span></div><div class="line">train_list_path = <span class="string">'list_train.txt'</span></div><div class="line">val_list_path = <span class="string">'list_val.txt'</span></div><div class="line"></div><div class="line">fd = open(list_path)</div><div class="line">lines = fd.readlines()</div><div class="line">fd.close()</div><div class="line">random.seed(_RANDOM_SEED)</div><div class="line">random.shuffle(lines)</div><div class="line"></div><div class="line">fd = open(train_list_path, <span class="string">'w'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines[_NUM_VALIDATION:]:</div><div class="line">    fd.write(line)</div><div class="line"></div><div class="line">fd.close()</div><div class="line">fd = open(val_list_path, <span class="string">'w'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines[:_NUM_VALIDATION]:</div><div class="line">    fd.write(line)</div><div class="line"></div><div class="line">fd.close()</div></pre></td></tr></table></figure><p>生成TFRecord数据：</p><p>Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.insert(<span class="number">0</span>, <span class="string">'../models/slim/'</span>)</div><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_utils</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_dataset</span><span class="params">(list_path, data_dir, output_dir, _NUM_SHARDS=<span class="number">5</span>)</span>:</span></div><div class="line">    fd = open(list_path)</div><div class="line">    lines = [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> fd]</div><div class="line">    fd.close()</div><div class="line">    num_per_shard = int(math.ceil(len(lines) / float(_NUM_SHARDS)))</div><div class="line">    <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">        decode_jpeg_data = tf.placeholder(dtype=tf.string)</div><div class="line">        decode_jpeg = tf.image.decode_jpeg(decode_jpeg_data, channels=<span class="number">3</span>)</div><div class="line">        <span class="keyword">with</span> tf.Session(<span class="string">''</span>) <span class="keyword">as</span> sess:</div><div class="line">            <span class="keyword">for</span> shard_id <span class="keyword">in</span> range(_NUM_SHARDS):</div><div class="line">                output_path = os.path.join(output_dir,</div><div class="line">                    <span class="string">'data_&#123;:05&#125;-of-&#123;:05&#125;.tfrecord'</span>.format(shard_id, _NUM_SHARDS))</div><div class="line">                tfrecord_writer = tf.python_io.TFRecordWriter(output_path)</div><div class="line">                start_ndx = shard_id * num_per_shard</div><div class="line">                end_ndx = min((shard_id + <span class="number">1</span>) * num_per_shard, len(lines))</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(start_ndx, end_ndx):</div><div class="line">                    sys.stdout.write(<span class="string">'\r&gt;&gt; Converting image &#123;&#125;/&#123;&#125; shard &#123;&#125;'</span>.format(</div><div class="line">                        i + <span class="number">1</span>, len(lines), shard_id))</div><div class="line">                    sys.stdout.flush()</div><div class="line">                    image_data = tf.gfile.FastGFile(os.path.join(data_dir, lines[i][<span class="number">0</span>]), <span class="string">'rb'</span>).read()</div><div class="line">                    image = sess.run(decode_jpeg, feed_dict=&#123;decode_jpeg_data: image_data&#125;)</div><div class="line">                    height, width = image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>]</div><div class="line">                    example = dataset_utils.image_to_tfexample(</div><div class="line">                        image_data, <span class="string">b'jpg'</span>, height, width, int(lines[i][<span class="number">1</span>]))</div><div class="line">                    tfrecord_writer.write(example.SerializeToString())</div><div class="line">                tfrecord_writer.close()</div><div class="line">    sys.stdout.write(<span class="string">'\n'</span>)</div><div class="line">    sys.stdout.flush()</div><div class="line"></div><div class="line">os.system(<span class="string">'mkdir -p train'</span>)</div><div class="line">convert_dataset(<span class="string">'list_train.txt'</span>, <span class="string">'flower_photos'</span>, <span class="string">'train/'</span>)</div><div class="line">os.system(<span class="string">'mkdir -p val'</span>)</div><div class="line">convert_dataset(<span class="string">'list_val.txt'</span>, <span class="string">'flower_photos'</span>, <span class="string">'val/'</span>)</div></pre></td></tr></table></figure><p>得到的文件夹结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">data</div><div class="line">├── flower_photos</div><div class="line">├── labels.txt</div><div class="line">├── list_train.txt</div><div class="line">├── list.txt</div><div class="line">├── list_val.txt</div><div class="line">├── train</div><div class="line">│   ├── data_00000-of-00005.tfrecord</div><div class="line">│   ├── ...</div><div class="line">│   └── data_00004-of-00005.tfrecord</div><div class="line">└── val</div><div class="line">    ├── data_00000-of-00005.tfrecord</div><div class="line">    ├── ...</div><div class="line">    └── data_00004-of-00005.tfrecord</div></pre></td></tr></table></figure></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型，这里以<code>Inception-ResNet-v2</code>以例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/checkpoints</div><div class="line">wget http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz</div><div class="line">tar zxf inception_resnet_v2_2016_08_30.tar.gz</div></pre></td></tr></table></figure></li></ol><h2 id="训练">训练</h2><ol style="list-style-type: decimal"><li><p>读入数据</p><p>官方提供了读入<code>Flowers</code>数据集的代码<code>models/slim/datasets/flowers.py</code>，同样这里也是参考并修改成能读入上面定义的通用数据集。</p><p>把下面代码写入<code>models/slim/datasets/dataset_classification.py</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">slim = tf.contrib.slim</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(dataset_dir, num_samples, num_classes, labels_to_names_path=None, file_pattern=<span class="string">'*.tfrecord'</span>)</span>:</span></div><div class="line">    file_pattern = os.path.join(dataset_dir, file_pattern)</div><div class="line">    keys_to_features = &#123;</div><div class="line">        <span class="string">'image/encoded'</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">''</span>),</div><div class="line">        <span class="string">'image/format'</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">'jpg'</span>),</div><div class="line">        <span class="string">'image/class/label'</span>: tf.FixedLenFeature(</div><div class="line">            [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),</div><div class="line">    &#125;</div><div class="line">    items_to_handlers = &#123;</div><div class="line">        <span class="string">'image'</span>: slim.tfexample_decoder.Image(),</div><div class="line">        <span class="string">'label'</span>: slim.tfexample_decoder.Tensor(<span class="string">'image/class/label'</span>),</div><div class="line">    &#125;</div><div class="line">    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)</div><div class="line">    items_to_descriptions = &#123;</div><div class="line">        <span class="string">'image'</span>: <span class="string">'A color image of varying size.'</span>,</div><div class="line">        <span class="string">'label'</span>: <span class="string">'A single integer between 0 and '</span> + str(num_classes - <span class="number">1</span>),</div><div class="line">    &#125;</div><div class="line">    labels_to_names = <span class="keyword">None</span></div><div class="line">    <span class="keyword">if</span> labels_to_names_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        fd = open(labels_to_names_path)</div><div class="line">        labels_to_names = &#123;i : line.strip() <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fd)&#125;</div><div class="line">        fd.close()</div><div class="line">    <span class="keyword">return</span> slim.dataset.Dataset(</div><div class="line">            data_sources=file_pattern,</div><div class="line">            reader=tf.TFRecordReader,</div><div class="line">            decoder=decoder,</div><div class="line">            num_samples=num_samples,</div><div class="line">            items_to_descriptions=items_to_descriptions,</div><div class="line">            num_classes=num_classes,</div><div class="line">            labels_to_names=labels_to_names)</div></pre></td></tr></table></figure></li><li><p>构建模型</p><p>官方提供了许多模型在<code>models/slim/nets/</code>。</p><p>如需要自定义模型，则参考官方提供的模型并放在对应的文件夹即可。</p></li><li><p>开始训练</p><p>官方提供了训练脚本，如果使用官方的数据读入和处理，可使用以下方式开始训练。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/models/slim</div><div class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0"</span> python train_image_classifier.py \</div><div class="line">    --train_dir=train_logs \</div><div class="line">    --dataset_name=flowers \</div><div class="line">    --dataset_split_name=train \</div><div class="line">    --dataset_dir=../../data/flowers \</div><div class="line">    --model_name=inception_resnet_v2 \</div><div class="line">    --checkpoint_path=../../checkpoints/inception_resnet_v2_2016_08_30.ckpt \</div><div class="line">    --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \</div><div class="line">    --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \</div><div class="line">    --max_number_of_steps=1000 \</div><div class="line">    --batch_size=32 \</div><div class="line">    --learning_rate=0.01 \</div><div class="line">    --learning_rate_decay_type=fixed \</div><div class="line">    --save_interval_secs=60 \</div><div class="line">    --save_summaries_secs=60 \</div><div class="line">    --log_every_n_steps=10 \</div><div class="line">    --optimizer=rmsprop \</div><div class="line">    --weight_decay=0.00004</div></pre></td></tr></table></figure><p>不fine-tune把<code>--checkpoint_path</code>, <code>--checkpoint_exclude_scopes</code>和<code>--trainable_scopes</code>删掉。</p><p>fine-tune所有层把<code>--checkpoint_exclude_scopes</code>和<code>--trainable_scopes</code>删掉。</p><p>如果只使用CPU则加上<code>--clone_on_cpu=True</code>。</p><p>其它参数可删掉用默认值或自行修改。</p><p>使用自己的数据则需要修改<code>models/slim/train_image_classifier.py</code>：</p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_factory</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_classification</div></pre></td></tr></table></figure></p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_factory.get_dataset(</div><div class="line">    FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_classification.get_dataset(</div><div class="line">    FLAGS.dataset_dir, FLAGS.num_samples, FLAGS.num_classes, FLAGS.labels_to_names_path)</div></pre></td></tr></table></figure></p><p>在<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'dataset_dir'</span>, <span class="keyword">None</span>, <span class="string">'The directory where the dataset files are stored.'</span>)</div></pre></td></tr></table></figure></p><p>后加入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_samples'</span>, <span class="number">3320</span>, <span class="string">'Number of samples.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_classes'</span>, <span class="number">5</span>, <span class="string">'Number of classes.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'labels_to_names_path'</span>, <span class="keyword">None</span>, <span class="string">'Label names file path.'</span>)</div></pre></td></tr></table></figure></p><p>训练时执行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/models/slim</div><div class="line">python train_image_classifier.py \</div><div class="line">    --train_dir=train_logs \</div><div class="line">    --dataset_dir=../../data/train \</div><div class="line">    --num_samples=3320 \</div><div class="line">    --num_classes=5 \</div><div class="line">    --labels_to_names_path=../../data/labels.txt \</div><div class="line">    --model_name=inception_resnet_v2 \</div><div class="line">    --checkpoint_path=../../checkpoints/inception_resnet_v2_2016_08_30.ckpt \</div><div class="line">    --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits \</div><div class="line">    --trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits</div></pre></td></tr></table></figure></li><li><p>可视化log</p><p>可一边训练一边可视化训练的log，可看到Loss趋势。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir train_logs/</div></pre></td></tr></table></figure></li></ol><h2 id="验证">验证</h2><p>官方提供了验证脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">python eval_image_classifier.py \</div><div class="line">    --checkpoint_path=train_logs \</div><div class="line">    --eval_dir=eval_logs \</div><div class="line">    --dataset_name=flowers \</div><div class="line">    --dataset_split_name=validation \</div><div class="line">    --dataset_dir=../../data/flowers \</div><div class="line">    --model_name=inception_resnet_v2</div></pre></td></tr></table></figure><p>同样，如果是使用自己的数据集，则需要修改<code>models/slim/eval_image_classifier.py</code>：</p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_factory</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> dataset_classification</div></pre></td></tr></table></figure></p><p>把<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_factory.get_dataset(</div><div class="line">    FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)</div></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = dataset_classification.get_dataset(</div><div class="line">    FLAGS.dataset_dir, FLAGS.num_samples, FLAGS.num_classes, FLAGS.labels_to_names_path)</div></pre></td></tr></table></figure></p><p>在<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'dataset_dir'</span>, <span class="keyword">None</span>, <span class="string">'The directory where the dataset files are stored.'</span>)</div></pre></td></tr></table></figure></p><p>后加入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_samples'</span>, <span class="number">350</span>, <span class="string">'Number of samples.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_classes'</span>, <span class="number">5</span>, <span class="string">'Number of classes.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'labels_to_names_path'</span>, <span class="keyword">None</span>, <span class="string">'Label names file path.'</span>)</div></pre></td></tr></table></figure></p><p>验证时执行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">python eval_image_classifier.py \</div><div class="line">    --checkpoint_path=train_logs \</div><div class="line">    --eval_dir=eval_logs \</div><div class="line">    --dataset_dir=../../data/val \</div><div class="line">    --num_samples=350 \</div><div class="line">    --num_classes=5 \</div><div class="line">    --model_name=inception_resnet_v2</div></pre></td></tr></table></figure><p>可以一边训练一边验证，，注意使用其它的GPU或合理分配显存。</p><p>同样也可以可视化log，如果已经在可视化训练的log则建议使用其它端口，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir eval_logs/ --port 6007</div></pre></td></tr></table></figure><h2 id="测试">测试</h2><p>参考<code>models/slim/eval_image_classifier.py</code>，可编写批量读取图片用模型进行推导的脚本<code>models/slim/test_image_classifier.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">from</span> nets <span class="keyword">import</span> nets_factory</div><div class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> preprocessing_factory</div><div class="line"></div><div class="line">slim = tf.contrib.slim</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'master'</span>, <span class="string">''</span>, <span class="string">'The address of the TensorFlow master to use.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'checkpoint_path'</span>, <span class="string">'/tmp/tfmodel/'</span>,</div><div class="line">    <span class="string">'The directory where the model was written to or an absolute path to a '</span></div><div class="line">    <span class="string">'checkpoint file.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'test_list'</span>, <span class="string">''</span>, <span class="string">'Test image list.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'test_dir'</span>, <span class="string">'.'</span>, <span class="string">'Test image directory.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'batch_size'</span>, <span class="number">16</span>, <span class="string">'Batch size.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'num_classes'</span>, <span class="number">5</span>, <span class="string">'Number of classes.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'labels_offset'</span>, <span class="number">0</span>,</div><div class="line">    <span class="string">'An offset for the labels in the dataset. This flag is primarily used to '</span></div><div class="line">    <span class="string">'evaluate the VGG and ResNet architectures which do not use a background '</span></div><div class="line">    <span class="string">'class for the ImageNet dataset.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'model_name'</span>, <span class="string">'inception_v3'</span>, <span class="string">'The name of the architecture to evaluate.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">    <span class="string">'preprocessing_name'</span>, <span class="keyword">None</span>, <span class="string">'The name of the preprocessing to use. If left '</span></div><div class="line">    <span class="string">'as `None`, then the model_name flag is used.'</span>)</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">    <span class="string">'test_image_size'</span>, <span class="keyword">None</span>, <span class="string">'Eval image size'</span>)</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.test_list:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'You must supply the test list with --test_list'</span>)</div><div class="line"></div><div class="line">    tf.logging.set_verbosity(tf.logging.INFO)</div><div class="line">    <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">        tf_global_step = slim.get_or_create_global_step()</div><div class="line"></div><div class="line">        <span class="comment">####################</span></div><div class="line">        <span class="comment"># Select the model #</span></div><div class="line">        <span class="comment">####################</span></div><div class="line">        network_fn = nets_factory.get_network_fn(</div><div class="line">            FLAGS.model_name,</div><div class="line">            num_classes=(FLAGS.num_classes - FLAGS.labels_offset),</div><div class="line">            is_training=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">        <span class="comment">#####################################</span></div><div class="line">        <span class="comment"># Select the preprocessing function #</span></div><div class="line">        <span class="comment">#####################################</span></div><div class="line">        preprocessing_name = FLAGS.preprocessing_name <span class="keyword">or</span> FLAGS.model_name</div><div class="line">        image_preprocessing_fn = preprocessing_factory.get_preprocessing(</div><div class="line">            preprocessing_name,</div><div class="line">            is_training=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">        test_image_size = FLAGS.test_image_size <span class="keyword">or</span> network_fn.default_image_size</div><div class="line"></div><div class="line">        <span class="keyword">if</span> tf.gfile.IsDirectory(FLAGS.checkpoint_path):</div><div class="line">            checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            checkpoint_path = FLAGS.checkpoint_path</div><div class="line"></div><div class="line">        batch_size = FLAGS.batch_size</div><div class="line">        tensor_input = tf.placeholder(tf.float32, [<span class="keyword">None</span>, test_image_size, test_image_size, <span class="number">3</span>])</div><div class="line">        logits, _ = network_fn(tensor_input)</div><div class="line">        logits = tf.nn.top_k(logits, <span class="number">5</span>)</div><div class="line">        config = tf.ConfigProto()</div><div class="line">        config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line"></div><div class="line">        test_ids = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> open(FLAGS.test_list)]</div><div class="line">        tot = len(test_ids)</div><div class="line">        results = list()</div><div class="line">        <span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</div><div class="line">            sess.run(tf.global_variables_initializer())</div><div class="line">            saver = tf.train.Saver()</div><div class="line">            saver.restore(sess, checkpoint_path)</div><div class="line">            time_start = time.time()</div><div class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">0</span>, tot, batch_size):</div><div class="line">                images = list()</div><div class="line">                idx_end = min(tot, idx + batch_size)</div><div class="line">                print(idx)</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(idx, idx_end):</div><div class="line">                    image_id = test_ids[i]</div><div class="line">                    test_path = os.path.join(FLAGS.test_dir, image_id)</div><div class="line">                    image = open(test_path, <span class="string">'rb'</span>).read()</div><div class="line">                    image = tf.image.decode_jpeg(image, channels=<span class="number">3</span>)</div><div class="line">                    processed_image = image_preprocessing_fn(image, test_image_size, test_image_size)</div><div class="line">                    processed_image = sess.run(processed_image)</div><div class="line">                    images.append(processed_image)</div><div class="line">                images = np.array(images)</div><div class="line">                predictions = sess.run(logits, feed_dict = &#123;tensor_input : images&#125;).indices</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(idx, idx_end):</div><div class="line">                    print(<span class="string">'&#123;&#125; &#123;&#125;'</span>.format(image_id, predictions[i - idx].tolist())</div><div class="line">            time_total = time.time() - time_start</div><div class="line">            print(<span class="string">'total time: &#123;&#125;, total images: &#123;&#125;, average time: &#123;&#125;'</span>.format(</div><div class="line">                time_total, len(test_ids), time_total / len(test_ids)))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure><p>测试时执行以下命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0"</span> python test_image_classifier.py \</div><div class="line">    --checkpoint_path=train_logs/ \</div><div class="line">    --test_list=../../data/list_val.txt \</div><div class="line">    --test_dir=../../data/flower_photos/ \</div><div class="line">    --batch_size=16 \</div><div class="line">    --num_classes=5 \</div><div class="line">    --model_name=inception_resnet_v2</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tensorflow/models/tree/master/research/slim&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用TensorFlow-Slim进行图像分类&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
</feed>
